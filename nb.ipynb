{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af6f887c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (0.49.1)\n",
      "Requirement already satisfied: torch<3,>=2.3 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from bitsandbytes) (2.9.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from bitsandbytes) (23.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (2024.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb39af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 21:41:46,849 - INFO - Zip file already exists.\n",
      "2026-01-12 21:41:46,850 - INFO - Extracting...\n",
      "2026-01-12 21:41:47,280 - INFO - Data successfully ready in D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\raw\n",
      "2026-01-12 21:41:47,281 - INFO - Files in data folder: ['hu-ro.txt.zip', 'JRC-Acquis.hu-ro.hu', 'JRC-Acquis.hu-ro.ro', 'JRC-Acquis.hu-ro.xml', 'LICENSE', 'README']\n"
     ]
    }
   ],
   "source": [
    "!python scripts/download_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb81f10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 21:42:55,001 - INFO - Loading JRC-Acquis.hu-ro.hu and JRC-Acquis.hu-ro.ro...\n",
      "2026-01-12 21:42:56,767 - INFO - Original: 417178 | Cleaned: 310067 | Removed: 107111\n",
      "2026-01-12 21:42:56,802 - INFO - Split Sizes -> Train: 248053, Val: 31007, Test: 31007\n",
      "2026-01-12 21:42:59,194 - INFO - SUCCESS! Splits saved to D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "!python scripts/prepare_splits.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38a10aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 21:47:32.958678: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-12 21:47:36.732384: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Loading test data from D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\processed\\test.csv...\n",
      "Loading model facebook/nllb-200-distilled-600M on cuda...\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RoG\\.cache\\huggingface\\hub\\models--facebook--nllb-200-distilled-600M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Translating...\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:551: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "\n",
      "  8%|â–Š         | 1/13 [00:03<00:47,  3.93s/it]\n",
      " 15%|â–ˆâ–Œ        | 2/13 [00:05<00:26,  2.45s/it]\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:06<00:19,  1.93s/it]\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:08<00:17,  2.00s/it]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:10<00:14,  1.76s/it]\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:11<00:10,  1.47s/it]\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:15<00:13,  2.31s/it]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:16<00:09,  1.98s/it]\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:21<00:11,  2.87s/it]\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:22<00:07,  2.42s/it]\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:25<00:04,  2.46s/it]\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:25<00:01,  1.96s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:26<00:00,  1.58s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:26<00:00,  2.05s/it]\n",
      "--------------------------------------------------\n",
      "BASELINE BLEU SCORE: 8.26\n",
      "--------------------------------------------------\n",
      "Downloading/Loading COMET model (this might take a moment)...\n",
      "\n",
      "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RoG\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "\n",
      "Fetching 5 files:  20%|â–ˆâ–ˆ        | 1/5 [00:03<00:12,  3.17s/it]\n",
      "Fetching 5 files:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:03<00:04,  1.58s/it]\n",
      "COMET calculation failed: \"Model 'Unbabel/wmt22-comet-da' not supported by COMET.\"\n",
      "Continuing to save predictions despite COMET failure.\n",
      "Predictions saved to D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\\baseline_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "!python scripts/evaluate_baseline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fae2b2",
   "metadata": {},
   "source": [
    "## Downloading JRC-Acquis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d150060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\n",
      "DATA_RAW: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\raw\n",
      "DATA_PROCESSED: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\processed\n",
      "OUTPUT_DIR: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\n"
     ]
    }
   ],
   "source": [
    "# If running on Colab, uncomment:\n",
    "# !pip -q install transformers datasets accelerate sacrebleu sentencepiece peft evaluate pandas scikit-learn tqdm comet-ml unbabel-comet\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"data\" / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_RAW:\", DATA_RAW)\n",
    "print(\"DATA_PROCESSED:\", DATA_PROCESSED)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d66c0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Zip already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files: ['hu-ro.txt.zip', 'JRC-Acquis.hu-ro.hu', 'JRC-Acquis.hu-ro.ro', 'JRC-Acquis.hu-ro.xml', 'LICENSE', 'README']\n"
     ]
    }
   ],
   "source": [
    "import requests, zipfile, logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "OPUS_DIRECT_URL = \"https://object.pouta.csc.fi/OPUS-JRC-Acquis/v3.0/moses/hu-ro.txt.zip\"\n",
    "zip_path = DATA_RAW / \"hu-ro.txt.zip\"\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not zip_path.exists():\n",
    "    logging.info(f\"Downloading {OPUS_DIRECT_URL}...\")\n",
    "    r = requests.get(OPUS_DIRECT_URL, stream=True, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "    r.raise_for_status()\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    logging.info(\"Download complete.\")\n",
    "else:\n",
    "    logging.info(\"Zip already exists.\")\n",
    "\n",
    "assert zipfile.is_zipfile(zip_path), \"Downloaded file is not a valid zip.\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    z.extractall(DATA_RAW)\n",
    "\n",
    "print(\"Extracted files:\", [p.name for p in DATA_RAW.glob(\"*\")][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c287252e",
   "metadata": {},
   "source": [
    "## Preparing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4bf14a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw pairs: 417178\n",
      "After cleaning: 303659\n",
      "Saved: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\processed\\train.csv D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\processed\\val.csv D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\processed\\test.csv\n",
      "Sizes: 242927 30366 30366\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "def load_parallel_from_raw(raw_dir: Path):\n",
    "    hu_path = list(raw_dir.glob(\"*.hu\"))[0]\n",
    "    ro_path = list(raw_dir.glob(\"*.ro\"))[0]\n",
    "    with open(hu_path, \"r\", encoding=\"utf-8\") as f: hu = [x.strip() for x in f]\n",
    "    with open(ro_path, \"r\", encoding=\"utf-8\") as f: ro = [x.strip() for x in f]\n",
    "    assert len(hu) == len(ro)\n",
    "    return pd.DataFrame({\"hu\": hu, \"ro\": ro})\n",
    "\n",
    "df = load_parallel_from_raw(DATA_RAW)\n",
    "print(\"Raw pairs:\", len(df))\n",
    "\n",
    "# Basic cleaning (like your script)\n",
    "df = df.drop_duplicates()\n",
    "df = df[df[\"hu\"].str.strip().astype(bool)]\n",
    "df = df[df[\"ro\"].str.strip().astype(bool)]\n",
    "\n",
    "# Length ratio filter (very effective for alignment noise)\n",
    "def len_ratio_ok(s, t, min_ratio=0.5, max_ratio=2.0):\n",
    "    ls, lt = max(len(s), 1), max(len(t), 1)\n",
    "    r = ls / lt\n",
    "    return (r >= min_ratio) and (r <= max_ratio)\n",
    "\n",
    "mask = [len_ratio_ok(s, t) for s, t in zip(df[\"hu\"], df[\"ro\"])]\n",
    "df = df[mask]\n",
    "\n",
    "print(\"After cleaning:\", len(df))\n",
    "\n",
    "# Split 80/10/10 (same logic as your script)\n",
    "train_df, test_val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(test_val_df, test_size=0.5, random_state=42)\n",
    "\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "train_df.to_csv(DATA_PROCESSED / \"train.csv\", index=False)\n",
    "val_df.to_csv(DATA_PROCESSED / \"val.csv\", index=False)\n",
    "test_df.to_csv(DATA_PROCESSED / \"test.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\",\n",
    "      (DATA_PROCESSED/\"train.csv\"),\n",
    "      (DATA_PROCESSED/\"val.csv\"),\n",
    "      (DATA_PROCESSED/\"test.csv\"))\n",
    "print(\"Sizes:\", len(train_df), len(val_df), len(test_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75edea19",
   "metadata": {},
   "source": [
    "## Re-running baseline evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1af32d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:551: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [23:22<00:00, 56.09s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline BLEU: 23.61\n",
      "Saved: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\\baseline_predictions_notebook.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
    "SRC_LANG = \"hun_Latn\"\n",
    "TGT_LANG = \"ron_Latn\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "test_df = pd.read_csv(DATA_PROCESSED / \"test.csv\").head(200)  # use 200 for a better signal\n",
    "src_sentences = test_df[\"hu\"].tolist()\n",
    "refs = test_df[\"ro\"].tolist()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "tokenizer.src_lang = SRC_LANG\n",
    "\n",
    "forced_id = tokenizer.lang_code_to_id[TGT_LANG] if hasattr(tokenizer, \"lang_code_to_id\") else tokenizer.convert_tokens_to_ids(TGT_LANG)\n",
    "\n",
    "def translate_batch(sentences, batch_size=8, max_len=512, num_beams=4):\n",
    "    hyps = []\n",
    "    for i in tqdm(range(0, len(sentences), batch_size)):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=forced_id,\n",
    "                max_length=max_len,\n",
    "                num_beams=num_beams,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        hyps.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "    return hyps\n",
    "\n",
    "baseline_hyps = translate_batch(src_sentences, num_beams=4)\n",
    "bleu = sacrebleu.corpus_bleu(baseline_hyps, [refs]).score\n",
    "print(\"Baseline BLEU:\", round(bleu, 2))\n",
    "\n",
    "pd.DataFrame({\"source\": src_sentences, \"reference\": refs, \"hypothesis\": baseline_hyps}) \\\n",
    "  .to_csv(OUTPUT_DIR / \"baseline_predictions_notebook.csv\", index=False)\n",
    "print(\"Saved:\", OUTPUT_DIR / \"baseline_predictions_notebook.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed7778d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not available: Helsinki-NLP/opus-mt-hu-ro | OSError\n",
      "Not available: Helsinki-NLP/opus-mt-ro-hu | OSError\n",
      "Chosen model: Helsinki-NLP/opus-mt-hu-en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:24<00:00, 2020.46 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 2026.35 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\RoG\\AppData\\Local\\Temp\\ipykernel_24200\\3687099608.py:113: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18750/18750 1:35:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.124100</td>\n",
       "      <td>4.837155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.807900</td>\n",
       "      <td>4.472488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.567900</td>\n",
       "      <td>4.242502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.453000</td>\n",
       "      <td>4.096498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.348400</td>\n",
       "      <td>3.986247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>4.205500</td>\n",
       "      <td>3.911134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>4.240700</td>\n",
       "      <td>3.863305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>4.187200</td>\n",
       "      <td>3.833374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>4.125500</td>\n",
       "      <td>3.817755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\other.py:1394: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 7488608e-62b0-4b4a-afd6-4e5641f0729b)') - silently ignoring the lookup for the file config.json in Helsinki-NLP/opus-mt-hu-en.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\save_and_load.py:295: UserWarning: Could not find a config file in Helsinki-NLP/opus-mt-hu-en - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\other.py:1394: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 6bcb5dce-39bd-478d-9735-be5584af4bf1)') - silently ignoring the lookup for the file config.json in Helsinki-NLP/opus-mt-hu-en.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\save_and_load.py:295: UserWarning: Could not find a config file in Helsinki-NLP/opus-mt-hu-en - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\other.py:1394: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: e5498515-5323-466c-9dd4-fb130c78c697)') - silently ignoring the lookup for the file config.json in Helsinki-NLP/opus-mt-hu-en.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\save_and_load.py:295: UserWarning: Could not find a config file in Helsinki-NLP/opus-mt-hu-en - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\other.py:1394: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: eec10271-0584-4077-aa65-2d86c8fc8270)') - silently ignoring the lookup for the file config.json in Helsinki-NLP/opus-mt-hu-en.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\save_and_load.py:295: UserWarning: Could not find a config file in Helsinki-NLP/opus-mt-hu-en - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\other.py:1394: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 0b4e3b3d-df78-4245-9332-fc6636de6f58)') - silently ignoring the lookup for the file config.json in Helsinki-NLP/opus-mt-hu-en.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\save_and_load.py:295: UserWarning: Could not find a config file in Helsinki-NLP/opus-mt-hu-en - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Saved to: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\opus_hu_ro_legal\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "CANDIDATES = [\n",
    "    # Try direct HU<->RO if available (best)\n",
    "    \"Helsinki-NLP/opus-mt-hu-ro\",\n",
    "    \"Helsinki-NLP/opus-mt-ro-hu\",\n",
    "    # Fallback: multi-step pivots (still useful as baseline) â€“ only if direct not available\n",
    "    # You can remove these if assignment requires direct HUâ†”RO model.\n",
    "    \"Helsinki-NLP/opus-mt-hu-en\",\n",
    "    \"Helsinki-NLP/opus-mt-en-ro\",\n",
    "]\n",
    "\n",
    "chosen = None\n",
    "for m in CANDIDATES:\n",
    "    try:\n",
    "        _ = AutoTokenizer.from_pretrained(m)\n",
    "        chosen = m\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Not available:\", m, \"|\", type(e).__name__)\n",
    "\n",
    "print(\"Chosen model:\", chosen)\n",
    "\n",
    "import os, gc, torch, pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "MODEL_NAME = chosen  # from previous cell\n",
    "\n",
    "# ---- settings ----\n",
    "max_len = 256        # opus models are smaller; can often handle 256\n",
    "batch_size = 8       # should fit on 6GB for small models; lower if needed\n",
    "grad_accum = 1\n",
    "train_n = 50000      # scale as you like\n",
    "val_n = 2000\n",
    "num_train_epochs = 3\n",
    "\n",
    "train_df = pd.read_csv(DATA_PROCESSED / \"train.csv\").sample(min(train_n, len(pd.read_csv(DATA_PROCESSED / \"train.csv\"))), random_state=42)\n",
    "val_df   = pd.read_csv(DATA_PROCESSED / \"val.csv\").head(val_n)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "val_ds   = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess(examples):\n",
    "    x = tokenizer(examples[\"hu\"], truncation=True, max_length=max_len)\n",
    "    y = tokenizer(text_target=examples[\"ro\"], truncation=True, max_length=max_len)\n",
    "    x[\"labels\"] = y[\"input_ids\"]\n",
    "    return x\n",
    "\n",
    "train_tok = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "val_tok   = val_ds.map(preprocess, batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else None\n",
    ").to(DEVICE)\n",
    "\n",
    "# Optional LoRA (usually not even necessary for small models, but helpful)\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Marian/opus often uses these; if it errors, remove LoRA entirely\n",
    ")\n",
    "\n",
    "try:\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    print(\"Using LoRA.\")\n",
    "except Exception as e:\n",
    "    print(\"LoRA failed, training full model instead:\", type(e).__name__, e)\n",
    "    model = base_model\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(PROJECT_ROOT / \"checkpoints\" / \"opus_hu_ro_legal\"),\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    fp16=(DEVICE == \"cuda\"),\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=2000,\n",
    "    save_steps=2000,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=False,\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=0,  # Windows-safe\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Done. Saved to:\", args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8171a881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\opus_hu_ro_legal\\checkpoint-18750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as full model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(62522, 512, padding_idx=62521)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(62522, 512, padding_idx=62521)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(62522, 512, padding_idx=62521)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=62522, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CKPT_DIR = Path(PROJECT_ROOT) / \"checkpoints\" / \"opus_hu_ro_legal\"\n",
    "\n",
    "# If you trained full model, trainer saves model weights directly in CKPT_DIR (or checkpoint-xxxx)\n",
    "# If you trained with LoRA, it may save adapters.\n",
    "# We'll load the latest checkpoint if present.\n",
    "def get_latest_checkpoint(folder: Path):\n",
    "    cks = sorted([p for p in folder.glob(\"checkpoint-*\") if p.is_dir()],\n",
    "                 key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else -1)\n",
    "    return cks[-1] if cks else folder\n",
    "\n",
    "LOAD_DIR = get_latest_checkpoint(CKPT_DIR)\n",
    "print(\"Loading from:\", LOAD_DIR)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOAD_DIR, use_fast=True)\n",
    "\n",
    "# Try load as a normal seq2seq model first\n",
    "try:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(LOAD_DIR).to(DEVICE)\n",
    "    print(\"Loaded as full model.\")\n",
    "except Exception as e:\n",
    "    print(\"Full-model load failed, trying LoRA adapter load:\", type(e).__name__, e)\n",
    "    # For LoRA: load base model then attach adapters\n",
    "    base = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    model = PeftModel.from_pretrained(base, LOAD_DIR).to(DEVICE)\n",
    "    print(\"Loaded as LoRA adapter on base model.\")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3519a1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 93\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hyps\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# ---- FAST ITERATION MODE ----\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Greedy (num_beams=1) is fastest + good for leak/glossary metrics.\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m hyps \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_batch_safe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# safe start; will auto-reduce if needed\u001b[39;49;00m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_input_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     99\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m pred_path \u001b[38;5;241m=\u001b[39m Path(PROJECT_ROOT) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopus_finetuned_predictions_greedy.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m pred_path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[25], line 50\u001b[0m, in \u001b[0;36mtranslate_batch_safe\u001b[1;34m(model, tokenizer, sentences, batch_size, max_input_len, max_new_tokens, num_beams)\u001b[0m\n\u001b[0;32m     41\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m     42\u001b[0m     batch,\n\u001b[0;32m     43\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_input_len,\n\u001b[0;32m     47\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m---> 50\u001b[0m     out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m     52\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mnum_beams,\n\u001b[0;32m     53\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     54\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[0;32m     55\u001b[0m         early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     56\u001b[0m     )\n\u001b[0;32m     58\u001b[0m hyps\u001b[38;5;241m.\u001b[39mextend(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(out, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     59\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bs\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot decorate classes; it is ambiguous whether or not only the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstructor or all methods should have the context manager applied; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindividually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misgeneratorfunction(func):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_generator(ctx_factory, func)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\generation\\utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2252\u001b[0m     )\n\u001b[0;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2256\u001b[0m         input_ids,\n\u001b[0;32m   2257\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2258\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2259\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2260\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2261\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2262\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2263\u001b[0m     )\n\u001b[0;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2275\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\generation\\utils.py:3243\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3240\u001b[0m         model_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_compiled_call(generation_config\u001b[38;5;241m.\u001b[39mcompile_config)\n\u001b[0;32m   3242\u001b[0m is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 3243\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[0;32m   3245\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   3246\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[0;32m   3247\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   3249\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\generation\\utils.py:2453\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[1;34m(self, this_peer_finished, synced_gpus, device, cur_len, max_length)\u001b[0m\n\u001b[0;32m   2451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m   2452\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- Load test ----\n",
    "test_df = pd.read_csv(DATA_PROCESSED / \"test.csv\")\n",
    "src_sentences = test_df[\"hu\"].tolist()\n",
    "refs = test_df[\"ro\"].tolist()\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA not available in this kernel.\"\n",
    "\n",
    "# ---- Put model on GPU ----\n",
    "model = model.to(\"cuda\")\n",
    "model.eval()\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "# ---- Make CUDA more stable after previous OOMs ----\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "\n",
    "# Optional: if you previously got OOMs, this helps avoid fragmentation\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def translate_batch_safe(\n",
    "    model, tokenizer, sentences,\n",
    "    batch_size=32,            # start conservative for 6GB\n",
    "    max_input_len=256,\n",
    "    max_new_tokens=96,\n",
    "    num_beams=1\n",
    "):\n",
    "    hyps = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(sentences):\n",
    "        bs = min(batch_size, len(sentences) - i)\n",
    "        batch = sentences[i:i+bs]\n",
    "\n",
    "        try:\n",
    "            inputs = tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_input_len,\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                out = model.generate(\n",
    "                    **inputs,\n",
    "                    num_beams=num_beams,\n",
    "                    do_sample=False,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    early_stopping=True,\n",
    "                )\n",
    "\n",
    "            hyps.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "            i += bs\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            torch.cuda.empty_cache()\n",
    "            if batch_size <= 1:\n",
    "                raise\n",
    "            batch_size = max(1, batch_size // 2)\n",
    "            print(f\"OOM -> reducing batch_size to {batch_size} and retrying...\")\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            # Catch cuBLAS internal error and similar CUDA runtime issues\n",
    "            msg = str(e)\n",
    "            if \"CUBLAS_STATUS_INTERNAL_ERROR\" in msg or \"CUDA error\" in msg:\n",
    "                torch.cuda.empty_cache()\n",
    "                if batch_size > 1:\n",
    "                    batch_size = max(1, batch_size // 2)\n",
    "                    print(f\"CUDA kernel error -> reducing batch_size to {batch_size} and retrying...\")\n",
    "                else:\n",
    "                    # If batch_size==1 and still failing, reduce lengths\n",
    "                    if max_input_len > 128:\n",
    "                        max_input_len = 128\n",
    "                        print(\"CUDA kernel error at batch_size=1 -> reducing max_input_len to 128 and retrying...\")\n",
    "                    elif max_new_tokens > 64:\n",
    "                        max_new_tokens = 64\n",
    "                        print(\"CUDA kernel error at batch_size=1 -> reducing max_new_tokens to 64 and retrying...\")\n",
    "                    else:\n",
    "                        raise\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    return hyps\n",
    "\n",
    "# ---- FAST ITERATION MODE ----\n",
    "# Greedy (num_beams=1) is fastest + good for leak/glossary metrics.\n",
    "hyps = translate_batch_safe(\n",
    "    model, tokenizer, src_sentences,\n",
    "    batch_size=32,         # safe start; will auto-reduce if needed\n",
    "    max_input_len=256,\n",
    "    max_new_tokens=96,\n",
    "    num_beams=1\n",
    ")\n",
    "\n",
    "pred_path = Path(PROJECT_ROOT) / \"data\" / \"outputs\" / \"opus_finetuned_predictions_greedy.csv\"\n",
    "pred_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.DataFrame({\"source\": src_sentences, \"reference\": refs, \"hypothesis\": hyps}).to_csv(pred_path, index=False)\n",
    "print(\"Saved predictions to:\", pred_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deb3cb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\RoG\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
      "INFO:comet.models.base:Encoder model frozen.\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "comet_model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "comet_model = load_from_checkpoint(comet_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a576c7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comet_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     scores \u001b[38;5;241m=\u001b[39m comet_model\u001b[38;5;241m.\u001b[39mpredict(data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(scores\u001b[38;5;241m.\u001b[39mscores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mscores)\n\u001b[1;32m----> 9\u001b[0m comet_score \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_comet\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_hyps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseline COMET:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(comet_score, \u001b[38;5;241m4\u001b[39m))\n",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m, in \u001b[0;36mcompute_comet\u001b[1;34m(src, hyp, ref, batch_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_comet\u001b[39m(src, hyp, ref, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m: s, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmt\u001b[39m\u001b[38;5;124m\"\u001b[39m: h, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m\"\u001b[39m: r}\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s, h, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(src, hyp, ref)\n\u001b[0;32m      5\u001b[0m     ]\n\u001b[1;32m----> 6\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mcomet_model\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(scores\u001b[38;5;241m.\u001b[39mscores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mscores)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comet_model' is not defined"
     ]
    }
   ],
   "source": [
    "def compute_comet(src, hyp, ref, batch_size=8):\n",
    "    data = [\n",
    "        {\"src\": s, \"mt\": h, \"ref\": r}\n",
    "        for s, h, r in zip(src, hyp, ref)\n",
    "    ]\n",
    "    scores = comet_model.predict(data, batch_size=batch_size, gpus=1 if torch.cuda.is_available() else 0)\n",
    "    return sum(scores.scores) / len(scores.scores)\n",
    "\n",
    "comet_score = compute_comet(src_sentences, baseline_hyps, refs)\n",
    "print(\"Baseline COMET:\", round(comet_score, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75c46b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 69/7592 [01:05<1:58:41,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\RoG\\AppData\\Local\\Temp\\ipykernel_24200\\260528873.py\", line 40, in <module>\n",
      "    finetuned_hyps = translate_batch(\n",
      "  File \"C:\\Users\\RoG\\AppData\\Local\\Temp\\ipykernel_24200\\260528873.py\", line 27, in translate_batch\n",
      "    out = model.generate(\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return _wrap_generator(ctx_factory, func)\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\generation\\utils.py\", line 2255, in generate\n",
      "    result = self._sample(\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\generation\\utils.py\", line 3257, in _sample\n",
      "    outputs = model_forward(**model_inputs, return_dict=True)\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "    self._backward_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    full_backward_hooks: list[Callable] = []\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py\", line 1401, in forward\n",
      "    outputs = self.model(\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "    self._backward_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    full_backward_hooks: list[Callable] = []\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py\", line 1195, in forward\n",
      "    decoder_outputs = self.decoder(\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "    self._backward_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    full_backward_hooks: list[Callable] = []\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py\", line 995, in forward\n",
      "    layer_outputs = decoder_layer(\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "    self._backward_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    full_backward_hooks: list[Callable] = []\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py\", line 405, in forward\n",
      "    hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "    self._backward_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    full_backward_hooks: list[Callable] = []\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py\", line 161, in forward\n",
      "    query_states = self.q_proj(hidden_states) * self.scaling\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "    self._backward_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    full_backward_hooks: list[Callable] = []\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\tuners\\lora\\layer.py\", line 807, in forward\n",
      "    result = result + lora_B(lora_A(dropout(x))) * scaling\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1511, in _wrapped_call_impl\n",
      "    self._backward_hooks.move_to_end(handle.id, last=False)  # type: ignore[attr-defined]\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1520, in _call_impl\n",
      "    full_backward_hooks: list[Callable] = []\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 116, in forward\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2142, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\executing\\executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# --- make sure we're on GPU ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "def translate_batch(model, tokenizer, sentences, batch_size=32, max_input_len=256, max_new_tokens=96, num_beams=1):\n",
    "    hyps = []\n",
    "    for i in tqdm(range(0, len(sentences), batch_size)):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_input_len\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                num_beams=num_beams,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "\n",
    "        hyps.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "    return hyps\n",
    "\n",
    "# âœ… SPEED MODE (recommended while iterating):\n",
    "# greedy decoding is much faster than beam=4\n",
    "finetuned_hyps = translate_batch(\n",
    "    model, tokenizer, src_sentences,\n",
    "    batch_size=4 if DEVICE==\"cuda\" else 4,\n",
    "    num_beams=1,          # change to 4 only for final scoring\n",
    "    max_new_tokens=96\n",
    ")\n",
    "\n",
    "bleu_ft = sacrebleu.corpus_bleu(finetuned_hyps, [refs]).score\n",
    "print(\"Finetuned BLEU:\", round(bleu_ft, 2))\n",
    "\n",
    "leak_rate_ft = np.mean([has_hu_diacritics(h) for h in finetuned_hyps])\n",
    "print(\"HU diacritics leak rate (FT):\", round(leak_rate_ft*100, 2), \"%\")\n",
    "\n",
    "all_hits_ft = [glossary_hit(s, h, glossary) for s, h in zip(src_sentences, finetuned_hyps)]\n",
    "flat_ft = [x for row in all_hits_ft for x in row]\n",
    "if flat_ft:\n",
    "    print(\"Glossary accuracy (FT):\", round(np.mean(flat_ft)*100, 2), \"%\")\n",
    "else:\n",
    "    print(\"No glossary terms found in sample.\")\n",
    "\n",
    "out_path = Path(OUTPUT_DIR) / \"finetuned_predictions.csv\"\n",
    "pd.DataFrame({\"source\": src_sentences, \"reference\": refs, \"hypothesis\": finetuned_hyps}).to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92ea00c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HU diacritics leak rate: 0.0 %\n",
      "Glossary accuracy (starter terms): 45.45 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# HU diacritics leak proxy (how often output still looks Hungarian)\n",
    "hu_diacritics = set(\"Ã¡Ã©Ã­Ã³Ã¶Å‘ÃºÃ¼Å±ÃÃ‰ÃÃ“Ã–ÅÃšÃœÅ°\")\n",
    "def has_hu_diacritics(s): \n",
    "    return any(c in hu_diacritics for c in s)\n",
    "\n",
    "leak_rate = np.mean([has_hu_diacritics(h) for h in baseline_hyps])\n",
    "print(\"HU diacritics leak rate:\", round(leak_rate*100, 2), \"%\")\n",
    "\n",
    "# Very small starter glossary for institutions/terms (expand later)\n",
    "glossary = {\n",
    "    \"BizottsÃ¡g\": [\"Comisia\"],\n",
    "    \"TanÃ¡cs\": [\"Consiliul\"],\n",
    "    \"KÃ¶zÃ¶ssÃ©g\": [\"Comunitatea\", \"ComunitÄƒÈ›ii\"],\n",
    "}\n",
    "\n",
    "def glossary_hit(src, hyp, glossary):\n",
    "    hits = []\n",
    "    for k, vals in glossary.items():\n",
    "        if k in src:\n",
    "            ok = any(v in hyp for v in vals)\n",
    "            hits.append(ok)\n",
    "    return hits\n",
    "\n",
    "all_hits = [glossary_hit(s, h, glossary) for s, h in zip(src_sentences, baseline_hyps)]\n",
    "flat = [x for row in all_hits for x in row]\n",
    "if flat:\n",
    "    print(\"Glossary accuracy (starter terms):\", round(np.mean(flat)*100, 2), \"%\")\n",
    "else:\n",
    "    print(\"No glossary terms found in sample.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "licenta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
