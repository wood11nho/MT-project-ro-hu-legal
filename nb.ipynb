{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af6f887c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bitsandbytes in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (0.49.1)\n",
      "Requirement already satisfied: torch<3,>=2.3 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from bitsandbytes) (2.9.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from bitsandbytes) (23.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.3)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from torch<3,>=2.3->bitsandbytes) (2024.3.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rog\\anaconda3\\envs\\licenta\\lib\\site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb39af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 21:41:46,849 - INFO - Zip file already exists.\n",
      "2026-01-12 21:41:46,850 - INFO - Extracting...\n",
      "2026-01-12 21:41:47,280 - INFO - Data successfully ready in D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\raw\n",
      "2026-01-12 21:41:47,281 - INFO - Files in data folder: ['hu-ro.txt.zip', 'JRC-Acquis.hu-ro.hu', 'JRC-Acquis.hu-ro.ro', 'JRC-Acquis.hu-ro.xml', 'LICENSE', 'README']\n"
     ]
    }
   ],
   "source": [
    "!python scripts/download_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb81f10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 21:42:55,001 - INFO - Loading JRC-Acquis.hu-ro.hu and JRC-Acquis.hu-ro.ro...\n",
      "2026-01-12 21:42:56,767 - INFO - Original: 417178 | Cleaned: 310067 | Removed: 107111\n",
      "2026-01-12 21:42:56,802 - INFO - Split Sizes -> Train: 248053, Val: 31007, Test: 31007\n",
      "2026-01-12 21:42:59,194 - INFO - SUCCESS! Splits saved to D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "!python scripts/prepare_splits.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38a10aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-12 21:47:32.958678: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-12 21:47:36.732384: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "Loading test data from D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\processed\\test.csv...\n",
      "Loading model facebook/nllb-200-distilled-600M on cuda...\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RoG\\.cache\\huggingface\\hub\\models--facebook--nllb-200-distilled-600M. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Translating...\n",
      "\n",
      "  0%|          | 0/13 [00:00<?, ?it/s]c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:551: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "\n",
      "  8%|â–Š         | 1/13 [00:03<00:47,  3.93s/it]\n",
      " 15%|â–ˆâ–Œ        | 2/13 [00:05<00:26,  2.45s/it]\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 3/13 [00:06<00:19,  1.93s/it]\n",
      " 31%|â–ˆâ–ˆâ–ˆ       | 4/13 [00:08<00:17,  2.00s/it]\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–Š      | 5/13 [00:10<00:14,  1.76s/it]\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 6/13 [00:11<00:10,  1.47s/it]\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 7/13 [00:15<00:13,  2.31s/it]\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 8/13 [00:16<00:09,  1.98s/it]\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 9/13 [00:21<00:11,  2.87s/it]\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 10/13 [00:22<00:07,  2.42s/it]\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 11/13 [00:25<00:04,  2.46s/it]\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 12/13 [00:25<00:01,  1.96s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:26<00:00,  1.58s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:26<00:00,  2.05s/it]\n",
      "--------------------------------------------------\n",
      "BASELINE BLEU SCORE: 8.26\n",
      "--------------------------------------------------\n",
      "Downloading/Loading COMET model (this might take a moment)...\n",
      "\n",
      "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RoG\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "\n",
      "Fetching 5 files:  20%|â–ˆâ–ˆ        | 1/5 [00:03<00:12,  3.17s/it]\n",
      "Fetching 5 files:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:03<00:04,  1.58s/it]\n",
      "COMET calculation failed: \"Model 'Unbabel/wmt22-comet-da' not supported by COMET.\"\n",
      "Continuing to save predictions despite COMET failure.\n",
      "Predictions saved to D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\\baseline_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "!python scripts/evaluate_baseline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fae2b2",
   "metadata": {},
   "source": [
    "## Downloading JRC-Acquis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d150060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\n",
      "DATA_RAW: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\raw\n",
      "DATA_PROCESSED: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\processed\n",
      "OUTPUT_DIR: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"data\" / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_RAW:\", DATA_RAW)\n",
    "print(\"DATA_PROCESSED:\", DATA_PROCESSED)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66c0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Zip already exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files: ['hu-ro.txt.zip', 'JRC-Acquis.hu-ro.hu', 'JRC-Acquis.hu-ro.ro', 'JRC-Acquis.hu-ro.xml', 'LICENSE', 'README']\n"
     ]
    }
   ],
   "source": [
    "import requests, zipfile, logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "OPUS_DIRECT_URL = \"https://object.pouta.csc.fi/OPUS-JRC-Acquis/v3.0/moses/hu-ro.txt.zip\"\n",
    "zip_path = DATA_RAW / \"hu-ro.txt.zip\"\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download the zip if not already present\n",
    "if not zip_path.exists():\n",
    "    logging.info(f\"Downloading {OPUS_DIRECT_URL}...\")\n",
    "    r = requests.get(OPUS_DIRECT_URL, stream=True, headers={\"User-Agent\":\"Mozilla/5.0\"})\n",
    "    r.raise_for_status()\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    logging.info(\"Download complete.\")\n",
    "else:\n",
    "    logging.info(\"Zip already exists.\")\n",
    "\n",
    "assert zipfile.is_zipfile(zip_path), \"Downloaded file is not a valid zip.\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "    z.extractall(DATA_RAW)\n",
    "\n",
    "print(\"Extracted files:\", [p.name for p in DATA_RAW.glob(\"*\")][:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c287252e",
   "metadata": {},
   "source": [
    "## Preparing splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bf14a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw pairs: 417178\n",
      "After cleaning: 303659\n",
      "Saved: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\processed\\train.csv D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\processed\\val.csv D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\processed\\test.csv\n",
      "Sizes: 242927 30366 30366\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "# Load raw parallel data\n",
    "def load_parallel_from_raw(raw_dir: Path):\n",
    "    hu_path = list(raw_dir.glob(\"*.hu\"))[0]\n",
    "    ro_path = list(raw_dir.glob(\"*.ro\"))[0]\n",
    "    with open(hu_path, \"r\", encoding=\"utf-8\") as f: hu = [x.strip() for x in f]\n",
    "    with open(ro_path, \"r\", encoding=\"utf-8\") as f: ro = [x.strip() for x in f]\n",
    "    assert len(hu) == len(ro)\n",
    "    return pd.DataFrame({\"hu\": hu, \"ro\": ro})\n",
    "\n",
    "df = load_parallel_from_raw(DATA_RAW)\n",
    "print(\"Raw pairs:\", len(df))\n",
    "\n",
    "# Basic cleaning\n",
    "df = df.drop_duplicates()\n",
    "df = df[df[\"hu\"].str.strip().astype(bool)]\n",
    "df = df[df[\"ro\"].str.strip().astype(bool)]\n",
    "\n",
    "# Length ratio filter \n",
    "def len_ratio_ok(s, t, min_ratio=0.5, max_ratio=2.0):\n",
    "    ls, lt = max(len(s), 1), max(len(t), 1)\n",
    "    r = ls / lt\n",
    "    return (r >= min_ratio) and (r <= max_ratio)\n",
    "\n",
    "mask = [len_ratio_ok(s, t) for s, t in zip(df[\"hu\"], df[\"ro\"])]\n",
    "df = df[mask]\n",
    "\n",
    "print(\"After cleaning:\", len(df))\n",
    "\n",
    "# Split 80/10/10 \n",
    "train_df, test_val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(test_val_df, test_size=0.5, random_state=42)\n",
    "\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "train_df.to_csv(DATA_PROCESSED / \"train.csv\", index=False)\n",
    "val_df.to_csv(DATA_PROCESSED / \"val.csv\", index=False)\n",
    "test_df.to_csv(DATA_PROCESSED / \"test.csv\", index=False)\n",
    "\n",
    "print(\"Saved:\",\n",
    "      (DATA_PROCESSED/\"train.csv\"),\n",
    "      (DATA_PROCESSED/\"val.csv\"),\n",
    "      (DATA_PROCESSED/\"test.csv\"))\n",
    "print(\"Sizes:\", len(train_df), len(val_df), len(test_df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75edea19",
   "metadata": {},
   "source": [
    "## Re-running baseline evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3912daa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForSeq2SeqLM\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msacrebleu\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\__init__.py:281\u001b[0m\n\u001b[0;32m    277\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    279\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 281\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\__init__.py:264\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    260\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    261\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    263\u001b[0m     )\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import sacrebleu\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af32d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\models\\m2m_100\\modeling_m2m_100.py:551: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [23:22<00:00, 56.09s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline BLEU: 23.61\n",
      "Saved: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\\baseline_predictions_notebook.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "\n",
    "MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
    "SRC_LANG = \"hun_Latn\"\n",
    "TGT_LANG = \"ron_Latn\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv(DATA_PROCESSED / \"test.csv\").head(200) \n",
    "src_sentences = test_df[\"hu\"].tolist()\n",
    "refs = test_df[\"ro\"].tolist()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME) # load tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE) # load model\n",
    "tokenizer.src_lang = SRC_LANG # load tokenizer\n",
    "\n",
    "# forcing the target language token\n",
    "forced_id = tokenizer.lang_code_to_id[TGT_LANG] if hasattr(tokenizer, \"lang_code_to_id\") else tokenizer.convert_tokens_to_ids(TGT_LANG)\n",
    "\n",
    "# Translation function\n",
    "def translate_batch(sentences, batch_size=8, max_len=512, num_beams=4):\n",
    "    hyps = []\n",
    "    for i in tqdm(range(0, len(sentences), batch_size)):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_len).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                forced_bos_token_id=forced_id,\n",
    "                max_length=max_len,\n",
    "                num_beams=num_beams,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        hyps.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "    return hyps\n",
    "\n",
    "# Baseline evaluation\n",
    "baseline_hyps = translate_batch(src_sentences, num_beams=4)\n",
    "bleu = sacrebleu.corpus_bleu(baseline_hyps, [refs]).score\n",
    "print(\"Baseline BLEU:\", round(bleu, 2))\n",
    "\n",
    "pd.DataFrame({\"source\": src_sentences, \"reference\": refs, \"hypothesis\": baseline_hyps}) \\\n",
    "  .to_csv(OUTPUT_DIR / \"baseline_predictions_notebook.csv\", index=False)\n",
    "print(\"Saved:\", OUTPUT_DIR / \"baseline_predictions_notebook.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7778d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not available: Helsinki-NLP/opus-mt-hu-ro | OSError\n",
      "Not available: Helsinki-NLP/opus-mt-ro-hu | OSError\n",
      "Chosen model: Helsinki-NLP/opus-mt-hu-en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [00:24<00:00, 2020.46 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:00<00:00, 2026.35 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LoRA.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\RoG\\AppData\\Local\\Temp\\ipykernel_24200\\3687099608.py:113: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18750' max='18750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18750/18750 1:35:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.124100</td>\n",
       "      <td>4.837155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>4.807900</td>\n",
       "      <td>4.472488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>4.567900</td>\n",
       "      <td>4.242502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>4.453000</td>\n",
       "      <td>4.096498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.348400</td>\n",
       "      <td>3.986247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>4.205500</td>\n",
       "      <td>3.911134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>4.240700</td>\n",
       "      <td>3.863305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>4.187200</td>\n",
       "      <td>3.833374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>4.125500</td>\n",
       "      <td>3.817755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\other.py:1394: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 7488608e-62b0-4b4a-afd6-4e5641f0729b)') - silently ignoring the lookup for the file config.json in Helsinki-NLP/opus-mt-hu-en.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\save_and_load.py:295: UserWarning: Could not find a config file in Helsinki-NLP/opus-mt-hu-en - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\other.py:1394: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 6bcb5dce-39bd-478d-9735-be5584af4bf1)') - silently ignoring the lookup for the file config.json in Helsinki-NLP/opus-mt-hu-en.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\save_and_load.py:295: UserWarning: Could not find a config file in Helsinki-NLP/opus-mt-hu-en - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\other.py:1394: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: e5498515-5323-466c-9dd4-fb130c78c697)') - silently ignoring the lookup for the file config.json in Helsinki-NLP/opus-mt-hu-en.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\save_and_load.py:295: UserWarning: Could not find a config file in Helsinki-NLP/opus-mt-hu-en - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\other.py:1394: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: eec10271-0584-4077-aa65-2d86c8fc8270)') - silently ignoring the lookup for the file config.json in Helsinki-NLP/opus-mt-hu-en.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\save_and_load.py:295: UserWarning: Could not find a config file in Helsinki-NLP/opus-mt-hu-en - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\other.py:1394: UserWarning: Unable to fetch remote file due to the following error (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 0b4e3b3d-df78-4245-9332-fc6636de6f58)') - silently ignoring the lookup for the file config.json in Helsinki-NLP/opus-mt-hu-en.\n",
      "  warnings.warn(\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\peft\\utils\\save_and_load.py:295: UserWarning: Could not find a config file in Helsinki-NLP/opus-mt-hu-en - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Saved to: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\opus_hu_ro_legal\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "CANDIDATES = [\n",
    "    # Trying direct HU<->RO if available \n",
    "    \"Helsinki-NLP/opus-mt-hu-ro\",\n",
    "    \"Helsinki-NLP/opus-mt-ro-hu\",\n",
    "    \"Helsinki-NLP/opus-mt-hu-en\",\n",
    "    \"Helsinki-NLP/opus-mt-en-ro\",\n",
    "]\n",
    "\n",
    "chosen = None\n",
    "for m in CANDIDATES:\n",
    "    try:\n",
    "        _ = AutoTokenizer.from_pretrained(m)\n",
    "        chosen = m\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Not available:\", m, \"|\", type(e).__name__)\n",
    "\n",
    "print(\"Chosen model:\", chosen)\n",
    "\n",
    "import os, gc, torch, pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "MODEL_NAME = chosen \n",
    "\n",
    "max_len = 256        \n",
    "batch_size = 8       \n",
    "grad_accum = 1\n",
    "train_n = 50000      \n",
    "val_n = 2000\n",
    "num_train_epochs = 3\n",
    "\n",
    "train_df = pd.read_csv(DATA_PROCESSED / \"train.csv\").sample(min(train_n, len(pd.read_csv(DATA_PROCESSED / \"train.csv\"))), random_state=42)\n",
    "val_df   = pd.read_csv(DATA_PROCESSED / \"val.csv\").head(val_n)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "val_ds   = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess(examples):\n",
    "    x = tokenizer(examples[\"hu\"], truncation=True, max_length=max_len)\n",
    "    y = tokenizer(text_target=examples[\"ro\"], truncation=True, max_length=max_len)\n",
    "    x[\"labels\"] = y[\"input_ids\"]\n",
    "    return x\n",
    "\n",
    "train_tok = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "val_tok   = val_ds.map(preprocess, batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else None\n",
    ").to(DEVICE)\n",
    "\n",
    "# Optional LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    ")\n",
    "\n",
    "try:\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    print(\"Using LoRA.\")\n",
    "except Exception as e:\n",
    "    print(\"LoRA failed, training full model instead:\", type(e).__name__, e)\n",
    "    model = base_model\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(PROJECT_ROOT / \"checkpoints\" / \"opus_hu_ro_legal\"),\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    fp16=(DEVICE == \"cuda\"),\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=2000,\n",
    "    save_steps=2000,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=False,\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=0,  \n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Done. Saved to:\", args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8171a881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\opus_hu_ro_legal\\checkpoint-18750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as full model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(62522, 512, padding_idx=62521)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(62522, 512, padding_idx=62521)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(62522, 512, padding_idx=62521)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (q_proj): lora.Linear(\n",
       "              (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (lora_dropout): ModuleDict(\n",
       "                (default): Dropout(p=0.05, inplace=False)\n",
       "              )\n",
       "              (lora_A): ModuleDict(\n",
       "                (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "              )\n",
       "              (lora_B): ModuleDict(\n",
       "                (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "              )\n",
       "              (lora_embedding_A): ParameterDict()\n",
       "              (lora_embedding_B): ParameterDict()\n",
       "              (lora_magnitude_vector): ModuleDict()\n",
       "            )\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=62522, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CKPT_DIR = Path(PROJECT_ROOT) / \"checkpoints\" / \"opus_hu_ro_legal\"\n",
    "\n",
    "def get_latest_checkpoint(folder: Path):\n",
    "    cks = sorted([p for p in folder.glob(\"checkpoint-*\") if p.is_dir()],\n",
    "                 key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else -1)\n",
    "    return cks[-1] if cks else folder\n",
    "\n",
    "LOAD_DIR = get_latest_checkpoint(CKPT_DIR)\n",
    "print(\"Loading from:\", LOAD_DIR)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LOAD_DIR, use_fast=True)\n",
    "\n",
    "try:\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(LOAD_DIR).to(DEVICE)\n",
    "    print(\"Loaded as full model.\")\n",
    "except Exception as e:\n",
    "    print(\"Full-model load failed, trying LoRA adapter load:\", type(e).__name__, e)\n",
    "    \n",
    "    base = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "    model = PeftModel.from_pretrained(base, LOAD_DIR).to(DEVICE)\n",
    "    print(\"Loaded as LoRA adapter on base model.\")\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3519a1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 93\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hyps\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# ---- FAST ITERATION MODE ----\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Greedy (num_beams=1) is fastest + good for leak/glossary metrics.\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m hyps \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_batch_safe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# safe start; will auto-reduce if needed\u001b[39;49;00m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_input_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m96\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m     99\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m pred_path \u001b[38;5;241m=\u001b[39m Path(PROJECT_ROOT) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopus_finetuned_predictions_greedy.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m pred_path\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[25], line 50\u001b[0m, in \u001b[0;36mtranslate_batch_safe\u001b[1;34m(model, tokenizer, sentences, batch_size, max_input_len, max_new_tokens, num_beams)\u001b[0m\n\u001b[0;32m     41\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m     42\u001b[0m     batch,\n\u001b[0;32m     43\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_input_len,\n\u001b[0;32m     47\u001b[0m )\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m---> 50\u001b[0m     out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m     52\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mnum_beams,\n\u001b[0;32m     53\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     54\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[0;32m     55\u001b[0m         early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     56\u001b[0m     )\n\u001b[0;32m     58\u001b[0m hyps\u001b[38;5;241m.\u001b[39mextend(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(out, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m     59\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bs\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot decorate classes; it is ambiguous whether or not only the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstructor or all methods should have the context manager applied; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindividually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m     )\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misgeneratorfunction(func):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_generator(ctx_factory, func)\n\u001b[0;32m    117\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\generation\\utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[0;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[0;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2252\u001b[0m     )\n\u001b[0;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[1;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[0;32m   2256\u001b[0m         input_ids,\n\u001b[0;32m   2257\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2258\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2259\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2260\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[0;32m   2261\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[0;32m   2262\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2263\u001b[0m     )\n\u001b[0;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[0;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[0;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[0;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   2275\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\generation\\utils.py:3243\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[0;32m   3240\u001b[0m         model_forward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_compiled_call(generation_config\u001b[38;5;241m.\u001b[39mcompile_config)\n\u001b[0;32m   3242\u001b[0m is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 3243\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\n\u001b[0;32m   3245\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   3246\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[0;32m   3247\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m   3249\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\transformers\\generation\\utils.py:2453\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[1;34m(self, this_peer_finished, synced_gpus, device, cur_len, max_length)\u001b[0m\n\u001b[0;32m   2451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m this_peer_finished_flag\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m   2452\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m this_peer_finished:\n\u001b[0;32m   2454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   2455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_df = pd.read_csv(DATA_PROCESSED / \"test.csv\")\n",
    "src_sentences = test_df[\"hu\"].tolist()\n",
    "refs = test_df[\"ro\"].tolist()\n",
    "\n",
    "assert torch.cuda.is_available(), \"CUDA not available in this kernel.\"\n",
    "\n",
    "model = model.to(\"cuda\")\n",
    "model.eval()\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "def translate_batch_safe(\n",
    "    model, tokenizer, sentences,\n",
    "    batch_size=32,           \n",
    "    max_input_len=256,\n",
    "    max_new_tokens=96,\n",
    "    num_beams=1\n",
    "):\n",
    "    hyps = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(sentences):\n",
    "        bs = min(batch_size, len(sentences) - i)\n",
    "        batch = sentences[i:i+bs]\n",
    "\n",
    "        try:\n",
    "            inputs = tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_input_len,\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                out = model.generate(\n",
    "                    **inputs,\n",
    "                    num_beams=num_beams,\n",
    "                    do_sample=False,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    early_stopping=True,\n",
    "                )\n",
    "\n",
    "            hyps.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "            i += bs\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            torch.cuda.empty_cache()\n",
    "            if batch_size <= 1:\n",
    "                raise\n",
    "            batch_size = max(1, batch_size // 2)\n",
    "            print(f\"OOM -> reducing batch_size to {batch_size} and retrying...\")\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            msg = str(e)\n",
    "            if \"CUBLAS_STATUS_INTERNAL_ERROR\" in msg or \"CUDA error\" in msg:\n",
    "                torch.cuda.empty_cache()\n",
    "                if batch_size > 1:\n",
    "                    batch_size = max(1, batch_size // 2)\n",
    "                    print(f\"CUDA kernel error -> reducing batch_size to {batch_size} and retrying...\")\n",
    "                else:\n",
    "                    if max_input_len > 128:\n",
    "                        max_input_len = 128\n",
    "                        print(\"CUDA kernel error at batch_size=1 -> reducing max_input_len to 128 and retrying...\")\n",
    "                    elif max_new_tokens > 64:\n",
    "                        max_new_tokens = 64\n",
    "                        print(\"CUDA kernel error at batch_size=1 -> reducing max_new_tokens to 64 and retrying...\")\n",
    "                    else:\n",
    "                        raise\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    return hyps\n",
    "\n",
    "hyps = translate_batch_safe(\n",
    "    model, tokenizer, src_sentences,\n",
    "    batch_size=32,         \n",
    "    max_input_len=256,\n",
    "    max_new_tokens=96,\n",
    "    num_beams=1\n",
    ")\n",
    "\n",
    "pred_path = Path(PROJECT_ROOT) / \"data\" / \"outputs\" / \"opus_finetuned_predictions_greedy.csv\"\n",
    "pred_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pd.DataFrame({\"source\": src_sentences, \"reference\": refs, \"hypothesis\": hyps}).to_csv(pred_path, index=False)\n",
    "print(\"Saved predictions to:\", pred_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deb3cb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\RoG\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
      "INFO:comet.models.base:Encoder model frozen.\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n"
     ]
    }
   ],
   "source": [
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "comet_model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "comet_model = load_from_checkpoint(comet_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a576c7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'comet_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     scores \u001b[38;5;241m=\u001b[39m comet_model\u001b[38;5;241m.\u001b[39mpredict(data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(scores\u001b[38;5;241m.\u001b[39mscores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mscores)\n\u001b[1;32m----> 9\u001b[0m comet_score \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_comet\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_sentences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_hyps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseline COMET:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(comet_score, \u001b[38;5;241m4\u001b[39m))\n",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m, in \u001b[0;36mcompute_comet\u001b[1;34m(src, hyp, ref, batch_size)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_comet\u001b[39m(src, hyp, ref, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m: s, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmt\u001b[39m\u001b[38;5;124m\"\u001b[39m: h, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m\"\u001b[39m: r}\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s, h, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(src, hyp, ref)\n\u001b[0;32m      5\u001b[0m     ]\n\u001b[1;32m----> 6\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mcomet_model\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(scores\u001b[38;5;241m.\u001b[39mscores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mscores)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'comet_model' is not defined"
     ]
    }
   ],
   "source": [
    "def compute_comet(src, hyp, ref, batch_size=8):\n",
    "    data = [\n",
    "        {\"src\": s, \"mt\": h, \"ref\": r}\n",
    "        for s, h, r in zip(src, hyp, ref)\n",
    "    ]\n",
    "    scores = comet_model.predict(data, batch_size=batch_size, gpus=1 if torch.cuda.is_available() else 0)\n",
    "    return sum(scores.scores) / len(scores.scores)\n",
    "\n",
    "comet_score = compute_comet(src_sentences, baseline_hyps, refs)\n",
    "print(\"Baseline COMET:\", round(comet_score, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c46b6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\__init__.py:281\u001b[0m\n\u001b[0;32m    277\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    279\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 281\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\__init__.py:264\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    260\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    261\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    263\u001b[0m     )\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "RUN_DIR = Path(PROJECT_ROOT) / \"checkpoints\" / \"opus_hu_ro_legal\"  \n",
    "\n",
    "def get_latest_checkpoint(run_dir: Path) -> Path:\n",
    "    ckpts = sorted(\n",
    "        [p for p in run_dir.glob(\"checkpoint-*\") if p.is_dir()],\n",
    "        key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else -1\n",
    "    )\n",
    "    return ckpts[-1] if ckpts else run_dir\n",
    "\n",
    "CKPT_DIR = get_latest_checkpoint(RUN_DIR)\n",
    "print(\"Using checkpoint:\", CKPT_DIR)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else None\n",
    ").to(DEVICE)\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, CKPT_DIR).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "test_df = pd.read_csv(DATA_PROCESSED / \"test.csv\")\n",
    "src_sentences = test_df[\"hu\"].tolist()\n",
    "refs = test_df[\"ro\"].tolist()\n",
    "\n",
    "\n",
    "def translate_batch(model, tokenizer, sentences, batch_size=32, max_input_len=256, max_new_tokens=96, num_beams=1):\n",
    "    hyps = []\n",
    "    for i in tqdm(range(0, len(sentences), batch_size)):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_input_len\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                num_beams=num_beams,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "\n",
    "        hyps.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "    return hyps\n",
    "\n",
    "\n",
    "finetuned_hyps = translate_batch(\n",
    "    model, tokenizer, src_sentences,\n",
    "    batch_size=16 if DEVICE==\"cuda\" else 4,  \n",
    "    num_beams=1,\n",
    "    max_new_tokens=96\n",
    ")\n",
    "\n",
    "\n",
    "bleu_ft = sacrebleu.corpus_bleu(finetuned_hyps, [refs]).score\n",
    "print(\"Finetuned BLEU:\", round(bleu_ft, 2))\n",
    "\n",
    "leak_rate_ft = np.mean([has_hu_diacritics(h) for h in finetuned_hyps])\n",
    "print(\"HU diacritics leak rate (FT):\", round(leak_rate_ft*100, 2), \"%\")\n",
    "\n",
    "all_hits_ft = [glossary_hit(s, h, glossary) for s, h in zip(src_sentences, finetuned_hyps)]\n",
    "flat_ft = [x for row in all_hits_ft for x in row]\n",
    "if flat_ft:\n",
    "    print(\"Glossary accuracy (FT):\", round(np.mean(flat_ft)*100, 2), \"%\")\n",
    "else:\n",
    "    print(\"No glossary terms found in sample.\")\n",
    "\n",
    "\n",
    "out_path = Path(OUTPUT_DIR) / f\"finetuned_predictions_lora_{CKPT_DIR.name}.csv\"\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "pd.DataFrame({\"source\": src_sentences, \"reference\": refs, \"hypothesis\": finetuned_hyps}).to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92ea00c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HU diacritics leak rate: 0.0 %\n",
      "Glossary accuracy (starter terms): 45.45 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# HU diacritics leak proxy (how often output still looks Hungarian)\n",
    "hu_diacritics = set(\"Ã¡Ã©Ã­Ã³Ã¶Å‘ÃºÃ¼Å±ÃÃ‰ÃÃ“Ã–ÅÃšÃœÅ°\")\n",
    "def has_hu_diacritics(s): \n",
    "    return any(c in hu_diacritics for c in s)\n",
    "\n",
    "leak_rate = np.mean([has_hu_diacritics(h) for h in baseline_hyps])\n",
    "print(\"HU diacritics leak rate:\", round(leak_rate*100, 2), \"%\")\n",
    "\n",
    "# Very small starter glossary for institutions/terms (expand later)\n",
    "glossary = {\n",
    "    \"BizottsÃ¡g\": [\"Comisia\"],\n",
    "    \"TanÃ¡cs\": [\"Consiliul\"],\n",
    "    \"KÃ¶zÃ¶ssÃ©g\": [\"Comunitatea\", \"ComunitÄƒÈ›ii\"],\n",
    "}\n",
    "\n",
    "def glossary_hit(src, hyp, glossary):\n",
    "    hits = []\n",
    "    for k, vals in glossary.items():\n",
    "        if k in src:\n",
    "            ok = any(v in hyp for v in vals)\n",
    "            hits.append(ok)\n",
    "    return hits\n",
    "\n",
    "all_hits = [glossary_hit(s, h, glossary) for s, h in zip(src_sentences, baseline_hyps)]\n",
    "flat = [x for row in all_hits for x in row]\n",
    "if flat:\n",
    "    print(\"Glossary accuracy (starter terms):\", round(np.mean(flat)*100, 2), \"%\")\n",
    "else:\n",
    "    print(\"No glossary terms found in sample.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b222b15e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mgc\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mmath\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\__init__.py:281\u001b[0m\n\u001b[0;32m    277\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    279\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 281\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\__init__.py:264\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    260\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    261\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    263\u001b[0m     )\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\RoG\\anaconda3\\envs\\licenta\\lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import os, gc, math, re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "# Pick best available HU<->RO model (prefer HU->RO)\n",
    "CANDIDATES = [\n",
    "    \"Helsinki-NLP/opus-mt-hu-ro\",\n",
    "    \"Helsinki-NLP/opus-mt-ro-hu\",\n",
    "]\n",
    "\n",
    "chosen = None\n",
    "for m in CANDIDATES:\n",
    "    try:\n",
    "        _ = AutoTokenizer.from_pretrained(m)\n",
    "        chosen = m\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Not available:\", m, \"|\", type(e).__name__)\n",
    "\n",
    "if chosen is None:\n",
    "    raise RuntimeError(\"No HU<->RO OPUS model available. You must use a pivot or different base.\")\n",
    "\n",
    "MODEL_NAME = chosen\n",
    "print(\"Chosen model:\", MODEL_NAME)\n",
    "\n",
    "\n",
    "REVERSE = (MODEL_NAME.endswith(\"ro-hu\"))\n",
    "print(\"REVERSE (means base is RO->HU):\", REVERSE)\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "# Load data\n",
    "train_path = DATA_PROCESSED / \"train.csv\"\n",
    "val_path   = DATA_PROCESSED / \"val.csv\"\n",
    "\n",
    "train_full = pd.read_csv(train_path)\n",
    "val_full   = pd.read_csv(val_path)\n",
    "\n",
    "# Curriculum: oversample \"legal-heavy\" lines early\n",
    "\n",
    "LEGAL_MARKERS_RO = [\n",
    "    \"Regulamentul\", \"Directiva\", \"articol\", \"alineat\", \"considerent\",\n",
    "    \"Comisia\", \"Consiliul\", \"Parlamentul\", \"Uniunii\", \"statele membre\"\n",
    "]\n",
    "LEGAL_MARKERS_HU = [\n",
    "    \"rendelet\", \"irÃ¡nyelv\", \"cikk\", \"bekezdÃ©s\",\n",
    "    \"BizottsÃ¡g\", \"TanÃ¡cs\", \"Parlament\", \"UniÃ³\", \"tagÃ¡llam\"\n",
    "]\n",
    "\n",
    "def is_legalish(row):\n",
    "    hu = str(row[\"hu\"])\n",
    "    ro = str(row[\"ro\"])\n",
    "    hu_hit = any(m in hu for m in LEGAL_MARKERS_HU)\n",
    "    ro_hit = any(m in ro for m in LEGAL_MARKERS_RO)\n",
    "    return hu_hit or ro_hit\n",
    "\n",
    "# pick sizes \n",
    "train_n = min(50000, len(train_full))\n",
    "val_n   = min(2000, len(val_full))\n",
    "\n",
    "train_sample = train_full.sample(train_n, random_state=42)\n",
    "\n",
    "# make a curriculum subset: legalish + some random\n",
    "legal_part = train_sample[train_sample.apply(is_legalish, axis=1)]\n",
    "rand_part  = train_sample.sample(min(len(train_sample), max(5000, train_n // 5)), random_state=43)\n",
    "cur_df = pd.concat([legal_part, rand_part], ignore_index=True).drop_duplicates()\n",
    "\n",
    "# final train is curriculum first then rest\n",
    "train_df = pd.concat([cur_df, train_sample], ignore_index=True).drop_duplicates()\n",
    "val_df   = val_full.head(val_n)\n",
    "\n",
    "print(\"Train size:\", len(train_df), \"| curriculum chunk:\", len(cur_df), \"| Val size:\", len(val_df))\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "val_ds   = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "\n",
    "# Tokenizer + preprocessing\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "max_src_len = 256\n",
    "max_tgt_len = 256\n",
    "\n",
    "def preprocess(examples):\n",
    "\n",
    "    src_texts = examples[\"hu\"]\n",
    "    tgt_texts = examples[\"ro\"]\n",
    "\n",
    "\n",
    "    if REVERSE:\n",
    "        pass\n",
    "\n",
    "    model_inputs = tokenizer(src_texts, truncation=True, max_length=max_src_len)\n",
    "\n",
    "    labels = tokenizer(text_target=tgt_texts, truncation=True, max_length=max_tgt_len)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_tok = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "val_tok   = val_ds.map(preprocess, batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else None\n",
    ").to(DEVICE)\n",
    "\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "if hasattr(base_model, \"gradient_checkpointing_enable\"):\n",
    "    base_model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "model = base_model\n",
    "using_lora = False\n",
    "try:\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    using_lora = True\n",
    "    print(\"Using LoRA.\")\n",
    "except Exception as e:\n",
    "    print(\"LoRA not compatible here; training full model instead:\", type(e).__name__, e)\n",
    "    model = base_model\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "\n",
    "batch_size = 4 if DEVICE == \"cuda\" else 2\n",
    "grad_accum = 4 if DEVICE == \"cuda\" else 8  \n",
    "num_train_epochs = 2                       \n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(PROJECT_ROOT / \"checkpoints\" / \"opus_hu_ro_legal_direct\"),\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    learning_rate=3e-4 if using_lora else 5e-5,  \n",
    "    num_train_epochs=num_train_epochs,\n",
    "    fp16=(DEVICE == \"cuda\"),\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=0,  \n",
    "    label_smoothing_factor=0.1,  \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01 if not using_lora else 0.0,\n",
    "    predict_with_generate=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Done. Saved to:\", args.output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "licenta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
