{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0265c774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\RoG\\\\anaconda3\\\\envs\\\\pythonRL\\\\Lib\\\\site-packages\\\\torch\\\\lib\\\\c10_cuda.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip -q install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04be83b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\n",
      "DATA_RAW: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\raw\n",
      "DATA_PROCESSED: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\processed\n",
      "OUTPUT_DIR: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"data\" / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-hu-en\"\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_RAW:\", DATA_RAW)\n",
    "print(\"DATA_PROCESSED:\", DATA_PROCESSED)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cd6b53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version (torch): 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version (torch):\", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b644474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# HU diacritics leak proxy (how often output still looks Hungarian)\n",
    "hu_diacritics = set(\"áéíóöőúüűÁÉÍÓÖŐÚÜŰ\")\n",
    "def has_hu_diacritics(s): \n",
    "    return any(c in hu_diacritics for c in s)\n",
    "\n",
    "\n",
    "# Very small starter glossary for institutions/terms (expand later)\n",
    "glossary = {\n",
    "    \"Bizottság\": [\"Comisia\"],\n",
    "    \"Tanács\": [\"Consiliul\"],\n",
    "    \"Közösség\": [\"Comunitatea\", \"Comunității\"],\n",
    "}\n",
    "\n",
    "def glossary_hit(src, hyp, glossary):\n",
    "    hits = []\n",
    "    for k, vals in glossary.items():\n",
    "        if k in src:\n",
    "            ok = any(v in hyp for v in vals)\n",
    "            hits.append(ok)\n",
    "    return hits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6370c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\opus_hu_ro_legal\\checkpoint-18750\n",
      "DEVICE: cuda\n",
      "Torch: 2.5.1+cu121\n",
      "Base safetensors already exists: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\safetensors_bases\\Helsinki-NLP__opus-mt-hu-en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter safetensors already exists: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\opus_hu_ro_legal\\checkpoint-18750\\adapter_model.safetensors\n",
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1898/1898 [30:52<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned BLEU: 10.01\n",
      "HU diacritics leak rate (FT): 0.81 %\n",
      "Glossary accuracy (FT): 36.38 %\n",
      "Saved: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\\finetuned_predictions_lora_checkpoint-18750.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors.torch import save_file\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# -------------------------\n",
    "# 0) Paths + find last checkpoint\n",
    "# -------------------------\n",
    "RUN_DIR = Path(PROJECT_ROOT) / \"checkpoints\" / \"opus_hu_ro_legal\"   # adjust if needed\n",
    "\n",
    "def get_latest_checkpoint(run_dir: Path) -> Path:\n",
    "    ckpts = sorted(\n",
    "        [p for p in run_dir.glob(\"checkpoint-*\") if p.is_dir()],\n",
    "        key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else -1\n",
    "    )\n",
    "    return ckpts[-1] if ckpts else run_dir\n",
    "\n",
    "CKPT_DIR = get_latest_checkpoint(RUN_DIR)\n",
    "print(\"Using checkpoint:\", CKPT_DIR)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "# -------------------------\n",
    "# 1) Convert BASE model to safetensors locally (if needed)\n",
    "# -------------------------\n",
    "def ensure_base_safetensors(model_id: str, out_root: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Creates a local folder containing config + model.safetensors for the base model,\n",
    "    without calling transformers.from_pretrained on .bin (avoids torch>=2.6 gate).\n",
    "    \"\"\"\n",
    "    out_dir = out_root / model_id.replace(\"/\", \"__\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # If already converted, reuse\n",
    "    if (out_dir / \"model.safetensors\").exists() and (out_dir / \"config.json\").exists():\n",
    "        print(\"Base safetensors already exists:\", out_dir)\n",
    "        return out_dir\n",
    "\n",
    "    print(\"Downloading base model snapshot:\", model_id)\n",
    "    snap_dir = Path(snapshot_download(repo_id=model_id))\n",
    "\n",
    "    # Locate weights\n",
    "    bin_path = snap_dir / \"pytorch_model.bin\"\n",
    "    if not bin_path.exists():\n",
    "        # sometimes sharded\n",
    "        shards = sorted(snap_dir.glob(\"pytorch_model-*.bin\"))\n",
    "        if not shards:\n",
    "            raise FileNotFoundError(\"Could not find pytorch_model.bin or shards in snapshot.\")\n",
    "        # Merge shards by loading into model later (transformers can merge but would hit restriction)\n",
    "        # We'll handle common non-sharded case; if sharded, tell user.\n",
    "        raise RuntimeError(\n",
    "            f\"Found sharded weights ({len(shards)} files). \"\n",
    "            \"This quick converter handles non-sharded pytorch_model.bin. \"\n",
    "            \"Tell me and I’ll give you the sharded merge converter.\"\n",
    "        )\n",
    "\n",
    "    # Load config normally (safe)\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "    # Load state dict via torch.load (allowed on torch 2.5)\n",
    "    print(\"Loading base .bin weights with torch.load:\", bin_path)\n",
    "    state = torch.load(bin_path, map_location=\"cpu\")\n",
    "    missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "    print(f\"Loaded base weights. missing={len(missing)} unexpected={len(unexpected)}\")\n",
    "\n",
    "    # Save safetensors + config\n",
    "    print(\"Saving base model as safetensors to:\", out_dir)\n",
    "    model.save_pretrained(out_dir, safe_serialization=True)\n",
    "    config.save_pretrained(out_dir)\n",
    "\n",
    "    return out_dir\n",
    "\n",
    "BASE_SAFE_ROOT = Path(PROJECT_ROOT) / \"safetensors_bases\"\n",
    "BASE_SAFE_DIR = ensure_base_safetensors(MODEL_NAME, BASE_SAFE_ROOT)\n",
    "\n",
    "# Tokenizer can be loaded from the original model id (safe), or from BASE_SAFE_DIR\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "# -------------------------\n",
    "# 2) Convert LoRA adapter_model.bin -> adapter_model.safetensors (if needed)\n",
    "# -------------------------\n",
    "def ensure_adapter_safetensors(ckpt_dir: Path):\n",
    "    bin_path = ckpt_dir / \"adapter_model.bin\"\n",
    "    st_path  = ckpt_dir / \"adapter_model.safetensors\"\n",
    "\n",
    "    if st_path.exists():\n",
    "        print(\"Adapter safetensors already exists:\", st_path)\n",
    "        return\n",
    "\n",
    "    if not bin_path.exists():\n",
    "        raise FileNotFoundError(f\"adapter_model.bin not found in {ckpt_dir}\")\n",
    "\n",
    "    print(\"Converting adapter bin -> safetensors:\", bin_path)\n",
    "    adapter_state = torch.load(bin_path, map_location=\"cpu\")\n",
    "    save_file(adapter_state, str(st_path))\n",
    "    print(\"Wrote:\", st_path)\n",
    "\n",
    "ensure_adapter_safetensors(CKPT_DIR)\n",
    "\n",
    "# -------------------------\n",
    "# 3) Load base model from local safetensors + attach LoRA\n",
    "# -------------------------\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    BASE_SAFE_DIR,\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else None,\n",
    ").to(DEVICE)\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, CKPT_DIR).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Load test data\n",
    "# -------------------------\n",
    "test_df = pd.read_csv(DATA_PROCESSED / \"test.csv\")\n",
    "src_sentences = test_df[\"hu\"].tolist()\n",
    "refs = test_df[\"ro\"].tolist()\n",
    "\n",
    "# -------------------------\n",
    "# 5) Translate\n",
    "# -------------------------\n",
    "def translate_batch(model, tokenizer, sentences, batch_size=16, max_input_len=256, max_new_tokens=96, num_beams=1):\n",
    "    hyps = []\n",
    "    for i in tqdm(range(0, len(sentences), batch_size)):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_input_len\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                num_beams=num_beams,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "        hyps.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "    return hyps\n",
    "\n",
    "# Greedy for speed (set num_beams=4 for final run)\n",
    "finetuned_hyps = translate_batch(\n",
    "    model, tokenizer, src_sentences,\n",
    "    batch_size=16 if DEVICE==\"cuda\" else 4,\n",
    "    num_beams=1,\n",
    "    max_new_tokens=96\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 6) Metrics\n",
    "# -------------------------\n",
    "bleu_ft = sacrebleu.corpus_bleu(finetuned_hyps, [refs]).score\n",
    "print(\"Finetuned BLEU:\", round(bleu_ft, 2))\n",
    "\n",
    "leak_rate_ft = np.mean([has_hu_diacritics(h) for h in finetuned_hyps])\n",
    "print(\"HU diacritics leak rate (FT):\", round(leak_rate_ft*100, 2), \"%\")\n",
    "\n",
    "all_hits_ft = [glossary_hit(s, h, glossary) for s, h in zip(src_sentences, finetuned_hyps)]\n",
    "flat_ft = [x for row in all_hits_ft for x in row]\n",
    "if flat_ft:\n",
    "    print(\"Glossary accuracy (FT):\", round(np.mean(flat_ft)*100, 2), \"%\")\n",
    "else:\n",
    "    print(\"No glossary terms found in sample.\")\n",
    "\n",
    "# -------------------------\n",
    "# 7) Save outputs\n",
    "# -------------------------\n",
    "out_path = Path(OUTPUT_DIR) / f\"finetuned_predictions_lora_{CKPT_DIR.name}.csv\"\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "pd.DataFrame({\"source\": src_sentences, \"reference\": refs, \"hypothesis\": finetuned_hyps}).to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd2dae37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 2183fc6f-a243-4f41-99b8-f34aac5e98f7)')' thrown while requesting HEAD https://huggingface.co/Helsinki-NLP/opus-mt-hu-en/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Torch: 2.5.1+cu121\n",
      "Using checkpoint: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\opus_hu_ro_legal\\checkpoint-18750\n",
      "Base safetensors already exists: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\safetensors_bases\\Helsinki-NLP__opus-mt-hu-en\n",
      "Adapter safetensors already exists: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\opus_hu_ro_legal\\checkpoint-18750\\adapter_model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Test size: 30366\n",
      "\n",
      "=== GREEDY RESULTS ===\n",
      "BLEU: 10.01\n",
      "HU diacritics leak %: 0.81\n",
      "English leak proxy %: 6.45\n",
      "Glossary accuracy %: 40.57\n",
      "Saved: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\\finetuned_predictions_lora_checkpoint-18750_greedy.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 279\u001b[0m\n\u001b[0;32m    277\u001b[0m cands \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lp \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.2\u001b[39m]:\n\u001b[1;32m--> 279\u001b[0m     hyps_b \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_bs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_input_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_INPUT_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlp\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m     bleu_b \u001b[38;5;241m=\u001b[39m sacrebleu\u001b[38;5;241m.\u001b[39mcorpus_bleu(hyps_b, [refs])\u001b[38;5;241m.\u001b[39mscore\n\u001b[0;32m    288\u001b[0m     cands\u001b[38;5;241m.\u001b[39mappend((bleu_b, lp, hyps_b))\n",
      "Cell \u001b[1;32mIn[7], line 220\u001b[0m, in \u001b[0;36mtranslate_batch\u001b[1;34m(model, tokenizer, sentences, batch_size, max_input_len, max_new_tokens, num_beams, length_penalty)\u001b[0m\n\u001b[0;32m    211\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m    212\u001b[0m     batch,\n\u001b[0;32m    213\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_input_len\n\u001b[0;32m    217\u001b[0m )\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m--> 220\u001b[0m     out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m    222\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mnum_beams,\n\u001b[0;32m    223\u001b[0m         length_penalty\u001b[38;5;241m=\u001b[39mlength_penalty,\n\u001b[0;32m    224\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    225\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[0;32m    226\u001b[0m         early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    227\u001b[0m     )\n\u001b[0;32m    229\u001b[0m hyps\u001b[38;5;241m.\u001b[39mextend(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(out, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    230\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bs\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\peft\\peft_model.py:2374\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.generate\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   2372\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2373\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 2374\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\generation\\utils.py:2566\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2563\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[0;32m   2565\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[1;32m-> 2566\u001b[0m result \u001b[38;5;241m=\u001b[39m decoding_method(\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2568\u001b[0m     input_ids,\n\u001b[0;32m   2569\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2570\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2571\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2572\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_mode_kwargs,\n\u001b[0;32m   2573\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2574\u001b[0m )\n\u001b[0;32m   2576\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[0;32m   2577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2578\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2579\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2580\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2581\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\generation\\utils.py:3267\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3264\u001b[0m flat_running_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten_beam_dim(running_sequences[:, :, :cur_len])\n\u001b[0;32m   3265\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(flat_running_sequences, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> 3267\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3269\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3270\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3271\u001b[0m     model_outputs,\n\u001b[0;32m   3272\u001b[0m     model_kwargs,\n\u001b[0;32m   3273\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3274\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:1522\u001b[0m, in \u001b[0;36mMarianMTModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1518\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1519\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1520\u001b[0m         )\n\u001b[1;32m-> 1522\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1540\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[0;32m   1542\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:1278\u001b[0m, in \u001b[0;36mMarianModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1271\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1272\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1273\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1274\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1275\u001b[0m     )\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1278\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:1069\u001b[0m, in \u001b[0;36mMarianDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropout_probability \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerdrop:\n\u001b[0;32m   1067\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1069\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[0;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1081\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:435\u001b[0m, in \u001b[0;36mMarianDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_values, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m--> 435\u001b[0m     hidden_states, cross_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    445\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:270\u001b[0m, in \u001b[0;36mMarianAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_values, attention_mask, layer_head_mask, output_attentions, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    258\u001b[0m     query_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    267\u001b[0m )\n\u001b[0;32m    269\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m--> 270\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# FULL IMPROVED EVAL + PREDICTION CODE (Option C: safetensors; no torch>=2.6 needed)\n",
    "# - Loads latest LoRA checkpoint\n",
    "# - Uses safetensors base + adapter (converts if needed; handles sharded base weights)\n",
    "# - Fast greedy decode + optional beam search sweep for best BLEU\n",
    "# - Computes BLEU, HU-diacritics leak, English-leak proxy, normalized glossary accuracy\n",
    "# - Saves predictions CSVs\n",
    "\n",
    "import os, re, json, unicodedata\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors.torch import save_file\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# -------------------------\n",
    "# 0) CONFIG (SET THESE)\n",
    "# -------------------------\n",
    "# Base model you used during fine-tuning:\n",
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-hu-en\"   # <-- keep consistent with training\n",
    "\n",
    "RUN_DIR    = Path(PROJECT_ROOT) / \"checkpoints\" / \"opus_hu_ro_legal\"\n",
    "OUT_DIR    = Path(OUTPUT_DIR)\n",
    "BASE_SAFE_ROOT = Path(PROJECT_ROOT) / \"safetensors_bases\"\n",
    "\n",
    "# decode defaults\n",
    "MAX_INPUT_LEN  = 256\n",
    "MAX_NEW_TOKENS = 96\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# -------------------------\n",
    "# 1) Find latest checkpoint\n",
    "# -------------------------\n",
    "def get_latest_checkpoint(run_dir: Path) -> Path:\n",
    "    ckpts = sorted(\n",
    "        [p for p in run_dir.glob(\"checkpoint-*\") if p.is_dir()],\n",
    "        key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else -1\n",
    "    )\n",
    "    return ckpts[-1] if ckpts else run_dir\n",
    "\n",
    "CKPT_DIR = get_latest_checkpoint(RUN_DIR)\n",
    "print(\"Using checkpoint:\", CKPT_DIR)\n",
    "\n",
    "# -------------------------\n",
    "# 2) Option C: ensure BASE safetensors exists (supports sharded weights)\n",
    "# -------------------------\n",
    "def ensure_base_safetensors(model_id: str, out_root: Path) -> Path:\n",
    "    out_dir = out_root / model_id.replace(\"/\", \"__\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if (out_dir / \"model.safetensors\").exists() and (out_dir / \"config.json\").exists():\n",
    "        print(\"Base safetensors already exists:\", out_dir)\n",
    "        return out_dir\n",
    "\n",
    "    print(\"Downloading base model snapshot:\", model_id)\n",
    "    snap_dir = Path(snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        allow_patterns=[\n",
    "            \"config.json\",\n",
    "            \"tokenizer.json\",\n",
    "            \"tokenizer_config.json\",\n",
    "            \"source.spm\",\n",
    "            \"vocab.json\", \"merges.txt\",\n",
    "            \"pytorch_model.bin\",\n",
    "            \"pytorch_model.bin.index.json\",\n",
    "            \"pytorch_model-*.bin\",\n",
    "        ],\n",
    "        ignore_patterns=[\"*.msgpack\", \"*.h5\", \"*.ot\", \"*.tflite\", \"*.onnx\"]\n",
    "    ))\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "    # Case 1: single weight file\n",
    "    bin_path = snap_dir / \"pytorch_model.bin\"\n",
    "    if bin_path.exists():\n",
    "        print(\"Found single pytorch_model.bin; loading with torch.load:\", bin_path)\n",
    "        state = torch.load(bin_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state, strict=False)\n",
    "        del state\n",
    "    else:\n",
    "        # Case 2: sharded weights (index.json + shard bins)\n",
    "        index_path = snap_dir / \"pytorch_model.bin.index.json\"\n",
    "        if not index_path.exists():\n",
    "            shards = sorted(snap_dir.glob(\"pytorch_model-*.bin\"))\n",
    "            raise FileNotFoundError(\n",
    "                f\"Could not find pytorch_model.bin or pytorch_model.bin.index.json.\\nFound {len(shards)} shard files.\"\n",
    "            )\n",
    "\n",
    "        with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            index = json.load(f)\n",
    "        shard_files = sorted(set(index[\"weight_map\"].values()))\n",
    "        print(f\"Found sharded weights: {len(shard_files)} shards\")\n",
    "\n",
    "        for sf in shard_files:\n",
    "            sp = snap_dir / sf\n",
    "            if not sp.exists():\n",
    "                raise FileNotFoundError(f\"Missing shard file: {sp}\")\n",
    "            shard_state = torch.load(sp, map_location=\"cpu\")\n",
    "            model.load_state_dict(shard_state, strict=False)\n",
    "            del shard_state\n",
    "\n",
    "    print(\"Saving base model as safetensors to:\", out_dir)\n",
    "    model.save_pretrained(out_dir, safe_serialization=True)\n",
    "    config.save_pretrained(out_dir)\n",
    "    return out_dir\n",
    "\n",
    "BASE_SAFE_DIR = ensure_base_safetensors(MODEL_NAME, BASE_SAFE_ROOT)\n",
    "\n",
    "# -------------------------\n",
    "# 3) Ensure adapter safetensors exists\n",
    "# -------------------------\n",
    "def ensure_adapter_safetensors(ckpt_dir: Path):\n",
    "    bin_path = ckpt_dir / \"adapter_model.bin\"\n",
    "    st_path  = ckpt_dir / \"adapter_model.safetensors\"\n",
    "\n",
    "    if st_path.exists():\n",
    "        print(\"Adapter safetensors already exists:\", st_path)\n",
    "        return\n",
    "\n",
    "    if not bin_path.exists():\n",
    "        raise FileNotFoundError(f\"adapter_model.bin not found in {ckpt_dir}\")\n",
    "\n",
    "    print(\"Converting adapter bin -> safetensors:\", bin_path)\n",
    "    adapter_state = torch.load(bin_path, map_location=\"cpu\")\n",
    "    save_file(adapter_state, str(st_path))\n",
    "    del adapter_state\n",
    "    print(\"Wrote:\", st_path)\n",
    "\n",
    "ensure_adapter_safetensors(CKPT_DIR)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Load tokenizer + base (safetensors) + attach LoRA\n",
    "# -------------------------\n",
    "# Tokenizer from HF id (safe)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    BASE_SAFE_DIR,\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else None,\n",
    ").to(DEVICE)\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, CKPT_DIR).to(DEVICE)\n",
    "model.eval()\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "# -------------------------\n",
    "# 5) Load test data\n",
    "# -------------------------\n",
    "test_df = pd.read_csv(Path(DATA_PROCESSED) / \"test.csv\")\n",
    "src_sentences = test_df[\"hu\"].tolist()\n",
    "refs = test_df[\"ro\"].tolist()\n",
    "print(\"Test size:\", len(src_sentences))\n",
    "\n",
    "# -------------------------\n",
    "# 6) Metrics helpers\n",
    "# -------------------------\n",
    "hu_diacritics = set(\"áéíóöőúüűÁÉÍÓÖŐÚÜŰ\")\n",
    "def has_hu_diacritics(s: str) -> bool:\n",
    "    return any(c in hu_diacritics for c in s)\n",
    "\n",
    "def en_leak(s: str) -> bool:\n",
    "    s = \" \" + re.sub(r\"\\s+\", \" \", s.lower()) + \" \"\n",
    "    common = [\" the \", \" and \", \" of \", \" to \", \" for \", \" with \", \" on \"]\n",
    "    return any(w in s for w in common)\n",
    "\n",
    "def norm_ro(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"ţ\", \"ț\").replace(\"ş\", \"ș\")  # unify diacritics variants\n",
    "    return unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "# Your glossary should already exist as: glossary = { \"Bizottság\": [\"Comisia\", ...], ... }\n",
    "# We'll normalize it for fairer matching.\n",
    "glossary_norm = {norm_ro(k): [norm_ro(v) for v in vs] for k, vs in glossary.items()}\n",
    "\n",
    "def glossary_hit(src: str, hyp: str, glos_norm: dict) -> list:\n",
    "    src_n, hyp_n = norm_ro(src), norm_ro(hyp)\n",
    "    checks = []\n",
    "    for src_term, ro_forms in glos_norm.items():\n",
    "        if src_term in src_n:\n",
    "            checks.append(any(f in hyp_n for f in ro_forms))\n",
    "    return checks\n",
    "\n",
    "# -------------------------\n",
    "# 7) Fast translation + OOM-safe batch fallback\n",
    "# -------------------------\n",
    "def translate_batch(\n",
    "    model, tokenizer, sentences,\n",
    "    batch_size=16, max_input_len=256, max_new_tokens=96,\n",
    "    num_beams=1, length_penalty=1.0\n",
    "):\n",
    "    hyps = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        bs = min(batch_size, len(sentences) - i)\n",
    "        batch = sentences[i:i+bs]\n",
    "        try:\n",
    "            inputs = tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_input_len\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                out = model.generate(\n",
    "                    **inputs,\n",
    "                    num_beams=num_beams,\n",
    "                    length_penalty=length_penalty,\n",
    "                    do_sample=False,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    early_stopping=True,\n",
    "                )\n",
    "\n",
    "            hyps.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "            i += bs\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            torch.cuda.empty_cache()\n",
    "            if batch_size <= 1:\n",
    "                raise\n",
    "            batch_size = max(1, batch_size // 2)\n",
    "            print(f\"OOM -> reducing batch_size to {batch_size} and retrying...\")\n",
    "\n",
    "    return hyps\n",
    "\n",
    "# -------------------------\n",
    "# 8) Greedy decode (fast) + metrics\n",
    "# -------------------------\n",
    "greedy_bs = 16 if DEVICE == \"cuda\" else 4\n",
    "finetuned_hyps = translate_batch(\n",
    "    model, tokenizer, src_sentences,\n",
    "    batch_size=greedy_bs,\n",
    "    max_input_len=MAX_INPUT_LEN,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    num_beams=1,\n",
    "    length_penalty=1.0\n",
    ")\n",
    "\n",
    "bleu_g = sacrebleu.corpus_bleu(finetuned_hyps, [refs]).score\n",
    "leak_hu_g = np.mean([has_hu_diacritics(h) for h in finetuned_hyps]) * 100\n",
    "leak_en_g = np.mean([en_leak(h) for h in finetuned_hyps]) * 100\n",
    "hits_g = [glossary_hit(s, h, glossary_norm) for s, h in zip(src_sentences, finetuned_hyps)]\n",
    "flat_g = [x for row in hits_g for x in row]\n",
    "glos_g = (np.mean(flat_g) * 100) if flat_g else float(\"nan\")\n",
    "\n",
    "print(\"\\n=== GREEDY RESULTS ===\")\n",
    "print(\"BLEU:\", round(bleu_g, 2))\n",
    "print(\"HU diacritics leak %:\", round(leak_hu_g, 2))\n",
    "print(\"English leak proxy %:\", round(leak_en_g, 2))\n",
    "print(\"Glossary accuracy %:\", round(glos_g, 2) if not np.isnan(glos_g) else \"(no glossary terms found)\")\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "out_path_g = OUT_DIR / f\"finetuned_predictions_lora_{CKPT_DIR.name}_greedy.csv\"\n",
    "pd.DataFrame({\"source\": src_sentences, \"reference\": refs, \"hypothesis\": finetuned_hyps}).to_csv(out_path_g, index=False)\n",
    "print(\"Saved:\", out_path_g)\n",
    "\n",
    "# -------------------------\n",
    "# 9) Optional: Beam search sweep for best BLEU (useful for final reporting)\n",
    "# -------------------------\n",
    "# Try a few length penalties; keep best BLEU.\n",
    "beam_bs = 8 if DEVICE == \"cuda\" else 2\n",
    "cands = []\n",
    "for lp in [0.8, 1.0, 1.2]:\n",
    "    hyps_b = translate_batch(\n",
    "        model, tokenizer, src_sentences,\n",
    "        batch_size=beam_bs,\n",
    "        max_input_len=MAX_INPUT_LEN,\n",
    "        max_new_tokens=128,\n",
    "        num_beams=4,\n",
    "        length_penalty=lp\n",
    "    )\n",
    "    bleu_b = sacrebleu.corpus_bleu(hyps_b, [refs]).score\n",
    "    cands.append((bleu_b, lp, hyps_b))\n",
    "    print(f\"Beam4 lp={lp} BLEU={bleu_b:.2f}\")\n",
    "\n",
    "best_bleu, best_lp, best_hyps = max(cands, key=lambda x: x[0])\n",
    "\n",
    "leak_hu_b = np.mean([has_hu_diacritics(h) for h in best_hyps]) * 100\n",
    "leak_en_b = np.mean([en_leak(h) for h in best_hyps]) * 100\n",
    "hits_b = [glossary_hit(s, h, glossary_norm) for s, h in zip(src_sentences, best_hyps)]\n",
    "flat_b = [x for row in hits_b for x in row]\n",
    "glos_b = (np.mean(flat_b) * 100) if flat_b else float(\"nan\")\n",
    "\n",
    "print(\"\\n=== BEST BEAM RESULTS ===\")\n",
    "print(\"Best lp:\", best_lp)\n",
    "print(\"BLEU:\", round(best_bleu, 2))\n",
    "print(\"HU diacritics leak %:\", round(leak_hu_b, 2))\n",
    "print(\"English leak proxy %:\", round(leak_en_b, 2))\n",
    "print(\"Glossary accuracy %:\", round(glos_b, 2) if not np.isnan(glos_b) else \"(no glossary terms found)\")\n",
    "\n",
    "out_path_b = OUT_DIR / f\"finetuned_predictions_lora_{CKPT_DIR.name}_beam4_lp{best_lp}.csv\"\n",
    "pd.DataFrame({\"source\": src_sentences, \"reference\": refs, \"hypothesis\": best_hyps}).to_csv(out_path_b, index=False)\n",
    "print(\"Saved:\", out_path_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d0cc0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: ecb16652-d4dc-423d-be78-350d7120b966)')' thrown while requesting HEAD https://huggingface.co/Helsinki-NLP/opus-mt-hu-ro/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not available: Helsinki-NLP/opus-mt-hu-ro | OSError\n",
      "Not available: Helsinki-NLP/opus-mt-ro-hu | OSError\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No HU<->RO OPUS model available. You must use a pivot or different base.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chosen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo HU<->RO OPUS model available. You must use a pivot or different base.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m MODEL_NAME \u001b[38;5;241m=\u001b[39m chosen\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChosen model:\u001b[39m\u001b[38;5;124m\"\u001b[39m, MODEL_NAME)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No HU<->RO OPUS model available. You must use a pivot or different base."
     ]
    }
   ],
   "source": [
    "import os, gc, math, re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# -------------------------\n",
    "# 0) Pick best available HU<->RO model (prefer HU->RO)\n",
    "# -------------------------\n",
    "CANDIDATES = [\n",
    "    \"Helsinki-NLP/opus-mt-hu-ro\",\n",
    "    \"Helsinki-NLP/opus-mt-ro-hu\",\n",
    "]\n",
    "\n",
    "chosen = None\n",
    "for m in CANDIDATES:\n",
    "    try:\n",
    "        _ = AutoTokenizer.from_pretrained(m)\n",
    "        chosen = m\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Not available:\", m, \"|\", type(e).__name__)\n",
    "\n",
    "if chosen is None:\n",
    "    raise RuntimeError(\"No HU<->RO OPUS model available. You must use a pivot or different base.\")\n",
    "\n",
    "MODEL_NAME = chosen\n",
    "print(\"Chosen model:\", MODEL_NAME)\n",
    "\n",
    "# Force direction HU -> RO\n",
    "# If you got ro-hu, we will swap columns at preprocessing time.\n",
    "REVERSE = (MODEL_NAME.endswith(\"ro-hu\"))\n",
    "print(\"REVERSE (means base is RO->HU):\", REVERSE)\n",
    "\n",
    "# -------------------------\n",
    "# 1) Environment / memory settings (Windows + 6GB friendly)\n",
    "# -------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "# -------------------------\n",
    "# 2) Load data\n",
    "# -------------------------\n",
    "train_path = DATA_PROCESSED / \"train.csv\"\n",
    "val_path   = DATA_PROCESSED / \"val.csv\"\n",
    "\n",
    "train_full = pd.read_csv(train_path)\n",
    "val_full   = pd.read_csv(val_path)\n",
    "\n",
    "# -------------------------\n",
    "# 3) Curriculum: oversample \"legal-heavy\" lines early\n",
    "#    (helps terminology + reduces hallucinated institutions)\n",
    "# -------------------------\n",
    "LEGAL_MARKERS_RO = [\n",
    "    \"Regulamentul\", \"Directiva\", \"articol\", \"alineat\", \"considerent\",\n",
    "    \"Comisia\", \"Consiliul\", \"Parlamentul\", \"Uniunii\", \"statele membre\"\n",
    "]\n",
    "LEGAL_MARKERS_HU = [\n",
    "    \"rendelet\", \"irányelv\", \"cikk\", \"bekezdés\",\n",
    "    \"Bizottság\", \"Tanács\", \"Parlament\", \"Unió\", \"tagállam\"\n",
    "]\n",
    "\n",
    "def is_legalish(row):\n",
    "    hu = str(row[\"hu\"])\n",
    "    ro = str(row[\"ro\"])\n",
    "    hu_hit = any(m in hu for m in LEGAL_MARKERS_HU)\n",
    "    ro_hit = any(m in ro for m in LEGAL_MARKERS_RO)\n",
    "    return hu_hit or ro_hit\n",
    "\n",
    "# pick sizes (adjust)\n",
    "train_n = min(50000, len(train_full))\n",
    "val_n   = min(2000, len(val_full))\n",
    "\n",
    "train_sample = train_full.sample(train_n, random_state=42)\n",
    "\n",
    "# make a curriculum subset: legalish + some random\n",
    "legal_part = train_sample[train_sample.apply(is_legalish, axis=1)]\n",
    "rand_part  = train_sample.sample(min(len(train_sample), max(5000, train_n // 5)), random_state=43)\n",
    "cur_df = pd.concat([legal_part, rand_part], ignore_index=True).drop_duplicates()\n",
    "\n",
    "# final train is curriculum first then rest (Trainer shuffles each epoch, but this still improves mix)\n",
    "train_df = pd.concat([cur_df, train_sample], ignore_index=True).drop_duplicates()\n",
    "val_df   = val_full.head(val_n)\n",
    "\n",
    "print(\"Train size:\", len(train_df), \"| curriculum chunk:\", len(cur_df), \"| Val size:\", len(val_df))\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "val_ds   = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "\n",
    "# -------------------------\n",
    "# 4) Tokenizer + preprocessing (correct direction)\n",
    "# -------------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "max_src_len = 256\n",
    "max_tgt_len = 256\n",
    "\n",
    "def preprocess(examples):\n",
    "    # If base is ro->hu, swap so the model sees ro as source and hu as target,\n",
    "    # BUT our task is hu->ro, so we instead invert columns at *inference* if needed.\n",
    "    # For training HU->RO specifically: always set source=hu and target=ro.\n",
    "    src_texts = examples[\"hu\"]\n",
    "    tgt_texts = examples[\"ro\"]\n",
    "\n",
    "    # If you accidentally ended up with ro-hu base, you can still train HU->RO,\n",
    "    # but it fights the pretrained direction; you’ll usually do better by using hu-ro.\n",
    "    # We'll still allow it, but warn.\n",
    "    if REVERSE:\n",
    "        # We keep HU->RO data, but note: base is RO->HU so performance may be weaker.\n",
    "        pass\n",
    "\n",
    "    model_inputs = tokenizer(src_texts, truncation=True, max_length=max_src_len)\n",
    "\n",
    "    labels = tokenizer(text_target=tgt_texts, truncation=True, max_length=max_tgt_len)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_tok = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "val_tok   = val_ds.map(preprocess, batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "# -------------------------\n",
    "# 5) Load model + training stability tweaks\n",
    "# -------------------------\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else None\n",
    ").to(DEVICE)\n",
    "\n",
    "# Memory saver + speed tradeoff\n",
    "base_model.config.use_cache = False\n",
    "if hasattr(base_model, \"gradient_checkpointing_enable\"):\n",
    "    base_model.gradient_checkpointing_enable()\n",
    "\n",
    "# -------------------------\n",
    "# 6) LoRA tuned for Marian (safe targeting)\n",
    "#    Marian usually has q_proj/v_proj in attention modules in newer versions,\n",
    "#    but if your build doesn't, we gracefully fall back to full fine-tune.\n",
    "# -------------------------\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "model = base_model\n",
    "using_lora = False\n",
    "try:\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    using_lora = True\n",
    "    print(\"Using LoRA.\")\n",
    "except Exception as e:\n",
    "    print(\"LoRA not compatible here; training full model instead:\", type(e).__name__, e)\n",
    "    model = base_model\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# -------------------------\n",
    "# 7) Training arguments tuned for legal MT on small GPU\n",
    "# -------------------------\n",
    "# 6GB-safe defaults:\n",
    "batch_size = 4 if DEVICE == \"cuda\" else 2\n",
    "grad_accum = 4 if DEVICE == \"cuda\" else 8  # effective batch ~16\n",
    "num_train_epochs = 2                       # start 2; go to 3 if still improving\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(PROJECT_ROOT / \"checkpoints\" / \"opus_hu_ro_legal_direct\"),\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    learning_rate=3e-4 if using_lora else 5e-5,   # LoRA can use higher LR\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    fp16=(DEVICE == \"cuda\"),\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=0,  # Windows-safe\n",
    "    # MT quality knobs\n",
    "    label_smoothing_factor=0.1,  # helps robustness + reduces overconfident weird tokens\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01 if not using_lora else 0.0,\n",
    "    # speed/memory\n",
    "    predict_with_generate=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Done. Saved to:\", args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea254e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Torch: 2.5.1+cu121\n",
      "Base safetensors exists: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\safetensors_bases\\Helsinki-NLP__opus-mt-hu-en\n",
      "Train size: 50000 Val size: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "HU→EN pseudo: 100%|██████████| 50000/50000 [29:52<00:00, 27.89sent/s]\n",
      "HU→EN pseudo: 100%|██████████| 2000/2000 [01:12<00:00, 27.61sent/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\pseudo\\train_hu_ro_enpseudo_50000.csv\n",
      "Saved: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\pseudo\\val_hu_ro_enpseudo_2000.csv\n"
     ]
    }
   ],
   "source": [
    "import os, gc, json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
    "\n",
    "# -------------------------\n",
    "# Settings\n",
    "# -------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "HU_EN_BASE = \"Helsinki-NLP/opus-mt-hu-en\"  # strong + small enough\n",
    "BASE_SAFE_ROOT = Path(PROJECT_ROOT) / \"safetensors_bases\"\n",
    "BASE_SAFE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_PSEUDO_DIR = Path(PROJECT_ROOT) / \"data\" / \"pseudo\"\n",
    "OUT_PSEUDO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Use a subset first if needed (speed)\n",
    "TRAIN_N = 50000\n",
    "VAL_N   = 2000\n",
    "\n",
    "# -------------------------\n",
    "# Option C helper: ensure base safetensors exists (handles sharded)\n",
    "# -------------------------\n",
    "def ensure_base_safetensors(model_id: str, out_root: Path) -> Path:\n",
    "    out_dir = out_root / model_id.replace(\"/\", \"__\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if (out_dir / \"model.safetensors\").exists() and (out_dir / \"config.json\").exists():\n",
    "        print(\"Base safetensors exists:\", out_dir)\n",
    "        return out_dir\n",
    "\n",
    "    print(f\"\\n[Convert] Downloading snapshot for: {model_id}\")\n",
    "    snap_dir = Path(snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        allow_patterns=[\n",
    "            \"config.json\",\n",
    "            \"tokenizer.json\",\n",
    "            \"tokenizer_config.json\",\n",
    "            \"source.spm\",\n",
    "            \"vocab.json\", \"merges.txt\",\n",
    "            \"pytorch_model.bin\",\n",
    "            \"pytorch_model.bin.index.json\",\n",
    "            \"pytorch_model-*.bin\",\n",
    "        ],\n",
    "        ignore_patterns=[\"*.msgpack\", \"*.h5\", \"*.ot\", \"*.tflite\", \"*.onnx\"]\n",
    "    ))\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "    bin_path = snap_dir / \"pytorch_model.bin\"\n",
    "    if bin_path.exists():\n",
    "        print(\"[Convert] Found single pytorch_model.bin -> torch.load\")\n",
    "        state = torch.load(bin_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state, strict=False)\n",
    "        del state\n",
    "    else:\n",
    "        index_path = snap_dir / \"pytorch_model.bin.index.json\"\n",
    "        if not index_path.exists():\n",
    "            raise FileNotFoundError(\"Could not find pytorch_model.bin or pytorch_model.bin.index.json\")\n",
    "\n",
    "        with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            index = json.load(f)\n",
    "        shard_files = sorted(set(index[\"weight_map\"].values()))\n",
    "        print(f\"[Convert] Found sharded weights: {len(shard_files)} shards\")\n",
    "\n",
    "        for sf in tqdm(shard_files, desc=f\"[Convert] Loading shards {model_id.split('/')[-1]}\"):\n",
    "            sp = snap_dir / sf\n",
    "            shard_state = torch.load(sp, map_location=\"cpu\")\n",
    "            model.load_state_dict(shard_state, strict=False)\n",
    "            del shard_state\n",
    "\n",
    "    print(\"[Convert] Saving safetensors ->\", out_dir)\n",
    "    model.save_pretrained(out_dir, safe_serialization=True)\n",
    "    config.save_pretrained(out_dir)\n",
    "    return out_dir\n",
    "\n",
    "HU_EN_SAFE = ensure_base_safetensors(HU_EN_BASE, BASE_SAFE_ROOT)\n",
    "\n",
    "# -------------------------\n",
    "# Translate with progress + OOM-safe batch shrink\n",
    "# -------------------------\n",
    "def translate(model, tok, texts, bs=16, max_new=128, num_beams=1):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    i = 0\n",
    "    pbar = tqdm(total=len(texts), desc=\"HU→EN pseudo\", unit=\"sent\")\n",
    "    while i < len(texts):\n",
    "        cur_bs = min(bs, len(texts) - i)\n",
    "        batch = texts[i:i+cur_bs]\n",
    "        try:\n",
    "            inp = tok(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(DEVICE)\n",
    "            with torch.inference_mode():\n",
    "                gen = model.generate(**inp, num_beams=num_beams, max_new_tokens=max_new, do_sample=False)\n",
    "            outs.extend(tok.batch_decode(gen, skip_special_tokens=True))\n",
    "            i += cur_bs\n",
    "            pbar.update(cur_bs)\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            torch.cuda.empty_cache()\n",
    "            bs = max(1, bs // 2)\n",
    "            print(f\"⚠ OOM -> reducing batch size to {bs}\")\n",
    "            if bs == 1:\n",
    "                continue\n",
    "    pbar.close()\n",
    "    return outs\n",
    "\n",
    "# -------------------------\n",
    "# Load data\n",
    "# -------------------------\n",
    "train_df = pd.read_csv(DATA_PROCESSED / \"train.csv\").sample(TRAIN_N, random_state=42)\n",
    "val_df   = pd.read_csv(DATA_PROCESSED / \"val.csv\").head(VAL_N)\n",
    "\n",
    "print(\"Train size:\", len(train_df), \"Val size:\", len(val_df))\n",
    "\n",
    "# -------------------------\n",
    "# Load tokenizer + model from safetensors folder\n",
    "# -------------------------\n",
    "tok_hu_en = AutoTokenizer.from_pretrained(HU_EN_BASE, use_fast=True)\n",
    "model_hu_en = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    HU_EN_SAFE,\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else None\n",
    ").to(DEVICE)\n",
    "\n",
    "# -------------------------\n",
    "# Generate pseudo EN\n",
    "# -------------------------\n",
    "train_en = translate(model_hu_en, tok_hu_en, train_df[\"hu\"].tolist(), bs=16, num_beams=1)\n",
    "val_en   = translate(model_hu_en, tok_hu_en, val_df[\"hu\"].tolist(), bs=16, num_beams=1)\n",
    "\n",
    "train_p1 = train_df.copy()\n",
    "val_p1   = val_df.copy()\n",
    "train_p1[\"en_pseudo\"] = train_en\n",
    "val_p1[\"en_pseudo\"]   = val_en\n",
    "\n",
    "# -------------------------\n",
    "# Save pseudo data\n",
    "# -------------------------\n",
    "train_p1_path = OUT_PSEUDO_DIR / f\"train_hu_ro_enpseudo_{TRAIN_N}.csv\"\n",
    "val_p1_path   = OUT_PSEUDO_DIR / f\"val_hu_ro_enpseudo_{VAL_N}.csv\"\n",
    "\n",
    "train_p1.to_csv(train_p1_path, index=False)\n",
    "val_p1.to_csv(val_p1_path, index=False)\n",
    "\n",
    "print(\"Saved:\", train_p1_path)\n",
    "print(\"Saved:\", val_p1_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3da7ee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Torch: 2.5.1+cu121\n",
      "✅ Base safetensors exists: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\safetensors_bases\\Helsinki-NLP__opus-mt-hu-en\n",
      "\n",
      "[Convert] Downloading snapshot for: Helsinki-NLP/opus-mt-en-ro\n",
      "[Convert] Found single pytorch_model.bin -> torch.load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RoG\\AppData\\Local\\Temp\\ipykernel_25452\\3399482996.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Convert] Saving safetensors -> D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\safetensors_bases\\Helsinki-NLP__opus-mt-en-ro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pseudo data: 50000 train | 2000 val\n",
      "\n",
      "[Train] Helsinki-NLP/opus-mt-hu-en | hu → en_pseudo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Map: 100%|██████████| 50000/50000 [00:19<00:00, 2565.35 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 2560.82 examples/s]\n",
      "C:\\Users\\RoG\\AppData\\Local\\Temp\\ipykernel_25452\\3399482996.py:197: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Using LoRA\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 36:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.731800</td>\n",
       "      <td>1.656345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.730500</td>\n",
       "      <td>1.652001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.724400</td>\n",
       "      <td>1.650608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.723200</td>\n",
       "      <td>1.648507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.716300</td>\n",
       "      <td>1.647224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.717500</td>\n",
       "      <td>1.647107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 43da0b4e-4bfd-4589-b5f7-f362117eecbf)')' thrown while requesting HEAD https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train] Helsinki-NLP/opus-mt-en-ro | en_pseudo → ro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Map: 100%|██████████| 50000/50000 [00:20<00:00, 2427.07 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 2427.18 examples/s]\n",
      "C:\\Users\\RoG\\AppData\\Local\\Temp\\ipykernel_25452\\3399482996.py:197: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Using LoRA\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 37:02, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.722100</td>\n",
       "      <td>2.654602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.728100</td>\n",
       "      <td>2.641886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.717300</td>\n",
       "      <td>2.640958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.704500</td>\n",
       "      <td>2.638611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.705600</td>\n",
       "      <td>2.636420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.681900</td>\n",
       "      <td>2.635623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done training pivot adapters.\n"
     ]
    }
   ],
   "source": [
    "import os, gc, json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors.torch import save_file\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# -------------------------\n",
    "# Settings\n",
    "# -------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "BASE_SAFE_ROOT = Path(PROJECT_ROOT) / \"safetensors_bases\"\n",
    "BASE_SAFE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PSEUDO_DIR = Path(PROJECT_ROOT) / \"data\" / \"pseudo\"\n",
    "# Match the filenames saved in Cell B:\n",
    "TRAIN_N = 50000\n",
    "VAL_N   = 2000\n",
    "train_p1_path = PSEUDO_DIR / f\"train_hu_ro_enpseudo_{TRAIN_N}.csv\"\n",
    "val_p1_path   = PSEUDO_DIR / f\"val_hu_ro_enpseudo_{VAL_N}.csv\"\n",
    "\n",
    "HU_EN_BASE = \"Helsinki-NLP/opus-mt-hu-en\"\n",
    "EN_RO_BASE = \"Helsinki-NLP/opus-mt-en-ro\"\n",
    "\n",
    "# Training knobs (6GB-friendly)\n",
    "MAX_LEN     = 256\n",
    "BATCH_SIZE  = 4\n",
    "GRAD_ACCUM  = 4\n",
    "EPOCHS      = 2\n",
    "\n",
    "# -------------------------\n",
    "# Option C helper: ensure base safetensors exists (handles sharded)\n",
    "# -------------------------\n",
    "def ensure_base_safetensors(model_id: str, out_root: Path) -> Path:\n",
    "    out_dir = out_root / model_id.replace(\"/\", \"__\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if (out_dir / \"model.safetensors\").exists() and (out_dir / \"config.json\").exists():\n",
    "        print(\"✅ Base safetensors exists:\", out_dir)\n",
    "        return out_dir\n",
    "\n",
    "    print(f\"\\n[Convert] Downloading snapshot for: {model_id}\")\n",
    "    snap_dir = Path(snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        allow_patterns=[\n",
    "            \"config.json\",\n",
    "            \"tokenizer.json\",\n",
    "            \"tokenizer_config.json\",\n",
    "            \"source.spm\",\n",
    "            \"vocab.json\", \"merges.txt\",\n",
    "            \"pytorch_model.bin\",\n",
    "            \"pytorch_model.bin.index.json\",\n",
    "            \"pytorch_model-*.bin\",\n",
    "        ],\n",
    "        ignore_patterns=[\"*.msgpack\", \"*.h5\", \"*.ot\", \"*.tflite\", \"*.onnx\"]\n",
    "    ))\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "    bin_path = snap_dir / \"pytorch_model.bin\"\n",
    "    if bin_path.exists():\n",
    "        print(\"[Convert] Found single pytorch_model.bin -> torch.load\")\n",
    "        state = torch.load(bin_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state, strict=False)\n",
    "        del state\n",
    "    else:\n",
    "        index_path = snap_dir / \"pytorch_model.bin.index.json\"\n",
    "        if not index_path.exists():\n",
    "            raise FileNotFoundError(\"Could not find pytorch_model.bin or pytorch_model.bin.index.json\")\n",
    "\n",
    "        with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            index = json.load(f)\n",
    "        shard_files = sorted(set(index[\"weight_map\"].values()))\n",
    "        print(f\"[Convert] Found sharded weights: {len(shard_files)} shards\")\n",
    "\n",
    "        for sf in tqdm(shard_files, desc=f\"[Convert] Loading shards {model_id.split('/')[-1]}\"):\n",
    "            sp = snap_dir / sf\n",
    "            shard_state = torch.load(sp, map_location=\"cpu\")\n",
    "            model.load_state_dict(shard_state, strict=False)\n",
    "            del shard_state\n",
    "\n",
    "    print(\"[Convert] Saving safetensors ->\", out_dir)\n",
    "    model.save_pretrained(out_dir, safe_serialization=True)\n",
    "    config.save_pretrained(out_dir)\n",
    "    return out_dir\n",
    "\n",
    "HU_EN_SAFE = ensure_base_safetensors(HU_EN_BASE, BASE_SAFE_ROOT)\n",
    "EN_RO_SAFE = ensure_base_safetensors(EN_RO_BASE, BASE_SAFE_ROOT)\n",
    "\n",
    "# -------------------------\n",
    "# LoRA fine-tune (older transformers: eval_strategy)\n",
    "# -------------------------\n",
    "def finetune_lora(\n",
    "    base_name: str,\n",
    "    base_safe_dir: Path,\n",
    "    src_col: str, tgt_col: str,\n",
    "    train_df: pd.DataFrame, val_df: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    lr_lora: float = 3e-4,\n",
    "    epochs: int = 2,\n",
    "    max_len: int = 256,\n",
    "    bs: int = 4,\n",
    "    grad_accum: int = 4,\n",
    "):\n",
    "    print(f\"\\n[Train] {base_name} | {src_col} → {tgt_col}\")\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(base_name, use_fast=True)\n",
    "\n",
    "    train_ds = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "    val_ds   = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "\n",
    "    def prep(ex):\n",
    "        x = tok(ex[src_col], truncation=True, max_length=max_len)\n",
    "        y = tok(text_target=ex[tgt_col], truncation=True, max_length=max_len)\n",
    "        x[\"labels\"] = y[\"input_ids\"]\n",
    "        return x\n",
    "\n",
    "    train_tok = train_ds.map(prep, batched=True, remove_columns=train_ds.column_names)\n",
    "    val_tok   = val_ds.map(prep, batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "    base = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        base_safe_dir,\n",
    "        use_safetensors=True,\n",
    "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else None\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    base.config.use_cache = False\n",
    "    if hasattr(base, \"gradient_checkpointing_enable\"):\n",
    "        base.gradient_checkpointing_enable()\n",
    "\n",
    "    lora_cfg = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        r=8, lora_alpha=16, lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    )\n",
    "\n",
    "    using_lora = False\n",
    "    try:\n",
    "        model = get_peft_model(base, lora_cfg)\n",
    "        if hasattr(model, \"enable_input_require_grads\"):\n",
    "            model.enable_input_require_grads()\n",
    "        using_lora = True\n",
    "        print(\"✔ Using LoRA\")\n",
    "    except Exception as e:\n",
    "        print(\"⚠ LoRA failed, full fine-tune:\", type(e).__name__, e)\n",
    "        model = base\n",
    "\n",
    "    collator = DataCollatorForSeq2Seq(tok, model=model)\n",
    "\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs,\n",
    "        gradient_accumulation_steps=grad_accum,\n",
    "        learning_rate=(lr_lora if using_lora else 5e-5),\n",
    "        num_train_epochs=epochs,\n",
    "        fp16=(DEVICE == \"cuda\"),\n",
    "\n",
    "        # older transformers compat:\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=1000,\n",
    "\n",
    "        save_steps=1000,\n",
    "        save_total_limit=2,\n",
    "        logging_steps=100,\n",
    "        report_to=\"none\",\n",
    "        dataloader_num_workers=0,\n",
    "\n",
    "        label_smoothing_factor=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        predict_with_generate=False,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=val_tok,\n",
    "        tokenizer=tok,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model, tok\n",
    "\n",
    "# -------------------------\n",
    "# Load pseudo-parallel data created in Cell B\n",
    "# -------------------------\n",
    "if not train_p1_path.exists() or not val_p1_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Pseudo data not found. Run Cell B first.\\n\"\n",
    "        f\"Missing: {train_p1_path} or {val_p1_path}\"\n",
    "    )\n",
    "\n",
    "train_p1 = pd.read_csv(train_p1_path)\n",
    "val_p1   = pd.read_csv(val_p1_path)\n",
    "\n",
    "print(\"Loaded pseudo data:\", len(train_p1), \"train |\", len(val_p1), \"val\")\n",
    "\n",
    "# -------------------------\n",
    "# C1) Fine-tune HU→EN on (HU, EN_pseudo)\n",
    "# -------------------------\n",
    "hu_en_ft, hu_en_ft_tok = finetune_lora(\n",
    "    HU_EN_BASE, HU_EN_SAFE,\n",
    "    src_col=\"hu\", tgt_col=\"en_pseudo\",\n",
    "    train_df=train_p1, val_df=val_p1,\n",
    "    out_dir=str(PROJECT_ROOT / \"checkpoints\" / \"hu_en_legal_lora\"),\n",
    "    epochs=EPOCHS,\n",
    "    max_len=MAX_LEN,\n",
    "    bs=BATCH_SIZE,\n",
    "    grad_accum=GRAD_ACCUM,\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# C2) Fine-tune EN→RO on (EN_pseudo, RO)\n",
    "# -------------------------\n",
    "en_ro_ft, en_ro_ft_tok = finetune_lora(\n",
    "    EN_RO_BASE, EN_RO_SAFE,\n",
    "    src_col=\"en_pseudo\", tgt_col=\"ro\",\n",
    "    train_df=train_p1, val_df=val_p1,\n",
    "    out_dir=str(PROJECT_ROOT / \"checkpoints\" / \"en_ro_legal_lora\"),\n",
    "    epochs=EPOCHS,\n",
    "    max_len=MAX_LEN,\n",
    "    bs=BATCH_SIZE,\n",
    "    grad_accum=GRAD_ACCUM,\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Done training pivot adapters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "320ad047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Torch: 2.5.1+cu121\n",
      "Loading Helsinki-NLP/opus-mt-hu-en + LoRA from: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\hu_en_legal_lora\\checkpoint-6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Helsinki-NLP/opus-mt-en-ro + LoRA from: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\en_ro_legal_lora\\checkpoint-6250\n",
      "Test size: 30366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HU→EN (legal): 100%|██████████| 30366/30366 [26:22<00:00, 19.19sent/s]\n",
      "EN→RO (legal): 100%|██████████| 30366/30366 [30:01<00:00, 16.86sent/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pivot BLEU: 38.64\n",
      "HU diacritics leak %: 0.51\n",
      "English leak proxy %: 0.07\n",
      "Glossary accuracy %: 46.56\n",
      "Saved: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\\pivot_predictions_checkpoint-6250__checkpoint-6250_beam1.csv\n"
     ]
    }
   ],
   "source": [
    "import os, gc, json, re, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# -------------------------\n",
    "# 0) Settings\n",
    "# -------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "# Bases\n",
    "HU_EN_BASE = \"Helsinki-NLP/opus-mt-hu-en\"\n",
    "EN_RO_BASE = \"Helsinki-NLP/opus-mt-en-ro\"\n",
    "\n",
    "# Run dirs (where Cell C saved checkpoints)\n",
    "HU_EN_RUN = Path(PROJECT_ROOT) / \"checkpoints\" / \"hu_en_legal_lora\"\n",
    "EN_RO_RUN = Path(PROJECT_ROOT) / \"checkpoints\" / \"en_ro_legal_lora\"\n",
    "\n",
    "BASE_SAFE_ROOT = Path(PROJECT_ROOT) / \"safetensors_bases\"\n",
    "BASE_SAFE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_DIR = Path(OUTPUT_DIR)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Decode knobs\n",
    "MAX_INPUT_LEN  = 256\n",
    "MAX_NEW_TOKENS = 128\n",
    "BS = 16 if DEVICE == \"cuda\" else 4\n",
    "NUM_BEAMS = 1   # use 1 for speed; set 4 for final run\n",
    "LENGTH_PENALTY = 1.0\n",
    "\n",
    "# -------------------------\n",
    "# 1) Helpers\n",
    "# -------------------------\n",
    "def get_latest_checkpoint(run_dir: Path) -> Path:\n",
    "    ckpts = sorted(\n",
    "        [p for p in run_dir.glob(\"checkpoint-*\") if p.is_dir()],\n",
    "        key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else -1\n",
    "    )\n",
    "    return ckpts[-1] if ckpts else run_dir\n",
    "\n",
    "def ensure_base_safetensors(model_id: str, out_root: Path) -> Path:\n",
    "    out_dir = out_root / model_id.replace(\"/\", \"__\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if (out_dir / \"model.safetensors\").exists() and (out_dir / \"config.json\").exists():\n",
    "        return out_dir\n",
    "\n",
    "    print(f\"[Convert] Creating base safetensors for {model_id}\")\n",
    "    snap_dir = Path(snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        allow_patterns=[\n",
    "            \"config.json\",\n",
    "            \"tokenizer.json\",\n",
    "            \"tokenizer_config.json\",\n",
    "            \"source.spm\",\n",
    "            \"vocab.json\", \"merges.txt\",\n",
    "            \"pytorch_model.bin\",\n",
    "            \"pytorch_model.bin.index.json\",\n",
    "            \"pytorch_model-*.bin\",\n",
    "        ],\n",
    "    ))\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "    bin_path = snap_dir / \"pytorch_model.bin\"\n",
    "    if bin_path.exists():\n",
    "        state = torch.load(bin_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state, strict=False)\n",
    "        del state\n",
    "    else:\n",
    "        index_path = snap_dir / \"pytorch_model.bin.index.json\"\n",
    "        with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            index = json.load(f)\n",
    "        shard_files = sorted(set(index[\"weight_map\"].values()))\n",
    "        for sf in tqdm(shard_files, desc=f\"[Convert] shards {model_id.split('/')[-1]}\"):\n",
    "            shard_state = torch.load(snap_dir / sf, map_location=\"cpu\")\n",
    "            model.load_state_dict(shard_state, strict=False)\n",
    "            del shard_state\n",
    "\n",
    "    model.save_pretrained(out_dir, safe_serialization=True)\n",
    "    config.save_pretrained(out_dir)\n",
    "    return out_dir\n",
    "\n",
    "def load_lora_model(base_id: str, run_dir: Path):\n",
    "    base_safe = ensure_base_safetensors(base_id, BASE_SAFE_ROOT)\n",
    "    ckpt = get_latest_checkpoint(run_dir)\n",
    "    print(f\"Loading {base_id} + LoRA from:\", ckpt)\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(base_id, use_fast=True)\n",
    "    base = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        base_safe,\n",
    "        use_safetensors=True,\n",
    "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else None\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    model = PeftModel.from_pretrained(base, ckpt).to(DEVICE)\n",
    "    model.eval()\n",
    "    return model, tok, ckpt\n",
    "\n",
    "def batched_generate(model, tok, texts, bs=16, max_input_len=256, max_new=128, num_beams=1, length_penalty=1.0, desc=\"Gen\"):\n",
    "    outs = []\n",
    "    i = 0\n",
    "    pbar = tqdm(total=len(texts), desc=desc, unit=\"sent\")\n",
    "    while i < len(texts):\n",
    "        cur_bs = min(bs, len(texts) - i)\n",
    "        batch = texts[i:i+cur_bs]\n",
    "        try:\n",
    "            inp = tok(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_len).to(DEVICE)\n",
    "            with torch.inference_mode():\n",
    "                gen = model.generate(\n",
    "                    **inp,\n",
    "                    num_beams=num_beams,\n",
    "                    length_penalty=length_penalty,\n",
    "                    max_new_tokens=max_new,\n",
    "                    do_sample=False,\n",
    "                )\n",
    "            outs.extend(tok.batch_decode(gen, skip_special_tokens=True))\n",
    "            i += cur_bs\n",
    "            pbar.update(cur_bs)\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            torch.cuda.empty_cache()\n",
    "            bs = max(1, bs // 2)\n",
    "            print(f\"⚠ OOM -> reducing batch size to {bs}\")\n",
    "    pbar.close()\n",
    "    return outs\n",
    "\n",
    "# -------------------------\n",
    "# 2) Load the two fine-tuned LoRA models\n",
    "# -------------------------\n",
    "hu_en_model, hu_en_tok, hu_en_ckpt = load_lora_model(HU_EN_BASE, HU_EN_RUN)\n",
    "en_ro_model, en_ro_tok, en_ro_ckpt = load_lora_model(EN_RO_BASE, EN_RO_RUN)\n",
    "\n",
    "# -------------------------\n",
    "# 3) Load test data\n",
    "# -------------------------\n",
    "test_df = pd.read_csv(Path(DATA_PROCESSED) / \"test.csv\")\n",
    "src_sentences = test_df[\"hu\"].tolist()\n",
    "refs = test_df[\"ro\"].tolist()\n",
    "print(\"Test size:\", len(src_sentences))\n",
    "\n",
    "# -------------------------\n",
    "# 4) Pivot translation: HU -> EN -> RO\n",
    "# -------------------------\n",
    "en_mid = batched_generate(\n",
    "    hu_en_model, hu_en_tok, src_sentences,\n",
    "    bs=BS, max_input_len=MAX_INPUT_LEN, max_new=MAX_NEW_TOKENS,\n",
    "    num_beams=NUM_BEAMS, length_penalty=LENGTH_PENALTY,\n",
    "    desc=\"HU→EN (legal)\"\n",
    ")\n",
    "\n",
    "hyps = batched_generate(\n",
    "    en_ro_model, en_ro_tok, en_mid,\n",
    "    bs=BS, max_input_len=MAX_INPUT_LEN, max_new=MAX_NEW_TOKENS,\n",
    "    num_beams=NUM_BEAMS, length_penalty=LENGTH_PENALTY,\n",
    "    desc=\"EN→RO (legal)\"\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Metrics\n",
    "# -------------------------\n",
    "bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n",
    "print(\"\\nPivot BLEU:\", round(bleu, 2))\n",
    "\n",
    "hu_diacritics = set(\"áéíóöőúüűÁÉÍÓÖŐÚÜŰ\")\n",
    "def has_hu_diacritics(s: str) -> bool:\n",
    "    return any(c in hu_diacritics for c in s)\n",
    "\n",
    "def en_leak(s: str) -> bool:\n",
    "    s = \" \" + re.sub(r\"\\s+\", \" \", s.lower()) + \" \"\n",
    "    return any(w in s for w in [\" the \", \" and \", \" of \", \" to \", \" for \", \" with \", \" on \"])\n",
    "\n",
    "hu_leak = np.mean([has_hu_diacritics(h) for h in hyps]) * 100\n",
    "en_leak_rate = np.mean([en_leak(h) for h in hyps]) * 100\n",
    "print(\"HU diacritics leak %:\", round(hu_leak, 2))\n",
    "print(\"English leak proxy %:\", round(en_leak_rate, 2))\n",
    "\n",
    "# ---- glossary metric (normalized) ----\n",
    "def norm_ro(s):\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"ţ\",\"ț\").replace(\"ş\",\"ș\")\n",
    "    return unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "glossary_norm = {norm_ro(k): [norm_ro(v) for v in vs] for k, vs in glossary.items()}\n",
    "\n",
    "def glossary_hit(src: str, hyp: str) -> list:\n",
    "    src_n, hyp_n = norm_ro(src), norm_ro(hyp)\n",
    "    checks = []\n",
    "    for src_term, ro_forms in glossary_norm.items():\n",
    "        if src_term in src_n:\n",
    "            checks.append(any(f in hyp_n for f in ro_forms))\n",
    "    return checks\n",
    "\n",
    "hits = [glossary_hit(s, h) for s, h in zip(src_sentences, hyps)]\n",
    "flat = [x for row in hits for x in row]\n",
    "if flat:\n",
    "    print(\"Glossary accuracy %:\", round(np.mean(flat) * 100, 2))\n",
    "else:\n",
    "    print(\"Glossary accuracy: (no glossary terms found in test)\")\n",
    "\n",
    "# -------------------------\n",
    "# 6) Save outputs\n",
    "# -------------------------\n",
    "out_path = OUT_DIR / f\"pivot_predictions_{hu_en_ckpt.name}__{en_ro_ckpt.name}_beam{NUM_BEAMS}.csv\"\n",
    "pd.DataFrame({\n",
    "    \"source_hu\": src_sentences,\n",
    "    \"pivot_en\": en_mid,\n",
    "    \"reference_ro\": refs,\n",
    "    \"hypothesis_ro\": hyps\n",
    "}).to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "# -------------------------\n",
    "# 7) Optional COMET on first N (fast sanity check)\n",
    "# -------------------------\n",
    "# If you want COMET, run:\n",
    "#   pip install -q unbabel-comet\n",
    "# Then uncomment below.\n",
    "\n",
    "# try:\n",
    "#     from comet import download_model, load_from_checkpoint\n",
    "#     N = 1000\n",
    "#     comet_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "#     comet_model = load_from_checkpoint(comet_path)\n",
    "#     data = [{\"src\": s, \"mt\": m, \"ref\": r} for s,m,r in zip(src_sentences[:N], hyps[:N], refs[:N])]\n",
    "#     out = comet_model.predict(data, batch_size=8, gpus=1 if DEVICE==\"cuda\" else 0)\n",
    "#     print(\"COMET (first 1000):\", round(float(np.mean(out.scores)), 4))\n",
    "# except Exception as e:\n",
    "#     print(\"COMET skipped:\", type(e).__name__, e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
