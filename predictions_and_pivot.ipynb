{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0265c774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\RoG\\\\anaconda3\\\\envs\\\\pythonRL\\\\Lib\\\\site-packages\\\\torch\\\\lib\\\\c10_cuda.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip -q install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04be83b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\n",
      "DATA_RAW: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\raw\n",
      "DATA_PROCESSED: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\processed\n",
      "OUTPUT_DIR: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"data\" / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-hu-en\"\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_RAW:\", DATA_RAW)\n",
    "print(\"DATA_PROCESSED:\", DATA_PROCESSED)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cd6b53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version (torch): 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version (torch):\", torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b644474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "hu_diacritics = set(\"áéíóöőúüűÁÉÍÓÖŐÚÜŰ\")\n",
    "def has_hu_diacritics(s): \n",
    "    return any(c in hu_diacritics for c in s)\n",
    "\n",
    "\n",
    "glossary = {\n",
    "    \"Bizottság\": [\"Comisia\"],\n",
    "    \"Tanács\": [\"Consiliul\"],\n",
    "    \"Közösség\": [\"Comunitatea\", \"Comunității\"],\n",
    "}\n",
    "\n",
    "def glossary_hit(src, hyp, glossary):\n",
    "    hits = [] \n",
    "    for k, vals in glossary.items():\n",
    "        if k in src:\n",
    "            ok = any(v in hyp for v in vals) \n",
    "            hits.append(ok)\n",
    "    return hits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6370c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using checkpoint: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\opus_hu_ro_legal\\checkpoint-18750\n",
      "DEVICE: cuda\n",
      "Torch: 2.5.1+cu121\n",
      "Base safetensors already exists: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\safetensors_bases\\Helsinki-NLP__opus-mt-hu-en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapter safetensors already exists: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\opus_hu_ro_legal\\checkpoint-18750\\adapter_model.safetensors\n",
      "Model device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1898/1898 [30:52<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuned BLEU: 10.01\n",
      "HU diacritics leak rate (FT): 0.81 %\n",
      "Glossary accuracy (FT): 36.38 %\n",
      "Saved: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\\finetuned_predictions_lora_checkpoint-18750.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors.torch import save_file\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "RUN_DIR = Path(PROJECT_ROOT) / \"checkpoints\" / \"opus_hu_ro_legal\"   \n",
    "\n",
    "\n",
    "def get_latest_checkpoint(run_dir: Path) -> Path:\n",
    "    ckpts = sorted(\n",
    "        [p for p in run_dir.glob(\"checkpoint-*\") if p.is_dir()],\n",
    "        key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else -1\n",
    "    )\n",
    "    return ckpts[-1] if ckpts else run_dir\n",
    "\n",
    "CKPT_DIR = get_latest_checkpoint(RUN_DIR)\n",
    "print(\"Using checkpoint:\", CKPT_DIR)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "\n",
    "\n",
    "def ensure_base_safetensors(model_id: str, out_root: Path) -> Path:\n",
    "\n",
    "    out_dir = out_root / model_id.replace(\"/\", \"__\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "    if (out_dir / \"model.safetensors\").exists() and (out_dir / \"config.json\").exists():\n",
    "        print(\"Base safetensors already exists:\", out_dir)\n",
    "        return out_dir\n",
    "\n",
    "    print(\"Downloading base model snapshot:\", model_id)\n",
    "    snap_dir = Path(snapshot_download(repo_id=model_id))\n",
    "\n",
    "    \n",
    "    bin_path = snap_dir / \"pytorch_model.bin\"\n",
    "    if not bin_path.exists():\n",
    "        \n",
    "        shards = sorted(snap_dir.glob(\"pytorch_model-*.bin\"))\n",
    "        if not shards:\n",
    "            raise FileNotFoundError(\"Could not find pytorch_model.bin or shards in snapshot.\")\n",
    "        \n",
    "        raise RuntimeError(\n",
    "            f\"Found sharded weights ({len(shards)} files). \"\n",
    "            \"This quick converter handles non-sharded pytorch_model.bin. \"\n",
    "            \"Tell me and I’ll give you the sharded merge converter.\"\n",
    "        )\n",
    "\n",
    " \n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "   \n",
    "    print(\"Loading base .bin weights with torch.load:\", bin_path)\n",
    "    state = torch.load(bin_path, map_location=\"cpu\")\n",
    "    missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "    print(f\"Loaded base weights. missing={len(missing)} unexpected={len(unexpected)}\")\n",
    "\n",
    "    \n",
    "    print(\"Saving base model as safetensors to:\", out_dir)\n",
    "    model.save_pretrained(out_dir, safe_serialization=True)\n",
    "    config.save_pretrained(out_dir)\n",
    "\n",
    "    return out_dir\n",
    "\n",
    "BASE_SAFE_ROOT = Path(PROJECT_ROOT) / \"safetensors_bases\"\n",
    "BASE_SAFE_DIR = ensure_base_safetensors(MODEL_NAME, BASE_SAFE_ROOT)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "\n",
    "def ensure_adapter_safetensors(ckpt_dir: Path):\n",
    "    bin_path = ckpt_dir / \"adapter_model.bin\"\n",
    "    st_path  = ckpt_dir / \"adapter_model.safetensors\"\n",
    "\n",
    "    if st_path.exists():\n",
    "        print(\"Adapter safetensors already exists:\", st_path)\n",
    "        return\n",
    "\n",
    "    if not bin_path.exists():\n",
    "        raise FileNotFoundError(f\"adapter_model.bin not found in {ckpt_dir}\")\n",
    "\n",
    "    print(\"Converting adapter bin -> safetensors:\", bin_path)\n",
    "    adapter_state = torch.load(bin_path, map_location=\"cpu\")\n",
    "    save_file(adapter_state, str(st_path))\n",
    "    print(\"Wrote:\", st_path)\n",
    "\n",
    "ensure_adapter_safetensors(CKPT_DIR)\n",
    "\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    BASE_SAFE_DIR,\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else None,\n",
    ").to(DEVICE)\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, CKPT_DIR).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(DATA_PROCESSED / \"test.csv\")\n",
    "src_sentences = test_df[\"hu\"].tolist()\n",
    "refs = test_df[\"ro\"].tolist()\n",
    "\n",
    "\n",
    "def translate_batch(model, tokenizer, sentences, batch_size=16, max_input_len=256, max_new_tokens=96, num_beams=1):\n",
    "    hyps = []\n",
    "    for i in tqdm(range(0, len(sentences), batch_size)):\n",
    "        batch = sentences[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_input_len\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            out = model.generate(\n",
    "                **inputs,\n",
    "                num_beams=num_beams,\n",
    "                do_sample=False,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                early_stopping=True,\n",
    "            )\n",
    "        hyps.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "    return hyps\n",
    "\n",
    "\n",
    "finetuned_hyps = translate_batch(\n",
    "    model, tokenizer, src_sentences,\n",
    "    batch_size=16 if DEVICE==\"cuda\" else 4,\n",
    "    num_beams=1,\n",
    "    max_new_tokens=96\n",
    ")\n",
    "\n",
    "\n",
    "bleu_ft = sacrebleu.corpus_bleu(finetuned_hyps, [refs]).score\n",
    "print(\"Finetuned BLEU:\", round(bleu_ft, 2))\n",
    "\n",
    "leak_rate_ft = np.mean([has_hu_diacritics(h) for h in finetuned_hyps])\n",
    "print(\"HU diacritics leak rate (FT):\", round(leak_rate_ft*100, 2), \"%\")\n",
    "\n",
    "all_hits_ft = [glossary_hit(s, h, glossary) for s, h in zip(src_sentences, finetuned_hyps)]\n",
    "flat_ft = [x for row in all_hits_ft for x in row]\n",
    "if flat_ft:\n",
    "    print(\"Glossary accuracy (FT):\", round(np.mean(flat_ft)*100, 2), \"%\")\n",
    "else:\n",
    "    print(\"No glossary terms found in sample.\")\n",
    "\n",
    "\n",
    "out_path = Path(OUTPUT_DIR) / f\"finetuned_predictions_lora_{CKPT_DIR.name}.csv\"\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "pd.DataFrame({\"source\": src_sentences, \"reference\": refs, \"hypothesis\": finetuned_hyps}).to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2dae37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 2183fc6f-a243-4f41-99b8-f34aac5e98f7)')' thrown while requesting HEAD https://huggingface.co/Helsinki-NLP/opus-mt-hu-en/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Torch: 2.5.1+cu121\n",
      "Using checkpoint: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\opus_hu_ro_legal\\checkpoint-18750\n",
      "Base safetensors already exists: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\safetensors_bases\\Helsinki-NLP__opus-mt-hu-en\n",
      "Adapter safetensors already exists: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\opus_hu_ro_legal\\checkpoint-18750\\adapter_model.safetensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n",
      "Test size: 30366\n",
      "\n",
      "=== GREEDY RESULTS ===\n",
      "BLEU: 10.01\n",
      "HU diacritics leak %: 0.81\n",
      "English leak proxy %: 6.45\n",
      "Glossary accuracy %: 40.57\n",
      "Saved: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\\finetuned_predictions_lora_checkpoint-18750_greedy.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 279\u001b[0m\n\u001b[0;32m    277\u001b[0m cands \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lp \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.2\u001b[39m]:\n\u001b[1;32m--> 279\u001b[0m     hyps_b \u001b[38;5;241m=\u001b[39m \u001b[43mtranslate_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_sentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_bs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_input_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_INPUT_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlp\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m     bleu_b \u001b[38;5;241m=\u001b[39m sacrebleu\u001b[38;5;241m.\u001b[39mcorpus_bleu(hyps_b, [refs])\u001b[38;5;241m.\u001b[39mscore\n\u001b[0;32m    288\u001b[0m     cands\u001b[38;5;241m.\u001b[39mappend((bleu_b, lp, hyps_b))\n",
      "Cell \u001b[1;32mIn[7], line 220\u001b[0m, in \u001b[0;36mtranslate_batch\u001b[1;34m(model, tokenizer, sentences, batch_size, max_input_len, max_new_tokens, num_beams, length_penalty)\u001b[0m\n\u001b[0;32m    211\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m    212\u001b[0m     batch,\n\u001b[0;32m    213\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_input_len\n\u001b[0;32m    217\u001b[0m )\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m--> 220\u001b[0m     out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs,\n\u001b[0;32m    222\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mnum_beams,\n\u001b[0;32m    223\u001b[0m         length_penalty\u001b[38;5;241m=\u001b[39mlength_penalty,\n\u001b[0;32m    224\u001b[0m         do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    225\u001b[0m         max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[0;32m    226\u001b[0m         early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    227\u001b[0m     )\n\u001b[0;32m    229\u001b[0m hyps\u001b[38;5;241m.\u001b[39mextend(tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(out, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    230\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bs\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\peft\\peft_model.py:2374\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.generate\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   2372\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2373\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 2374\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\generation\\utils.py:2566\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2563\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[0;32m   2565\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[1;32m-> 2566\u001b[0m result \u001b[38;5;241m=\u001b[39m decoding_method(\n\u001b[0;32m   2567\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2568\u001b[0m     input_ids,\n\u001b[0;32m   2569\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2570\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2571\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2572\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_mode_kwargs,\n\u001b[0;32m   2573\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2574\u001b[0m )\n\u001b[0;32m   2576\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[0;32m   2577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2578\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2579\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2580\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2581\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\generation\\utils.py:3267\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3264\u001b[0m flat_running_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten_beam_dim(running_sequences[:, :, :cur_len])\n\u001b[0;32m   3265\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(flat_running_sequences, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> 3267\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3269\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3270\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3271\u001b[0m     model_outputs,\n\u001b[0;32m   3272\u001b[0m     model_kwargs,\n\u001b[0;32m   3273\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3274\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:1522\u001b[0m, in \u001b[0;36mMarianMTModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1517\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1518\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1519\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1520\u001b[0m         )\n\u001b[1;32m-> 1522\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1523\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1524\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1525\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1526\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1527\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1528\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1530\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1531\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1532\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1533\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1534\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1535\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1536\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1537\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1538\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1540\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[0;32m   1542\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:1278\u001b[0m, in \u001b[0;36mMarianModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1271\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1272\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1273\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1274\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1275\u001b[0m     )\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1278\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1287\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1288\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1289\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1292\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:1069\u001b[0m, in \u001b[0;36mMarianDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropout_probability \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerdrop:\n\u001b[0;32m   1067\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1069\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[0;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1081\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:435\u001b[0m, in \u001b[0;36mMarianDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_values, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m--> 435\u001b[0m     hidden_states, cross_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    445\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\modeling_marian.py:270\u001b[0m, in \u001b[0;36mMarianAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_values, attention_mask, layer_head_mask, output_attentions, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    258\u001b[0m     query_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    267\u001b[0m )\n\u001b[0;32m    269\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m--> 270\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, re, json, unicodedata\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sacrebleu\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors.torch import save_file\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "MODEL_NAME = \"Helsinki-NLP/opus-mt-hu-en\"   \n",
    "\n",
    "RUN_DIR    = Path(PROJECT_ROOT) / \"checkpoints\" / \"opus_hu_ro_legal\"\n",
    "OUT_DIR    = Path(OUTPUT_DIR)\n",
    "BASE_SAFE_ROOT = Path(PROJECT_ROOT) / \"safetensors_bases\"\n",
    "\n",
    "\n",
    "MAX_INPUT_LEN  = 256\n",
    "MAX_NEW_TOKENS = 96\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "torch.backends.cuda.matmul.allow_tf32 = False\n",
    "torch.backends.cudnn.allow_tf32 = False\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "\n",
    "def get_latest_checkpoint(run_dir: Path) -> Path:\n",
    "    ckpts = sorted(\n",
    "        [p for p in run_dir.glob(\"checkpoint-*\") if p.is_dir()],\n",
    "        key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else -1\n",
    "    )\n",
    "    return ckpts[-1] if ckpts else run_dir\n",
    "\n",
    "CKPT_DIR = get_latest_checkpoint(RUN_DIR)\n",
    "print(\"Using checkpoint:\", CKPT_DIR)\n",
    "\n",
    "\n",
    "def ensure_base_safetensors(model_id: str, out_root: Path) -> Path:\n",
    "    out_dir = out_root / model_id.replace(\"/\", \"__\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if (out_dir / \"model.safetensors\").exists() and (out_dir / \"config.json\").exists():\n",
    "        print(\"Base safetensors already exists:\", out_dir)\n",
    "        return out_dir\n",
    "\n",
    "    print(\"Downloading base model snapshot:\", model_id)\n",
    "    snap_dir = Path(snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        allow_patterns=[\n",
    "            \"config.json\",\n",
    "            \"tokenizer.json\",\n",
    "            \"tokenizer_config.json\",\n",
    "            \"source.spm\",\n",
    "            \"vocab.json\", \"merges.txt\",\n",
    "            \"pytorch_model.bin\",\n",
    "            \"pytorch_model.bin.index.json\",\n",
    "            \"pytorch_model-*.bin\",\n",
    "        ],\n",
    "        ignore_patterns=[\"*.msgpack\", \"*.h5\", \"*.ot\", \"*.tflite\", \"*.onnx\"]\n",
    "    ))\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "    \n",
    "    bin_path = snap_dir / \"pytorch_model.bin\"\n",
    "    if bin_path.exists():\n",
    "        print(\"Found single pytorch_model.bin; loading with torch.load:\", bin_path)\n",
    "        state = torch.load(bin_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state, strict=False)\n",
    "        del state\n",
    "    else:\n",
    "        \n",
    "        index_path = snap_dir / \"pytorch_model.bin.index.json\"\n",
    "        if not index_path.exists():\n",
    "            shards = sorted(snap_dir.glob(\"pytorch_model-*.bin\"))\n",
    "            raise FileNotFoundError(\n",
    "                f\"Could not find pytorch_model.bin or pytorch_model.bin.index.json.\\nFound {len(shards)} shard files.\"\n",
    "            )\n",
    "\n",
    "        with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            index = json.load(f)\n",
    "        shard_files = sorted(set(index[\"weight_map\"].values()))\n",
    "        print(f\"Found sharded weights: {len(shard_files)} shards\")\n",
    "\n",
    "        for sf in shard_files:\n",
    "            sp = snap_dir / sf\n",
    "            if not sp.exists():\n",
    "                raise FileNotFoundError(f\"Missing shard file: {sp}\")\n",
    "            shard_state = torch.load(sp, map_location=\"cpu\")\n",
    "            model.load_state_dict(shard_state, strict=False)\n",
    "            del shard_state\n",
    "\n",
    "    print(\"Saving base model as safetensors to:\", out_dir)\n",
    "    model.save_pretrained(out_dir, safe_serialization=True)\n",
    "    config.save_pretrained(out_dir)\n",
    "    return out_dir\n",
    "\n",
    "BASE_SAFE_DIR = ensure_base_safetensors(MODEL_NAME, BASE_SAFE_ROOT)\n",
    "\n",
    "\n",
    "def ensure_adapter_safetensors(ckpt_dir: Path):\n",
    "    bin_path = ckpt_dir / \"adapter_model.bin\"\n",
    "    st_path  = ckpt_dir / \"adapter_model.safetensors\"\n",
    "\n",
    "    if st_path.exists():\n",
    "        print(\"Adapter safetensors already exists:\", st_path)\n",
    "        return\n",
    "\n",
    "    if not bin_path.exists():\n",
    "        raise FileNotFoundError(f\"adapter_model.bin not found in {ckpt_dir}\")\n",
    "\n",
    "    print(\"Converting adapter bin -> safetensors:\", bin_path)\n",
    "    adapter_state = torch.load(bin_path, map_location=\"cpu\")\n",
    "    save_file(adapter_state, str(st_path))\n",
    "    del adapter_state\n",
    "    print(\"Wrote:\", st_path)\n",
    "\n",
    "ensure_adapter_safetensors(CKPT_DIR)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    BASE_SAFE_DIR,\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else None,\n",
    ").to(DEVICE)\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, CKPT_DIR).to(DEVICE)\n",
    "model.eval()\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(Path(DATA_PROCESSED) / \"test.csv\")\n",
    "src_sentences = test_df[\"hu\"].tolist()\n",
    "refs = test_df[\"ro\"].tolist()\n",
    "print(\"Test size:\", len(src_sentences))\n",
    "\n",
    "\n",
    "hu_diacritics = set(\"áéíóöőúüűÁÉÍÓÖŐÚÜŰ\")\n",
    "def has_hu_diacritics(s: str) -> bool:\n",
    "    return any(c in hu_diacritics for c in s)\n",
    "\n",
    "def en_leak(s: str) -> bool:\n",
    "    s = \" \" + re.sub(r\"\\s+\", \" \", s.lower()) + \" \"\n",
    "    common = [\" the \", \" and \", \" of \", \" to \", \" for \", \" with \", \" on \"]\n",
    "    return any(w in s for w in common)\n",
    "\n",
    "def norm_ro(s: str) -> str:\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"ţ\", \"ț\").replace(\"ş\", \"ș\")\n",
    "    return unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "\n",
    "glossary_norm = {norm_ro(k): [norm_ro(v) for v in vs] for k, vs in glossary.items()}\n",
    "\n",
    "def glossary_hit(src: str, hyp: str, glos_norm: dict) -> list:\n",
    "    src_n, hyp_n = norm_ro(src), norm_ro(hyp)\n",
    "    checks = []\n",
    "    for src_term, ro_forms in glos_norm.items():\n",
    "        if src_term in src_n:\n",
    "            checks.append(any(f in hyp_n for f in ro_forms))\n",
    "    return checks\n",
    "\n",
    "\n",
    "def translate_batch(\n",
    "    model, tokenizer, sentences,\n",
    "    batch_size=16, max_input_len=256, max_new_tokens=96,\n",
    "    num_beams=1, length_penalty=1.0\n",
    "):\n",
    "    hyps = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        bs = min(batch_size, len(sentences) - i)\n",
    "        batch = sentences[i:i+bs]\n",
    "        try:\n",
    "            inputs = tokenizer(\n",
    "                batch,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_input_len\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                out = model.generate(\n",
    "                    **inputs,\n",
    "                    num_beams=num_beams,\n",
    "                    length_penalty=length_penalty,\n",
    "                    do_sample=False,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    early_stopping=True,\n",
    "                )\n",
    "\n",
    "            hyps.extend(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "            i += bs\n",
    "\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            torch.cuda.empty_cache()\n",
    "            if batch_size <= 1:\n",
    "                raise\n",
    "            batch_size = max(1, batch_size // 2)\n",
    "            print(f\"OOM -> reducing batch_size to {batch_size} and retrying...\")\n",
    "\n",
    "    return hyps\n",
    "\n",
    "\n",
    "greedy_bs = 16 if DEVICE == \"cuda\" else 4\n",
    "finetuned_hyps = translate_batch(\n",
    "    model, tokenizer, src_sentences,\n",
    "    batch_size=greedy_bs,\n",
    "    max_input_len=MAX_INPUT_LEN,\n",
    "    max_new_tokens=MAX_NEW_TOKENS,\n",
    "    num_beams=1,\n",
    "    length_penalty=1.0\n",
    ")\n",
    "\n",
    "bleu_g = sacrebleu.corpus_bleu(finetuned_hyps, [refs]).score\n",
    "leak_hu_g = np.mean([has_hu_diacritics(h) for h in finetuned_hyps]) * 100\n",
    "leak_en_g = np.mean([en_leak(h) for h in finetuned_hyps]) * 100\n",
    "hits_g = [glossary_hit(s, h, glossary_norm) for s, h in zip(src_sentences, finetuned_hyps)]\n",
    "flat_g = [x for row in hits_g for x in row]\n",
    "glos_g = (np.mean(flat_g) * 100) if flat_g else float(\"nan\")\n",
    "\n",
    "print(\"\\n=== GREEDY RESULTS ===\")\n",
    "print(\"BLEU:\", round(bleu_g, 2))\n",
    "print(\"HU diacritics leak %:\", round(leak_hu_g, 2))\n",
    "print(\"English leak proxy %:\", round(leak_en_g, 2))\n",
    "print(\"Glossary accuracy %:\", round(glos_g, 2) if not np.isnan(glos_g) else \"(no glossary terms found)\")\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "out_path_g = OUT_DIR / f\"finetuned_predictions_lora_{CKPT_DIR.name}_greedy.csv\"\n",
    "pd.DataFrame({\"source\": src_sentences, \"reference\": refs, \"hypothesis\": finetuned_hyps}).to_csv(out_path_g, index=False)\n",
    "print(\"Saved:\", out_path_g)\n",
    "\n",
    "\n",
    "beam_bs = 8 if DEVICE == \"cuda\" else 2\n",
    "cands = []\n",
    "for lp in [0.8, 1.0, 1.2]:\n",
    "    hyps_b = translate_batch(\n",
    "        model, tokenizer, src_sentences,\n",
    "        batch_size=beam_bs,\n",
    "        max_input_len=MAX_INPUT_LEN,\n",
    "        max_new_tokens=128,\n",
    "        num_beams=4,\n",
    "        length_penalty=lp\n",
    "    )\n",
    "    bleu_b = sacrebleu.corpus_bleu(hyps_b, [refs]).score\n",
    "    cands.append((bleu_b, lp, hyps_b))\n",
    "    print(f\"Beam4 lp={lp} BLEU={bleu_b:.2f}\")\n",
    "\n",
    "best_bleu, best_lp, best_hyps = max(cands, key=lambda x: x[0])\n",
    "\n",
    "leak_hu_b = np.mean([has_hu_diacritics(h) for h in best_hyps]) * 100\n",
    "leak_en_b = np.mean([en_leak(h) for h in best_hyps]) * 100\n",
    "hits_b = [glossary_hit(s, h, glossary_norm) for s, h in zip(src_sentences, best_hyps)]\n",
    "flat_b = [x for row in hits_b for x in row]\n",
    "glos_b = (np.mean(flat_b) * 100) if flat_b else float(\"nan\")\n",
    "\n",
    "print(\"\\n=== BEST BEAM RESULTS ===\")\n",
    "print(\"Best lp:\", best_lp)\n",
    "print(\"BLEU:\", round(best_bleu, 2))\n",
    "print(\"HU diacritics leak %:\", round(leak_hu_b, 2))\n",
    "print(\"English leak proxy %:\", round(leak_en_b, 2))\n",
    "print(\"Glossary accuracy %:\", round(glos_b, 2) if not np.isnan(glos_b) else \"(no glossary terms found)\")\n",
    "\n",
    "out_path_b = OUT_DIR / f\"finetuned_predictions_lora_{CKPT_DIR.name}_beam4_lp{best_lp}.csv\"\n",
    "pd.DataFrame({\"source\": src_sentences, \"reference\": refs, \"hypothesis\": best_hyps}).to_csv(out_path_b, index=False)\n",
    "print(\"Saved:\", out_path_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0cc0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: ecb16652-d4dc-423d-be78-350d7120b966)')' thrown while requesting HEAD https://huggingface.co/Helsinki-NLP/opus-mt-hu-ro/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not available: Helsinki-NLP/opus-mt-hu-ro | OSError\n",
      "Not available: Helsinki-NLP/opus-mt-ro-hu | OSError\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No HU<->RO OPUS model available. You must use a pivot or different base.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot available:\u001b[39m\u001b[38;5;124m\"\u001b[39m, m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(e)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chosen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo HU<->RO OPUS model available. You must use a pivot or different base.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m MODEL_NAME \u001b[38;5;241m=\u001b[39m chosen\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChosen model:\u001b[39m\u001b[38;5;124m\"\u001b[39m, MODEL_NAME)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No HU<->RO OPUS model available. You must use a pivot or different base."
     ]
    }
   ],
   "source": [
    "import os, gc, math, re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "CANDIDATES = [\n",
    "    \"Helsinki-NLP/opus-mt-hu-ro\",\n",
    "    \"Helsinki-NLP/opus-mt-ro-hu\",\n",
    "]\n",
    "\n",
    "chosen = None\n",
    "for m in CANDIDATES:\n",
    "    try:\n",
    "        _ = AutoTokenizer.from_pretrained(m)\n",
    "        chosen = m\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Not available:\", m, \"|\", type(e).__name__)\n",
    "\n",
    "if chosen is None:\n",
    "    raise RuntimeError(\"No HU<->RO OPUS model available. You must use a pivot or different base.\")\n",
    "\n",
    "MODEL_NAME = chosen\n",
    "print(\"Chosen model:\", MODEL_NAME)\n",
    "\n",
    "\n",
    "REVERSE = (MODEL_NAME.endswith(\"ro-hu\"))\n",
    "print(\"REVERSE (means base is RO->HU):\", REVERSE)\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "train_path = DATA_PROCESSED / \"train.csv\"\n",
    "val_path   = DATA_PROCESSED / \"val.csv\"\n",
    "\n",
    "train_full = pd.read_csv(train_path)\n",
    "val_full   = pd.read_csv(val_path)\n",
    "\n",
    "\n",
    "LEGAL_MARKERS_RO = [\n",
    "    \"Regulamentul\", \"Directiva\", \"articol\", \"alineat\", \"considerent\",\n",
    "    \"Comisia\", \"Consiliul\", \"Parlamentul\", \"Uniunii\", \"statele membre\"\n",
    "]\n",
    "LEGAL_MARKERS_HU = [\n",
    "    \"rendelet\", \"irányelv\", \"cikk\", \"bekezdés\",\n",
    "    \"Bizottság\", \"Tanács\", \"Parlament\", \"Unió\", \"tagállam\"\n",
    "]\n",
    "\n",
    "def is_legalish(row):\n",
    "    hu = str(row[\"hu\"])\n",
    "    ro = str(row[\"ro\"])\n",
    "    hu_hit = any(m in hu for m in LEGAL_MARKERS_HU)\n",
    "    ro_hit = any(m in ro for m in LEGAL_MARKERS_RO)\n",
    "    return hu_hit or ro_hit\n",
    "\n",
    "\n",
    "train_n = min(50000, len(train_full))\n",
    "val_n   = min(2000, len(val_full))\n",
    "\n",
    "train_sample = train_full.sample(train_n, random_state=42)\n",
    "\n",
    "legal_part = train_sample[train_sample.apply(is_legalish, axis=1)]\n",
    "rand_part  = train_sample.sample(min(len(train_sample), max(5000, train_n // 5)), random_state=43)\n",
    "cur_df = pd.concat([legal_part, rand_part], ignore_index=True).drop_duplicates()\n",
    "\n",
    "train_df = pd.concat([cur_df, train_sample], ignore_index=True).drop_duplicates()\n",
    "val_df   = val_full.head(val_n)\n",
    "\n",
    "print(\"Train size:\", len(train_df), \"| curriculum chunk:\", len(cur_df), \"| Val size:\", len(val_df))\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "val_ds   = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "\n",
    "max_src_len = 256\n",
    "max_tgt_len = 256\n",
    "\n",
    "def preprocess(examples):\n",
    "\n",
    "    src_texts = examples[\"hu\"]\n",
    "    tgt_texts = examples[\"ro\"]\n",
    "\n",
    "    if REVERSE:\n",
    "        pass\n",
    "\n",
    "    model_inputs = tokenizer(src_texts, truncation=True, max_length=max_src_len)\n",
    "\n",
    "    labels = tokenizer(text_target=tgt_texts, truncation=True, max_length=max_tgt_len)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_tok = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\n",
    "val_tok   = val_ds.map(preprocess, batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else None\n",
    ").to(DEVICE)\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "if hasattr(base_model, \"gradient_checkpointing_enable\"):\n",
    "    base_model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    ")\n",
    "\n",
    "model = base_model\n",
    "using_lora = False\n",
    "try:\n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    if hasattr(model, \"enable_input_require_grads\"):\n",
    "        model.enable_input_require_grads()\n",
    "    using_lora = True\n",
    "    print(\"Using LoRA.\")\n",
    "except Exception as e:\n",
    "    print(\"LoRA not compatible here; training full model instead:\", type(e).__name__, e)\n",
    "    model = base_model\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "\n",
    "batch_size = 4 if DEVICE == \"cuda\" else 2\n",
    "grad_accum = 4 if DEVICE == \"cuda\" else 8  \n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(PROJECT_ROOT / \"checkpoints\" / \"opus_hu_ro_legal_direct\"),\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=grad_accum,\n",
    "    learning_rate=3e-4 if using_lora else 5e-5,   \n",
    "    num_train_epochs=num_train_epochs,\n",
    "    fp16=(DEVICE == \"cuda\"),\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=0,  \n",
    "    \n",
    "    label_smoothing_factor=0.1, \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    weight_decay=0.01 if not using_lora else 0.0,\n",
    "    \n",
    "    predict_with_generate=False,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Done. Saved to:\", args.output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea254e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Torch: 2.5.1+cu121\n",
      "Base safetensors exists: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\safetensors_bases\\Helsinki-NLP__opus-mt-hu-en\n",
      "Train size: 50000 Val size: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "HU→EN pseudo: 100%|██████████| 50000/50000 [29:52<00:00, 27.89sent/s]\n",
      "HU→EN pseudo: 100%|██████████| 2000/2000 [01:12<00:00, 27.61sent/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\pseudo\\train_hu_ro_enpseudo_50000.csv\n",
      "Saved: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\pseudo\\val_hu_ro_enpseudo_2000.csv\n"
     ]
    }
   ],
   "source": [
    "import os, gc, json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "HU_EN_BASE = \"Helsinki-NLP/opus-mt-hu-en\"  \n",
    "BASE_SAFE_ROOT = Path(PROJECT_ROOT) / \"safetensors_bases\"\n",
    "BASE_SAFE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_PSEUDO_DIR = Path(PROJECT_ROOT) / \"data\" / \"pseudo\"\n",
    "OUT_PSEUDO_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "TRAIN_N = 50000\n",
    "VAL_N   = 2000\n",
    "\n",
    "def ensure_base_safetensors(model_id: str, out_root: Path) -> Path:\n",
    "    out_dir = out_root / model_id.replace(\"/\", \"__\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if (out_dir / \"model.safetensors\").exists() and (out_dir / \"config.json\").exists():\n",
    "        print(\"Base safetensors exists:\", out_dir)\n",
    "        return out_dir\n",
    "\n",
    "    print(f\"\\n[Convert] Downloading snapshot for: {model_id}\")\n",
    "    snap_dir = Path(snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        allow_patterns=[\n",
    "            \"config.json\",\n",
    "            \"tokenizer.json\",\n",
    "            \"tokenizer_config.json\",\n",
    "            \"source.spm\",\n",
    "            \"vocab.json\", \"merges.txt\",\n",
    "            \"pytorch_model.bin\",\n",
    "            \"pytorch_model.bin.index.json\",\n",
    "            \"pytorch_model-*.bin\",\n",
    "        ],\n",
    "        ignore_patterns=[\"*.msgpack\", \"*.h5\", \"*.ot\", \"*.tflite\", \"*.onnx\"]\n",
    "    ))\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "    bin_path = snap_dir / \"pytorch_model.bin\"\n",
    "    if bin_path.exists():\n",
    "        print(\"[Convert] Found single pytorch_model.bin -> torch.load\")\n",
    "        state = torch.load(bin_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state, strict=False)\n",
    "        del state\n",
    "    else:\n",
    "        index_path = snap_dir / \"pytorch_model.bin.index.json\"\n",
    "        if not index_path.exists():\n",
    "            raise FileNotFoundError(\"Could not find pytorch_model.bin or pytorch_model.bin.index.json\")\n",
    "\n",
    "        with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            index = json.load(f)\n",
    "        shard_files = sorted(set(index[\"weight_map\"].values()))\n",
    "        print(f\"[Convert] Found sharded weights: {len(shard_files)} shards\")\n",
    "\n",
    "        for sf in tqdm(shard_files, desc=f\"[Convert] Loading shards {model_id.split('/')[-1]}\"):\n",
    "            sp = snap_dir / sf\n",
    "            shard_state = torch.load(sp, map_location=\"cpu\")\n",
    "            model.load_state_dict(shard_state, strict=False)\n",
    "            del shard_state\n",
    "\n",
    "    print(\"[Convert] Saving safetensors ->\", out_dir)\n",
    "    model.save_pretrained(out_dir, safe_serialization=True)\n",
    "    config.save_pretrained(out_dir)\n",
    "    return out_dir\n",
    "\n",
    "HU_EN_SAFE = ensure_base_safetensors(HU_EN_BASE, BASE_SAFE_ROOT)\n",
    "\n",
    "\n",
    "def translate(model, tok, texts, bs=16, max_new=128, num_beams=1):\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    i = 0\n",
    "    pbar = tqdm(total=len(texts), desc=\"HU→EN pseudo\", unit=\"sent\")\n",
    "    while i < len(texts):\n",
    "        cur_bs = min(bs, len(texts) - i)\n",
    "        batch = texts[i:i+cur_bs]\n",
    "        try:\n",
    "            inp = tok(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=256).to(DEVICE)\n",
    "            with torch.inference_mode():\n",
    "                gen = model.generate(**inp, num_beams=num_beams, max_new_tokens=max_new, do_sample=False)\n",
    "            outs.extend(tok.batch_decode(gen, skip_special_tokens=True))\n",
    "            i += cur_bs\n",
    "            pbar.update(cur_bs)\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            torch.cuda.empty_cache()\n",
    "            bs = max(1, bs // 2)\n",
    "            print(f\"⚠ OOM -> reducing batch size to {bs}\")\n",
    "            if bs == 1:\n",
    "                continue\n",
    "    pbar.close()\n",
    "    return outs\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(DATA_PROCESSED / \"train.csv\").sample(TRAIN_N, random_state=42)\n",
    "val_df   = pd.read_csv(DATA_PROCESSED / \"val.csv\").head(VAL_N)\n",
    "\n",
    "print(\"Train size:\", len(train_df), \"Val size:\", len(val_df))\n",
    "\n",
    "\n",
    "tok_hu_en = AutoTokenizer.from_pretrained(HU_EN_BASE, use_fast=True)\n",
    "model_hu_en = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    HU_EN_SAFE,\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16 if DEVICE == \"cuda\" else None\n",
    ").to(DEVICE)\n",
    "\n",
    "train_en = translate(model_hu_en, tok_hu_en, train_df[\"hu\"].tolist(), bs=16, num_beams=1)\n",
    "val_en   = translate(model_hu_en, tok_hu_en, val_df[\"hu\"].tolist(), bs=16, num_beams=1)\n",
    "\n",
    "train_p1 = train_df.copy()\n",
    "val_p1   = val_df.copy()\n",
    "train_p1[\"en_pseudo\"] = train_en\n",
    "val_p1[\"en_pseudo\"]   = val_en\n",
    "\n",
    "\n",
    "train_p1_path = OUT_PSEUDO_DIR / f\"train_hu_ro_enpseudo_{TRAIN_N}.csv\"\n",
    "val_p1_path   = OUT_PSEUDO_DIR / f\"val_hu_ro_enpseudo_{VAL_N}.csv\"\n",
    "\n",
    "train_p1.to_csv(train_p1_path, index=False)\n",
    "val_p1.to_csv(val_p1_path, index=False)\n",
    "\n",
    "print(\"Saved:\", train_p1_path)\n",
    "print(\"Saved:\", val_p1_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da7ee33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Torch: 2.5.1+cu121\n",
      "✅ Base safetensors exists: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\safetensors_bases\\Helsinki-NLP__opus-mt-hu-en\n",
      "\n",
      "[Convert] Downloading snapshot for: Helsinki-NLP/opus-mt-en-ro\n",
      "[Convert] Found single pytorch_model.bin -> torch.load\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RoG\\AppData\\Local\\Temp\\ipykernel_25452\\3399482996.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(bin_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Convert] Saving safetensors -> D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\safetensors_bases\\Helsinki-NLP__opus-mt-en-ro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59542]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pseudo data: 50000 train | 2000 val\n",
      "\n",
      "[Train] Helsinki-NLP/opus-mt-hu-en | hu → en_pseudo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Map: 100%|██████████| 50000/50000 [00:19<00:00, 2565.35 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 2560.82 examples/s]\n",
      "C:\\Users\\RoG\\AppData\\Local\\Temp\\ipykernel_25452\\3399482996.py:197: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Using LoRA\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 36:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.731800</td>\n",
       "      <td>1.656345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.730500</td>\n",
       "      <td>1.652001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.724400</td>\n",
       "      <td>1.650608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.723200</td>\n",
       "      <td>1.648507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.716300</td>\n",
       "      <td>1.647224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.717500</td>\n",
       "      <td>1.647107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 43da0b4e-4bfd-4589-b5f7-f362117eecbf)')' thrown while requesting HEAD https://huggingface.co/Helsinki-NLP/opus-mt-en-ro/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train] Helsinki-NLP/opus-mt-en-ro | en_pseudo → ro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Map: 100%|██████████| 50000/50000 [00:20<00:00, 2427.07 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 2427.18 examples/s]\n",
      "C:\\Users\\RoG\\AppData\\Local\\Temp\\ipykernel_25452\\3399482996.py:197: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Using LoRA\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 37:02, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.722100</td>\n",
       "      <td>2.654602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.728100</td>\n",
       "      <td>2.641886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.717300</td>\n",
       "      <td>2.640958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.704500</td>\n",
       "      <td>2.638611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.705600</td>\n",
       "      <td>2.636420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.681900</td>\n",
       "      <td>2.635623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Done training pivot adapters.\n"
     ]
    }
   ],
   "source": [
    "import os, gc, json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from safetensors.torch import save_file\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "BASE_SAFE_ROOT = Path(PROJECT_ROOT) / \"safetensors_bases\"\n",
    "BASE_SAFE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PSEUDO_DIR = Path(PROJECT_ROOT) / \"data\" / \"pseudo\"\n",
    "\n",
    "TRAIN_N = 50000\n",
    "VAL_N   = 2000\n",
    "train_p1_path = PSEUDO_DIR / f\"train_hu_ro_enpseudo_{TRAIN_N}.csv\"\n",
    "val_p1_path   = PSEUDO_DIR / f\"val_hu_ro_enpseudo_{VAL_N}.csv\"\n",
    "\n",
    "HU_EN_BASE = \"Helsinki-NLP/opus-mt-hu-en\"\n",
    "EN_RO_BASE = \"Helsinki-NLP/opus-mt-en-ro\"\n",
    "\n",
    "\n",
    "MAX_LEN     = 256\n",
    "BATCH_SIZE  = 4\n",
    "GRAD_ACCUM  = 4\n",
    "EPOCHS      = 2\n",
    "\n",
    "\n",
    "def ensure_base_safetensors(model_id: str, out_root: Path) -> Path:\n",
    "    out_dir = out_root / model_id.replace(\"/\", \"__\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if (out_dir / \"model.safetensors\").exists() and (out_dir / \"config.json\").exists():\n",
    "        print(\"✅ Base safetensors exists:\", out_dir)\n",
    "        return out_dir\n",
    "\n",
    "    print(f\"\\n[Convert] Downloading snapshot for: {model_id}\")\n",
    "    snap_dir = Path(snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        allow_patterns=[\n",
    "            \"config.json\",\n",
    "            \"tokenizer.json\",\n",
    "            \"tokenizer_config.json\",\n",
    "            \"source.spm\",\n",
    "            \"vocab.json\", \"merges.txt\",\n",
    "            \"pytorch_model.bin\",\n",
    "            \"pytorch_model.bin.index.json\",\n",
    "            \"pytorch_model-*.bin\",\n",
    "        ],\n",
    "        ignore_patterns=[\"*.msgpack\", \"*.h5\", \"*.ot\", \"*.tflite\", \"*.onnx\"]\n",
    "    ))\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "    bin_path = snap_dir / \"pytorch_model.bin\"\n",
    "    if bin_path.exists():\n",
    "        print(\"[Convert] Found single pytorch_model.bin -> torch.load\")\n",
    "        state = torch.load(bin_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state, strict=False)\n",
    "        del state\n",
    "    else:\n",
    "        index_path = snap_dir / \"pytorch_model.bin.index.json\"\n",
    "        if not index_path.exists():\n",
    "            raise FileNotFoundError(\"Could not find pytorch_model.bin or pytorch_model.bin.index.json\")\n",
    "\n",
    "        with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            index = json.load(f)\n",
    "        shard_files = sorted(set(index[\"weight_map\"].values()))\n",
    "        print(f\"[Convert] Found sharded weights: {len(shard_files)} shards\")\n",
    "\n",
    "        for sf in tqdm(shard_files, desc=f\"[Convert] Loading shards {model_id.split('/')[-1]}\"):\n",
    "            sp = snap_dir / sf\n",
    "            shard_state = torch.load(sp, map_location=\"cpu\")\n",
    "            model.load_state_dict(shard_state, strict=False)\n",
    "            del shard_state\n",
    "\n",
    "    print(\"[Convert] Saving safetensors ->\", out_dir)\n",
    "    model.save_pretrained(out_dir, safe_serialization=True)\n",
    "    config.save_pretrained(out_dir)\n",
    "    return out_dir\n",
    "\n",
    "HU_EN_SAFE = ensure_base_safetensors(HU_EN_BASE, BASE_SAFE_ROOT)\n",
    "EN_RO_SAFE = ensure_base_safetensors(EN_RO_BASE, BASE_SAFE_ROOT)\n",
    "\n",
    "\n",
    "def finetune_lora(\n",
    "    base_name: str,\n",
    "    base_safe_dir: Path,\n",
    "    src_col: str, tgt_col: str,\n",
    "    train_df: pd.DataFrame, val_df: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    lr_lora: float = 3e-4,\n",
    "    epochs: int = 2,\n",
    "    max_len: int = 256,\n",
    "    bs: int = 4,\n",
    "    grad_accum: int = 4,\n",
    "):\n",
    "    print(f\"\\n[Train] {base_name} | {src_col} → {tgt_col}\")\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(base_name, use_fast=True)\n",
    "\n",
    "    train_ds = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "    val_ds   = Dataset.from_pandas(val_df, preserve_index=False)\n",
    "\n",
    "    def prep(ex):\n",
    "        x = tok(ex[src_col], truncation=True, max_length=max_len)\n",
    "        y = tok(text_target=ex[tgt_col], truncation=True, max_length=max_len)\n",
    "        x[\"labels\"] = y[\"input_ids\"]\n",
    "        return x\n",
    "\n",
    "    train_tok = train_ds.map(prep, batched=True, remove_columns=train_ds.column_names)\n",
    "    val_tok   = val_ds.map(prep, batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "    base = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        base_safe_dir,\n",
    "        use_safetensors=True,\n",
    "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else None\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    base.config.use_cache = False\n",
    "    if hasattr(base, \"gradient_checkpointing_enable\"):\n",
    "        base.gradient_checkpointing_enable()\n",
    "\n",
    "    lora_cfg = LoraConfig(\n",
    "        task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "        r=8, lora_alpha=16, lora_dropout=0.05,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    )\n",
    "\n",
    "    using_lora = False\n",
    "    try:\n",
    "        model = get_peft_model(base, lora_cfg)\n",
    "        if hasattr(model, \"enable_input_require_grads\"):\n",
    "            model.enable_input_require_grads()\n",
    "        using_lora = True\n",
    "        print(\"✔ Using LoRA\")\n",
    "    except Exception as e:\n",
    "        print(\"⚠ LoRA failed, full fine-tune:\", type(e).__name__, e)\n",
    "        model = base\n",
    "\n",
    "    collator = DataCollatorForSeq2Seq(tok, model=model)\n",
    "\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=out_dir,\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs,\n",
    "        gradient_accumulation_steps=grad_accum,\n",
    "        learning_rate=(lr_lora if using_lora else 5e-5),\n",
    "        num_train_epochs=epochs,\n",
    "        fp16=(DEVICE == \"cuda\"),\n",
    "\n",
    "        # older transformers compat:\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=1000,\n",
    "\n",
    "        save_steps=1000,\n",
    "        save_total_limit=2,\n",
    "        logging_steps=100,\n",
    "        report_to=\"none\",\n",
    "        dataloader_num_workers=0,\n",
    "\n",
    "        label_smoothing_factor=0.1,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        predict_with_generate=False,\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_tok,\n",
    "        eval_dataset=val_tok,\n",
    "        tokenizer=tok,\n",
    "        data_collator=collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    return model, tok\n",
    "\n",
    "if not train_p1_path.exists() or not val_p1_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Pseudo data not found. Run Cell B first.\\n\"\n",
    "        f\"Missing: {train_p1_path} or {val_p1_path}\"\n",
    "    )\n",
    "\n",
    "train_p1 = pd.read_csv(train_p1_path)\n",
    "val_p1   = pd.read_csv(val_p1_path)\n",
    "\n",
    "print(\"Loaded pseudo data:\", len(train_p1), \"train |\", len(val_p1), \"val\")\n",
    "\n",
    "\n",
    "hu_en_ft, hu_en_ft_tok = finetune_lora(\n",
    "    HU_EN_BASE, HU_EN_SAFE,\n",
    "    src_col=\"hu\", tgt_col=\"en_pseudo\",\n",
    "    train_df=train_p1, val_df=val_p1,\n",
    "    out_dir=str(PROJECT_ROOT / \"checkpoints\" / \"hu_en_legal_lora\"),\n",
    "    epochs=EPOCHS,\n",
    "    max_len=MAX_LEN,\n",
    "    bs=BATCH_SIZE,\n",
    "    grad_accum=GRAD_ACCUM,\n",
    ")\n",
    "\n",
    "\n",
    "en_ro_ft, en_ro_ft_tok = finetune_lora(\n",
    "    EN_RO_BASE, EN_RO_SAFE,\n",
    "    src_col=\"en_pseudo\", tgt_col=\"ro\",\n",
    "    train_df=train_p1, val_df=val_p1,\n",
    "    out_dir=str(PROJECT_ROOT / \"checkpoints\" / \"en_ro_legal_lora\"),\n",
    "    epochs=EPOCHS,\n",
    "    max_len=MAX_LEN,\n",
    "    bs=BATCH_SIZE,\n",
    "    grad_accum=GRAD_ACCUM,\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Done training pivot adapters.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ad047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Torch: 2.5.1+cu121\n",
      "Loading Helsinki-NLP/opus-mt-hu-en + LoRA from: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\hu_en_legal_lora\\checkpoint-6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Helsinki-NLP/opus-mt-en-ro + LoRA from: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\en_ro_legal_lora\\checkpoint-6250\n",
      "Test size: 30366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HU→EN (legal): 100%|██████████| 30366/30366 [26:22<00:00, 19.19sent/s]\n",
      "EN→RO (legal): 100%|██████████| 30366/30366 [30:01<00:00, 16.86sent/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pivot BLEU: 38.64\n",
      "HU diacritics leak %: 0.51\n",
      "English leak proxy %: 0.07\n",
      "Glossary accuracy %: 46.56\n",
      "Saved: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\\pivot_predictions_checkpoint-6250__checkpoint-6250_beam1.csv\n"
     ]
    }
   ],
   "source": [
    "import os, gc, json, re, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "# Bases\n",
    "HU_EN_BASE = \"Helsinki-NLP/opus-mt-hu-en\"\n",
    "EN_RO_BASE = \"Helsinki-NLP/opus-mt-en-ro\"\n",
    "\n",
    "HU_EN_RUN = Path(PROJECT_ROOT) / \"checkpoints\" / \"hu_en_legal_lora\"\n",
    "EN_RO_RUN = Path(PROJECT_ROOT) / \"checkpoints\" / \"en_ro_legal_lora\"\n",
    "\n",
    "BASE_SAFE_ROOT = Path(PROJECT_ROOT) / \"safetensors_bases\"\n",
    "BASE_SAFE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_DIR = Path(OUTPUT_DIR)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MAX_INPUT_LEN  = 256\n",
    "MAX_NEW_TOKENS = 128\n",
    "BS = 16 if DEVICE == \"cuda\" else 4\n",
    "NUM_BEAMS = 1  \n",
    "LENGTH_PENALTY = 1.0\n",
    "\n",
    "\n",
    "def get_latest_checkpoint(run_dir: Path) -> Path:\n",
    "    ckpts = sorted(\n",
    "        [p for p in run_dir.glob(\"checkpoint-*\") if p.is_dir()],\n",
    "        key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else -1\n",
    "    )\n",
    "    return ckpts[-1] if ckpts else run_dir\n",
    "\n",
    "def ensure_base_safetensors(model_id: str, out_root: Path) -> Path:\n",
    "    out_dir = out_root / model_id.replace(\"/\", \"__\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if (out_dir / \"model.safetensors\").exists() and (out_dir / \"config.json\").exists():\n",
    "        return out_dir\n",
    "\n",
    "    print(f\"[Convert] Creating base safetensors for {model_id}\")\n",
    "    snap_dir = Path(snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        allow_patterns=[\n",
    "            \"config.json\",\n",
    "            \"tokenizer.json\",\n",
    "            \"tokenizer_config.json\",\n",
    "            \"source.spm\",\n",
    "            \"vocab.json\", \"merges.txt\",\n",
    "            \"pytorch_model.bin\",\n",
    "            \"pytorch_model.bin.index.json\",\n",
    "            \"pytorch_model-*.bin\",\n",
    "        ],\n",
    "    ))\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "    bin_path = snap_dir / \"pytorch_model.bin\"\n",
    "    if bin_path.exists():\n",
    "        state = torch.load(bin_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state, strict=False)\n",
    "        del state\n",
    "    else:\n",
    "        index_path = snap_dir / \"pytorch_model.bin.index.json\"\n",
    "        with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            index = json.load(f)\n",
    "        shard_files = sorted(set(index[\"weight_map\"].values()))\n",
    "        for sf in tqdm(shard_files, desc=f\"[Convert] shards {model_id.split('/')[-1]}\"):\n",
    "            shard_state = torch.load(snap_dir / sf, map_location=\"cpu\")\n",
    "            model.load_state_dict(shard_state, strict=False)\n",
    "            del shard_state\n",
    "\n",
    "    model.save_pretrained(out_dir, safe_serialization=True)\n",
    "    config.save_pretrained(out_dir)\n",
    "    return out_dir\n",
    "\n",
    "def load_lora_model(base_id: str, run_dir: Path):\n",
    "    base_safe = ensure_base_safetensors(base_id, BASE_SAFE_ROOT)\n",
    "    ckpt = get_latest_checkpoint(run_dir)\n",
    "    print(f\"Loading {base_id} + LoRA from:\", ckpt)\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(base_id, use_fast=True)\n",
    "    base = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        base_safe,\n",
    "        use_safetensors=True,\n",
    "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else None\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    model = PeftModel.from_pretrained(base, ckpt).to(DEVICE)\n",
    "    model.eval()\n",
    "    return model, tok, ckpt\n",
    "\n",
    "def batched_generate(model, tok, texts, bs=16, max_input_len=256, max_new=128, num_beams=1, length_penalty=1.0, desc=\"Gen\"):\n",
    "    outs = []\n",
    "    i = 0\n",
    "    pbar = tqdm(total=len(texts), desc=desc, unit=\"sent\")\n",
    "    while i < len(texts):\n",
    "        cur_bs = min(bs, len(texts) - i)\n",
    "        batch = texts[i:i+cur_bs]\n",
    "        try:\n",
    "            inp = tok(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_len).to(DEVICE)\n",
    "            with torch.inference_mode():\n",
    "                gen = model.generate(\n",
    "                    **inp,\n",
    "                    num_beams=num_beams,\n",
    "                    length_penalty=length_penalty,\n",
    "                    max_new_tokens=max_new,\n",
    "                    do_sample=False,\n",
    "                )\n",
    "            outs.extend(tok.batch_decode(gen, skip_special_tokens=True))\n",
    "            i += cur_bs\n",
    "            pbar.update(cur_bs)\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            torch.cuda.empty_cache()\n",
    "            bs = max(1, bs // 2)\n",
    "            print(f\"⚠ OOM -> reducing batch size to {bs}\")\n",
    "    pbar.close()\n",
    "    return outs\n",
    "\n",
    "hu_en_model, hu_en_tok, hu_en_ckpt = load_lora_model(HU_EN_BASE, HU_EN_RUN)\n",
    "en_ro_model, en_ro_tok, en_ro_ckpt = load_lora_model(EN_RO_BASE, EN_RO_RUN)\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(Path(DATA_PROCESSED) / \"test.csv\")\n",
    "src_sentences = test_df[\"hu\"].tolist()\n",
    "refs = test_df[\"ro\"].tolist()\n",
    "print(\"Test size:\", len(src_sentences))\n",
    "\n",
    "\n",
    "en_mid = batched_generate(\n",
    "    hu_en_model, hu_en_tok, src_sentences,\n",
    "    bs=BS, max_input_len=MAX_INPUT_LEN, max_new=MAX_NEW_TOKENS,\n",
    "    num_beams=NUM_BEAMS, length_penalty=LENGTH_PENALTY,\n",
    "    desc=\"HU→EN (legal)\"\n",
    ")\n",
    "\n",
    "hyps = batched_generate(\n",
    "    en_ro_model, en_ro_tok, en_mid,\n",
    "    bs=BS, max_input_len=MAX_INPUT_LEN, max_new=MAX_NEW_TOKENS,\n",
    "    num_beams=NUM_BEAMS, length_penalty=LENGTH_PENALTY,\n",
    "    desc=\"EN→RO (legal)\"\n",
    ")\n",
    "\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n",
    "print(\"\\nPivot BLEU:\", round(bleu, 2))\n",
    "\n",
    "hu_diacritics = set(\"áéíóöőúüűÁÉÍÓÖŐÚÜŰ\")\n",
    "def has_hu_diacritics(s: str) -> bool:\n",
    "    return any(c in hu_diacritics for c in s)\n",
    "\n",
    "def en_leak(s: str) -> bool:\n",
    "    s = \" \" + re.sub(r\"\\s+\", \" \", s.lower()) + \" \"\n",
    "    return any(w in s for w in [\" the \", \" and \", \" of \", \" to \", \" for \", \" with \", \" on \"])\n",
    "\n",
    "hu_leak = np.mean([has_hu_diacritics(h) for h in hyps]) * 100\n",
    "en_leak_rate = np.mean([en_leak(h) for h in hyps]) * 100\n",
    "print(\"HU diacritics leak %:\", round(hu_leak, 2))\n",
    "print(\"English leak proxy %:\", round(en_leak_rate, 2))\n",
    "\n",
    "\n",
    "def norm_ro(s):\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"ţ\",\"ț\").replace(\"ş\",\"ș\")\n",
    "    return unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "glossary_norm = {norm_ro(k): [norm_ro(v) for v in vs] for k, vs in glossary.items()}\n",
    "\n",
    "def glossary_hit(src: str, hyp: str) -> list:\n",
    "    src_n, hyp_n = norm_ro(src), norm_ro(hyp)\n",
    "    checks = []\n",
    "    for src_term, ro_forms in glossary_norm.items():\n",
    "        if src_term in src_n:\n",
    "            checks.append(any(f in hyp_n for f in ro_forms))\n",
    "    return checks\n",
    "\n",
    "hits = [glossary_hit(s, h) for s, h in zip(src_sentences, hyps)]\n",
    "flat = [x for row in hits for x in row]\n",
    "if flat:\n",
    "    print(\"Glossary accuracy %:\", round(np.mean(flat) * 100, 2))\n",
    "else:\n",
    "    print(\"Glossary accuracy: (no glossary terms found in test)\")\n",
    "\n",
    "\n",
    "out_path = OUT_DIR / f\"pivot_predictions_{hu_en_ckpt.name}__{en_ro_ckpt.name}_beam{NUM_BEAMS}.csv\"\n",
    "pd.DataFrame({\n",
    "    \"source_hu\": src_sentences,\n",
    "    \"pivot_en\": en_mid,\n",
    "    \"reference_ro\": refs,\n",
    "    \"hypothesis_ro\": hyps\n",
    "}).to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n",
    "\n",
    "\n",
    "# try:\n",
    "#     from comet import download_model, load_from_checkpoint\n",
    "#     N = 1000\n",
    "#     comet_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "#     comet_model = load_from_checkpoint(comet_path)\n",
    "#     data = [{\"src\": s, \"mt\": m, \"ref\": r} for s,m,r in zip(src_sentences[:N], hyps[:N], refs[:N])]\n",
    "#     out = comet_model.predict(data, batch_size=8, gpus=1 if DEVICE==\"cuda\" else 0)\n",
    "#     print(\"COMET (first 1000):\", round(float(np.mean(out.scores)), 4))\n",
    "# except Exception as e:\n",
    "#     print(\"COMET skipped:\", type(e).__name__, e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58da67b5",
   "metadata": {},
   "source": [
    "## 4 beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c21ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Torch: 2.5.1+cu121\n",
      "Loading Helsinki-NLP/opus-mt-hu-en + LoRA from: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\hu_en_legal_lora\\checkpoint-6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\nn\\modules\\module.py:1326: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Helsinki-NLP/opus-mt-en-ro + LoRA from: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\en_ro_legal_lora\\checkpoint-6250\n",
      "Test size: 30366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HU→EN (legal) [normal]: 100%|██████████| 29820/29820 [1:20:20<00:00,  6.19sent/s]\n",
      "HU→EN (legal) [table]: 100%|██████████| 546/546 [00:12<00:00, 44.88sent/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building per-sentence glossary constraints (conservative)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Glossary constraints: 100%|██████████| 30366/30366 [00:00<00:00, 2169378.90it/s]\n",
      "EN→RO (legal) [normal]: 100%|██████████| 30200/30200 [1:28:12<00:00,  5.71sent/s]\n",
      "EN→RO (legal) [table]: 100%|██████████| 166/166 [00:02<00:00, 63.02sent/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pivot BLEU: 38.13\n",
      "HU diacritics leak %: 0.53\n",
      "English leak proxy %: 0.11\n",
      "Glossary accuracy: skipped (no glossary loaded)\n",
      "Saved: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\\pivot_predictions_checkpoint-6250__checkpoint-6250_beam4_antiRepeat.csv\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os, gc, json, re, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "HU_EN_BASE = \"Helsinki-NLP/opus-mt-hu-en\"\n",
    "EN_RO_BASE = \"Helsinki-NLP/opus-mt-en-ro\"\n",
    "\n",
    "HU_EN_RUN = Path(PROJECT_ROOT) / \"checkpoints\" / \"hu_en_legal_lora\"\n",
    "EN_RO_RUN = Path(PROJECT_ROOT) / \"checkpoints\" / \"en_ro_legal_lora\"\n",
    "\n",
    "BASE_SAFE_ROOT = Path(PROJECT_ROOT) / \"safetensors_bases\"\n",
    "BASE_SAFE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_DIR = Path(OUTPUT_DIR)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# FINAL decode settings (beam=4 + anti-repeat)\n",
    "NUM_BEAMS = 4\n",
    "LENGTH_PENALTY = 1.0\n",
    "NO_REPEAT_NGRAM = 3\n",
    "REPETITION_PENALTY = 1.10\n",
    "\n",
    "MAX_INPUT_LEN  = 256\n",
    "MAX_NEW_TOKENS_NORMAL = 128\n",
    "\n",
    "\n",
    "MAX_NEW_TOKENS_TABLE = 32\n",
    "NUM_BEAMS_TABLE = 1\n",
    "\n",
    "BS = 12 if DEVICE == \"cuda\" else 4  \n",
    "\n",
    "\n",
    "USE_GLOSSARY_CONSTRAINTS = True\n",
    "MAX_FORCED_TERMS_PER_SENT = 3\n",
    "\n",
    "\n",
    "def get_latest_checkpoint(run_dir: Path) -> Path:\n",
    "    ckpts = sorted(\n",
    "        [p for p in run_dir.glob(\"checkpoint-*\") if p.is_dir()],\n",
    "        key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else -1\n",
    "    )\n",
    "    return ckpts[-1] if ckpts else run_dir\n",
    "\n",
    "def ensure_base_safetensors(model_id: str, out_root: Path) -> Path:\n",
    "    out_dir = out_root / model_id.replace(\"/\", \"__\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if (out_dir / \"model.safetensors\").exists() and (out_dir / \"config.json\").exists():\n",
    "        return out_dir\n",
    "\n",
    "    print(f\"[Convert] Creating base safetensors for {model_id}\")\n",
    "    snap_dir = Path(snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        allow_patterns=[\n",
    "            \"config.json\",\n",
    "            \"tokenizer.json\",\n",
    "            \"tokenizer_config.json\",\n",
    "            \"source.spm\",\n",
    "            \"vocab.json\", \"merges.txt\",\n",
    "            \"pytorch_model.bin\",\n",
    "            \"pytorch_model.bin.index.json\",\n",
    "            \"pytorch_model-*.bin\",\n",
    "        ],\n",
    "    ))\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "    bin_path = snap_dir / \"pytorch_model.bin\"\n",
    "    if bin_path.exists():\n",
    "        state = torch.load(bin_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state, strict=False)\n",
    "        del state\n",
    "    else:\n",
    "        index_path = snap_dir / \"pytorch_model.bin.index.json\"\n",
    "        with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            index = json.load(f)\n",
    "        shard_files = sorted(set(index[\"weight_map\"].values()))\n",
    "        for sf in tqdm(shard_files, desc=f\"[Convert] shards {model_id.split('/')[-1]}\"):\n",
    "            shard_state = torch.load(snap_dir / sf, map_location=\"cpu\")\n",
    "            model.load_state_dict(shard_state, strict=False)\n",
    "            del shard_state\n",
    "\n",
    "    model.save_pretrained(out_dir, safe_serialization=True)\n",
    "    config.save_pretrained(out_dir)\n",
    "    return out_dir\n",
    "\n",
    "def load_lora_model(base_id: str, run_dir: Path):\n",
    "    base_safe = ensure_base_safetensors(base_id, BASE_SAFE_ROOT)\n",
    "    ckpt = get_latest_checkpoint(run_dir)\n",
    "    print(f\"Loading {base_id} + LoRA from:\", ckpt)\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(base_id, use_fast=True)\n",
    "    base = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        base_safe,\n",
    "        use_safetensors=True,\n",
    "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else None\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    model = PeftModel.from_pretrained(base, ckpt).to(DEVICE)\n",
    "    model.eval()\n",
    "    return model, tok, ckpt\n",
    "\n",
    "\n",
    "_punct = set(list(\"0123456789.,;:-/()[]{}<>|_+*=—–%°\\\"' \\t\"))\n",
    "def is_table_like(s: str) -> bool:\n",
    "    if s is None:\n",
    "        return True\n",
    "    t = str(s).strip()\n",
    "    if len(t) == 0:\n",
    "        return True\n",
    "    if \"_BAR_\" in t or t.count(\"|\") >= 2:\n",
    "        return True\n",
    "   \n",
    "    good = sum(ch in _punct for ch in t)\n",
    "    if good / max(1, len(t)) > 0.80:\n",
    "        return True\n",
    "    # Many long dash runs\n",
    "    if re.search(r\"[-—–]{6,}\", t):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def norm_ro(s):\n",
    "    s = s.lower()\n",
    "    s = s.replace(\"ţ\",\"ț\").replace(\"ş\",\"ș\")\n",
    "    return unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "def build_force_words_ids_for_sentence(en_ro_tok, hu_src: str):\n",
    "  \n",
    "    \n",
    "    if not USE_GLOSSARY_CONSTRAINTS:\n",
    "        return None\n",
    "    if \"glossary\" not in globals() or glossary is None:\n",
    "        return None\n",
    "\n",
    "    forced = []\n",
    "    hu_s = str(hu_src)\n",
    "    for hu_term, ro_forms in glossary.items():\n",
    "        if hu_term in hu_s and ro_forms:\n",
    "            ro_form = ro_forms[0]\n",
    "            ids = en_ro_tok(ro_form, add_special_tokens=False).input_ids\n",
    "            if ids:\n",
    "                forced.append(ids)\n",
    "        if len(forced) >= MAX_FORCED_TERMS_PER_SENT:\n",
    "            break\n",
    "\n",
    "    return forced if forced else None\n",
    "\n",
    "def batched_generate_adaptive(\n",
    "    model, tok,\n",
    "    texts,\n",
    "    bs: int,\n",
    "    desc: str,\n",
    "    max_input_len: int,\n",
    "   \n",
    "    num_beams: int,\n",
    "    max_new_tokens: int,\n",
    "    length_penalty: float,\n",
    "    no_repeat_ngram_size: int,\n",
    "    repetition_penalty: float,\n",
    "   \n",
    "    num_beams_table: int,\n",
    "    max_new_tokens_table: int,\n",
    "    per_sentence_force_words_ids=None\n",
    "):\n",
    "    outs = [None] * len(texts)\n",
    "\n",
    "    idx_table = [i for i, s in enumerate(texts) if is_table_like(s)]\n",
    "    idx_norm  = [i for i in range(len(texts)) if i not in set(idx_table)]\n",
    "\n",
    "    def _run(indices, beams, max_new, tag):\n",
    "        if not indices:\n",
    "            return\n",
    "        i = 0\n",
    "        pbar = tqdm(total=len(indices), desc=f\"{desc} [{tag}]\", unit=\"sent\")\n",
    "        cur_bs = bs\n",
    "        while i < len(indices):\n",
    "            take = min(cur_bs, len(indices) - i)\n",
    "            batch_idx = indices[i:i+take]\n",
    "            batch_txt = [texts[j] for j in batch_idx]\n",
    "\n",
    "            try:\n",
    "                inp = tok(\n",
    "                    batch_txt,\n",
    "                    return_tensors=\"pt\",\n",
    "                    padding=True,\n",
    "                    truncation=True,\n",
    "                    max_length=max_input_len\n",
    "                ).to(DEVICE)\n",
    "\n",
    "               \n",
    "                gen_kwargs = dict(\n",
    "                    num_beams=beams,\n",
    "                    length_penalty=length_penalty if tag == \"normal\" else 1.0,\n",
    "                    max_new_tokens=max_new,\n",
    "                    do_sample=False,\n",
    "                    no_repeat_ngram_size=no_repeat_ngram_size if tag == \"normal\" else 0,\n",
    "                    repetition_penalty=repetition_penalty if tag == \"normal\" else 1.0,\n",
    "                )\n",
    "\n",
    "             \n",
    "                if per_sentence_force_words_ids is not None and tag == \"normal\":\n",
    "                    \n",
    "                    any_forced = any(per_sentence_force_words_ids[j] for j in batch_idx)\n",
    "                    if any_forced:\n",
    "                        \n",
    "                        decoded = []\n",
    "                        for j, txt in zip(batch_idx, batch_txt):\n",
    "                            one_inp = tok(txt, return_tensors=\"pt\", truncation=True, max_length=max_input_len).to(DEVICE)\n",
    "                            one_kwargs = dict(gen_kwargs)\n",
    "                            fw = per_sentence_force_words_ids[j]\n",
    "                            if fw:\n",
    "                                one_kwargs[\"force_words_ids\"] = fw\n",
    "                            with torch.inference_mode():\n",
    "                                one_gen = model.generate(**one_inp, **one_kwargs)\n",
    "                            decoded.append(tok.batch_decode(one_gen, skip_special_tokens=True)[0])\n",
    "                        for j, d in zip(batch_idx, decoded):\n",
    "                            outs[j] = d\n",
    "                        i += take\n",
    "                        pbar.update(take)\n",
    "                        continue\n",
    "\n",
    "                with torch.inference_mode():\n",
    "                    gen = model.generate(**inp, **gen_kwargs)\n",
    "\n",
    "                decoded = tok.batch_decode(gen, skip_special_tokens=True)\n",
    "                for j, d in zip(batch_idx, decoded):\n",
    "                    outs[j] = d\n",
    "\n",
    "                i += take\n",
    "                pbar.update(take)\n",
    "\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                torch.cuda.empty_cache()\n",
    "                cur_bs = max(1, cur_bs // 2)\n",
    "                print(f\"⚠ OOM in {desc}/{tag} -> reducing batch size to {cur_bs}\")\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "    _run(idx_norm,  num_beams,       max_new_tokens,       \"normal\")\n",
    "    _run(idx_table, num_beams_table, max_new_tokens_table, \"table\")\n",
    "\n",
    "    assert all(o is not None for o in outs), \"Some generations failed unexpectedly.\"\n",
    "    return outs\n",
    "\n",
    "\n",
    "hu_en_model, hu_en_tok, hu_en_ckpt = load_lora_model(HU_EN_BASE, HU_EN_RUN)\n",
    "en_ro_model, en_ro_tok, en_ro_ckpt = load_lora_model(EN_RO_BASE, EN_RO_RUN)\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(Path(DATA_PROCESSED) / \"test.csv\")\n",
    "src_sentences = test_df[\"hu\"].tolist()\n",
    "refs = test_df[\"ro\"].tolist()\n",
    "print(\"Test size:\", len(src_sentences))\n",
    "\n",
    "en_mid = batched_generate_adaptive(\n",
    "    hu_en_model, hu_en_tok,\n",
    "    src_sentences,\n",
    "    bs=BS,\n",
    "    desc=\"HU→EN (legal)\",\n",
    "    max_input_len=MAX_INPUT_LEN,\n",
    "    num_beams=NUM_BEAMS,\n",
    "    max_new_tokens=MAX_NEW_TOKENS_NORMAL,\n",
    "    length_penalty=LENGTH_PENALTY,\n",
    "    no_repeat_ngram_size=NO_REPEAT_NGRAM,\n",
    "    repetition_penalty=REPETITION_PENALTY,\n",
    "    num_beams_table=NUM_BEAMS_TABLE,\n",
    "    max_new_tokens_table=MAX_NEW_TOKENS_TABLE,\n",
    "    per_sentence_force_words_ids=None\n",
    ")\n",
    "\n",
    "\n",
    "force_ids = None\n",
    "if USE_GLOSSARY_CONSTRAINTS:\n",
    "    print(\"Building per-sentence glossary constraints (conservative)...\")\n",
    "    force_ids = [build_force_words_ids_for_sentence(en_ro_tok, hu) for hu in tqdm(src_sentences, desc=\"Glossary constraints\")]\n",
    "\n",
    "hyps = batched_generate_adaptive(\n",
    "    en_ro_model, en_ro_tok,\n",
    "    en_mid,\n",
    "    bs=BS,\n",
    "    desc=\"EN→RO (legal)\",\n",
    "    max_input_len=MAX_INPUT_LEN,\n",
    "    num_beams=NUM_BEAMS,\n",
    "    max_new_tokens=MAX_NEW_TOKENS_NORMAL,\n",
    "    length_penalty=LENGTH_PENALTY,\n",
    "    no_repeat_ngram_size=NO_REPEAT_NGRAM,\n",
    "    repetition_penalty=REPETITION_PENALTY,\n",
    "    num_beams_table=NUM_BEAMS_TABLE,\n",
    "    max_new_tokens_table=MAX_NEW_TOKENS_TABLE,\n",
    "    per_sentence_force_words_ids=force_ids\n",
    ")\n",
    "\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n",
    "print(\"\\nPivot BLEU:\", round(bleu, 2))\n",
    "\n",
    "hu_diacritics = set(\"áéíóöőúüűÁÉÍÓÖŐÚÜŰ\")\n",
    "def has_hu_diacritics(s: str) -> bool:\n",
    "    return any(c in hu_diacritics for c in s)\n",
    "\n",
    "def en_leak(s: str) -> bool:\n",
    "    s = \" \" + re.sub(r\"\\s+\", \" \", str(s).lower()) + \" \"\n",
    "    return any(w in s for w in [\" the \", \" and \", \" of \", \" to \", \" for \", \" with \", \" on \"])\n",
    "\n",
    "hu_leak = np.mean([has_hu_diacritics(h) for h in hyps]) * 100\n",
    "en_leak_rate = np.mean([en_leak(h) for h in hyps]) * 100\n",
    "print(\"HU diacritics leak %:\", round(hu_leak, 2))\n",
    "print(\"English leak proxy %:\", round(en_leak_rate, 2))\n",
    "\n",
    "\n",
    "def norm_ro(s):\n",
    "    s = str(s).lower()\n",
    "    s = s.replace(\"ţ\",\"ț\").replace(\"ş\",\"ș\")\n",
    "    return unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "glossary_norm = {k: [norm_ro(v) for v in vs] for k, vs in glossary.items()} if \"glossary\" in globals() else {}\n",
    "\n",
    "def glossary_hit(src: str, hyp: str) -> list:\n",
    "    hyp_n = norm_ro(hyp)\n",
    "    checks = []\n",
    "    for hu_term, ro_forms in glossary_norm.items():\n",
    "        if hu_term in str(src):\n",
    "            checks.append(any(f in hyp_n for f in ro_forms))\n",
    "    return checks\n",
    "\n",
    "if glossary_norm:\n",
    "    hits = [glossary_hit(s, h) for s, h in zip(src_sentences, hyps)]\n",
    "    flat = [x for row in hits for x in row]\n",
    "    if flat:\n",
    "        print(\"Glossary accuracy %:\", round(np.mean(flat) * 100, 2))\n",
    "    else:\n",
    "        print(\"Glossary accuracy: (no glossary terms found in test)\")\n",
    "else:\n",
    "    print(\"Glossary accuracy: skipped (no glossary loaded)\")\n",
    "\n",
    "\n",
    "out_path = OUT_DIR / f\"pivot_predictions_{hu_en_ckpt.name}__{en_ro_ckpt.name}_beam{NUM_BEAMS}_antiRepeat.csv\"\n",
    "pd.DataFrame({\n",
    "    \"source_hu\": src_sentences,\n",
    "    \"pivot_en\": en_mid,\n",
    "    \"reference_ro\": refs,\n",
    "    \"hypothesis_ro\": hyps\n",
    "}).to_csv(out_path, index=False)\n",
    "\n",
    "print(\"Saved:\", out_path)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049b854e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote glossary: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\glossary.csv\n",
      "Entries: 500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hu</th>\n",
       "      <th>ro</th>\n",
       "      <th>count</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rendelet</td>\n",
       "      <td>Regulamentul|CEE|CE|Regulamentului|anexa</td>\n",
       "      <td>10427</td>\n",
       "      <td>58937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Bizottság</td>\n",
       "      <td>Comisia|Comisiei|Comisie|CE|În</td>\n",
       "      <td>7088</td>\n",
       "      <td>42319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>irányelv</td>\n",
       "      <td>Directiva|CEE|CE|anexa|Directivei</td>\n",
       "      <td>6457</td>\n",
       "      <td>40351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tagállamok</td>\n",
       "      <td>Statele|Comisiei|Comisia|CEE|În</td>\n",
       "      <td>6332</td>\n",
       "      <td>33337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>tanácsi</td>\n",
       "      <td>CEE|Regulamentul|CE|Directiva|Regulamentul Con...</td>\n",
       "      <td>5597</td>\n",
       "      <td>39791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cikke</td>\n",
       "      <td>CEE|Regulamentul|CE|Directiva|Regulamentul Con...</td>\n",
       "      <td>3982</td>\n",
       "      <td>30115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>mellékletben</td>\n",
       "      <td>anexa|II</td>\n",
       "      <td>2711</td>\n",
       "      <td>10752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rendelettel</td>\n",
       "      <td>Regulamentul|CEE|CE|Regulamentul Consiliului|R...</td>\n",
       "      <td>2650</td>\n",
       "      <td>12772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>tagállam</td>\n",
       "      <td>statul|În|Comisia|Dacă|Comisiei</td>\n",
       "      <td>2538</td>\n",
       "      <td>16640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Közösség</td>\n",
       "      <td>Comunităţii|Comunitate|Comunitatea|CEE|În</td>\n",
       "      <td>2416</td>\n",
       "      <td>18043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>cikkének</td>\n",
       "      <td>CEE|Regulamentul|CE|Directiva|Tratat</td>\n",
       "      <td>2410</td>\n",
       "      <td>20809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>melléklet</td>\n",
       "      <td>anexa|II|III|Anexa</td>\n",
       "      <td>2305</td>\n",
       "      <td>9574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bizottsági</td>\n",
       "      <td>Regulamentul|CEE|CE|Regulamentul Comisiei|Comi...</td>\n",
       "      <td>2031</td>\n",
       "      <td>16779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>rendelete</td>\n",
       "      <td>REGULAMENTUL|CE|REGULAMENTUL COMISIEI|CEE|REGU...</td>\n",
       "      <td>1902</td>\n",
       "      <td>6593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Bizottságot</td>\n",
       "      <td>Comisia|Comisiei|Statele</td>\n",
       "      <td>1847</td>\n",
       "      <td>7562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Tanács</td>\n",
       "      <td>Consiliul|Consiliului|CE|Comisiei|CEE</td>\n",
       "      <td>1750</td>\n",
       "      <td>18128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>rendeletre</td>\n",
       "      <td>Regulamentul|Regulamentul Consiliului|CEE|CE|C...</td>\n",
       "      <td>1707</td>\n",
       "      <td>7929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>határozat</td>\n",
       "      <td>Decizia|CE|CEE|anexa|În</td>\n",
       "      <td>1657</td>\n",
       "      <td>12974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Szerződés</td>\n",
       "      <td>Tratat|Comisiei</td>\n",
       "      <td>1610</td>\n",
       "      <td>6979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>mellékletében</td>\n",
       "      <td>anexa|CEE|Regulamentul|CE|Directiva</td>\n",
       "      <td>1596</td>\n",
       "      <td>8875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                hu                                                 ro  count  \\\n",
       "6         rendelet           Regulamentul|CEE|CE|Regulamentului|anexa  10427   \n",
       "18       Bizottság                     Comisia|Comisiei|Comisie|CE|În   7088   \n",
       "25        irányelv                  Directiva|CEE|CE|anexa|Directivei   6457   \n",
       "0       tagállamok                    Statele|Comisiei|Comisia|CEE|În   6332   \n",
       "15         tanácsi  CEE|Regulamentul|CE|Directiva|Regulamentul Con...   5597   \n",
       "9            cikke  CEE|Regulamentul|CE|Directiva|Regulamentul Con...   3982   \n",
       "35    mellékletben                                           anexa|II   2711   \n",
       "2      rendelettel  Regulamentul|CEE|CE|Regulamentul Consiliului|R...   2650   \n",
       "34        tagállam                    statul|În|Comisia|Dacă|Comisiei   2538   \n",
       "30        Közösség          Comunităţii|Comunitate|Comunitatea|CEE|În   2416   \n",
       "63        cikkének               CEE|Regulamentul|CE|Directiva|Tratat   2410   \n",
       "86       melléklet                                 anexa|II|III|Anexa   2305   \n",
       "4       bizottsági  Regulamentul|CEE|CE|Regulamentul Comisiei|Comi...   2031   \n",
       "70       rendelete  REGULAMENTUL|CE|REGULAMENTUL COMISIEI|CEE|REGU...   1902   \n",
       "37     Bizottságot                           Comisia|Comisiei|Statele   1847   \n",
       "71          Tanács              Consiliul|Consiliului|CE|Comisiei|CEE   1750   \n",
       "45      rendeletre  Regulamentul|Regulamentul Consiliului|CEE|CE|C...   1707   \n",
       "41       határozat                            Decizia|CE|CEE|anexa|În   1657   \n",
       "73       Szerződés                                    Tratat|Comisiei   1610   \n",
       "142  mellékletében                anexa|CEE|Regulamentul|CE|Directiva   1596   \n",
       "\n",
       "     total  \n",
       "6    58937  \n",
       "18   42319  \n",
       "25   40351  \n",
       "0    33337  \n",
       "15   39791  \n",
       "9    30115  \n",
       "35   10752  \n",
       "2    12772  \n",
       "34   16640  \n",
       "30   18043  \n",
       "63   20809  \n",
       "86    9574  \n",
       "4    16779  \n",
       "70    6593  \n",
       "37    7562  \n",
       "71   18128  \n",
       "45    7929  \n",
       "41   12974  \n",
       "73    6979  \n",
       "142   8875  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "TRAIN_PATH = Path(DATA_PROCESSED) / \"train.csv\"\n",
    "OUT_PATH = Path(PROJECT_ROOT) / \"data\" / \"glossary.csv\"\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(TRAIN_PATH)\n",
    "\n",
    "hu = df[\"hu\"].astype(str).tolist()\n",
    "ro = df[\"ro\"].astype(str).tolist()\n",
    "\n",
    "\n",
    "LEGAL_HINTS_HU = [\n",
    "    \"bizottság\", \"tanács\", \"parlament\", \"rendelet\", \"irányelv\", \"határozat\",\n",
    "    \"cikk\", \"bekezdés\", \"melléklet\", \"fejezet\", \"szakasz\",\n",
    "    \"közösség\", \"unió\", \"szerződés\",\n",
    "    \"európai\", \"bíróság\", \"biztos\", \"tagállam\"\n",
    "]\n",
    "\n",
    "\n",
    "cap_phrase = re.compile(r\"\\b([A-ZÁÉÍÓÖŐÚÜŰ][\\wÁÉÍÓÖŐÚÜŰáéíóöőúüű\\-]+(?:\\s+[A-ZÁÉÍÓÖŐÚÜŰ][\\wÁÉÍÓÖŐÚÜŰáéíóöőúüű\\-]+){0,4})\\b\")\n",
    "word = re.compile(r\"\\b[\\wÁÉÍÓÖŐÚÜŰáéíóöőúüű\\-]{4,}\\b\")\n",
    "\n",
    "def extract_candidates(sent: str):\n",
    "    sent_l = sent.lower()\n",
    "    out = set()\n",
    "\n",
    "    for m in cap_phrase.finditer(sent):\n",
    "        t = m.group(1).strip()\n",
    "        if len(t) >= 4:\n",
    "            out.add(t)\n",
    "\n",
    "    \n",
    "    for w_ in word.findall(sent):\n",
    "        wl = w_.lower()\n",
    "        if any(h in wl for h in LEGAL_HINTS_HU):\n",
    "            out.add(w_)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "cap_phrase_ro = re.compile(r\"\\b([A-ZĂÂÎȘȚ][\\wĂÂÎȘȚăâîșț\\-]+(?:\\s+[A-ZĂÂÎȘȚ][\\wĂÂÎȘȚăâîșț\\-]+){0,4})\\b\")\n",
    "legal_ro_words = re.compile(r\"\\b(comisia|consiliul|parlamentul|regulamentul|directiva|decizia|articolul|alineatul|anexa|capitolul|secțiunea|uniunea|comunitatea|curtea|statul)\\b\", re.IGNORECASE)\n",
    "\n",
    "def ro_signals(sent: str):\n",
    "    s = sent.strip()\n",
    "    outs = []\n",
    "\n",
    "    outs += [m.group(1).strip() for m in cap_phrase_ro.finditer(s)]\n",
    "\n",
    "    \n",
    "    outs += [m.group(0).strip() for m in legal_ro_words.finditer(s)]\n",
    "\n",
    "    \n",
    "    seen = set()\n",
    "    out2 = []\n",
    "    for x in outs:\n",
    "        xl = x.lower()\n",
    "        if xl not in seen:\n",
    "            seen.add(xl)\n",
    "            out2.append(x)\n",
    "    return out2\n",
    "\n",
    "pair_counts = defaultdict(Counter)\n",
    "\n",
    "\n",
    "N = min(200000, len(hu))\n",
    "for s_hu, s_ro in zip(hu[:N], ro[:N]):\n",
    "    cands = extract_candidates(s_hu)\n",
    "    ro_sigs = ro_signals(s_ro)\n",
    "    if not ro_sigs:\n",
    "        continue\n",
    "    for term in cands:\n",
    "        for r_ in ro_sigs:\n",
    "            pair_counts[term][r_] += 1\n",
    "\n",
    "\n",
    "rows = []\n",
    "for term, ctr in pair_counts.items():\n",
    "    total = sum(ctr.values())\n",
    "    if total < 30:  \n",
    "        continue\n",
    "    top = ctr.most_common(5)\n",
    "    ro_forms = [r for r, c in top if c >= max(5, 0.15 * top[0][1])]\n",
    "    if ro_forms:\n",
    "        rows.append({\"hu\": term, \"ro\": \"|\".join(ro_forms), \"count\": top[0][1], \"total\": total})\n",
    "\n",
    "gloss_df = pd.DataFrame(rows).sort_values([\"count\", \"total\"], ascending=False)\n",
    "\n",
    "gloss_df = gloss_df.head(500)\n",
    "\n",
    "gloss_df[[\"hu\", \"ro\"]].to_csv(OUT_PATH, index=False)\n",
    "print(\"✅ Wrote glossary:\", OUT_PATH)\n",
    "print(\"Entries:\", len(gloss_df))\n",
    "gloss_df.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c4360c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded glossary entries: 500\n",
      "\n",
      "=== Glossary coverage on TEST ===\n",
      "Test sentences: 30366\n",
      "Sentences containing ≥1 glossary term: 21210\n",
      "Unique glossary terms that appear in test: 500  / 500\n",
      "Total term mentions (all occurrences): 83464\n",
      "Reference-side hit rate (upper bound realism): 90.59 %\n",
      "\n",
      "Top glossary terms in TEST (by occurrences)\n",
      "cikk                                      occ=6560  hit=5737  miss= 823\n",
      "tagállam                                  occ=5154  hit=4641  miss= 513\n",
      "bekezdés                                  occ=4557  hit=3961  miss= 596\n",
      "rendelet                                  occ=4248  hit=3761  miss= 487\n",
      "Bizottság                                 occ=3270  hit=3245  miss=  25\n",
      "tagállamok                                occ=3226  hit=3115  miss= 111\n",
      "irányelv                                  occ=3183  hit=2839  miss= 344\n",
      "melléklet                                 occ=2077  hit=1898  miss= 179\n",
      "Közösség                                  occ=2024  hit=1964  miss=  60\n",
      "biztosít                                  occ=1795  hit=1689  miss= 106\n",
      "cikke                                     occ=1617  hit=1375  miss= 242\n",
      "tanács                                    occ=1540  hit=1506  miss=  34\n",
      "közösségi                                 occ=1367  hit=1159  miss= 208\n",
      "bizottság                                 occ=1358  hit=1260  miss=  98\n",
      "tanácsi                                   occ=1300  hit=1285  miss=  15\n",
      "határozat                                 occ=1292  hit=1224  miss=  68\n",
      "cikkben                                   occ=1136  hit=1049  miss=  87\n",
      "bekezdésében                              occ=1062  hit= 842  miss= 220\n",
      "Európai                                   occ=1033  hit= 584  miss= 449\n",
      "bekezdésének                              occ= 926  hit= 874  miss=  52\n",
      "bekezdése                                 occ= 904  hit= 645  miss= 259\n",
      "cikkének                                  occ= 883  hit= 828  miss=  55\n",
      "Tanács                                    occ= 882  hit= 843  miss=  39\n",
      "bekezdésben                               occ= 874  hit= 808  miss=  66\n",
      "mellékletbe                               occ= 672  hit= 630  miss=  42\n",
      "\n",
      "Top MISSED glossary terms in TEST (ref doesn't contain expected RO forms)\n",
      "cikk                                      occ=6560  hit=5737  miss= 823\n",
      "bekezdés                                  occ=4557  hit=3961  miss= 596\n",
      "tagállam                                  occ=5154  hit=4641  miss= 513\n",
      "rendelet                                  occ=4248  hit=3761  miss= 487\n",
      "Európai                                   occ=1033  hit= 584  miss= 449\n",
      "irányelv                                  occ=3183  hit=2839  miss= 344\n",
      "bekezdése                                 occ= 904  hit= 645  miss= 259\n",
      "cikke                                     occ=1617  hit=1375  miss= 242\n",
      "bekezdésében                              occ=1062  hit= 842  miss= 220\n",
      "közösségi                                 occ=1367  hit=1159  miss= 208\n",
      "Hivatal                                   occ= 353  hit= 153  miss= 200\n",
      "melléklet                                 occ=2077  hit=1898  miss= 179\n",
      "Gazdasági                                 occ= 192  hit=  66  miss= 126\n",
      "Európai Közösség                          occ= 457  hit= 344  miss= 113\n",
      "tagállamok                                occ=3226  hit=3115  miss= 111\n",
      "biztosít                                  occ=1795  hit=1689  miss= 106\n",
      "rendeletbe                                occ= 374  hit= 268  miss= 106\n",
      "bizottság                                 occ=1358  hit=1260  miss=  98\n",
      "cikkben                                   occ=1136  hit=1049  miss=  87\n",
      "szerződés                                 occ= 628  hit= 554  miss=  74\n",
      "rendelete                                 occ= 602  hit= 530  miss=  72\n",
      "határozat                                 occ=1292  hit=1224  miss=  68\n",
      "Ezek                                      occ= 163  hit=  96  miss=  67\n",
      "bekezdésben                               occ= 874  hit= 808  miss=  66\n",
      "albekezdés                                occ= 471  hit= 407  miss=  64\n",
      "\n",
      "✅ Wrote: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\glossary_hard_terms.csv\n",
      "These are frequent terms where your RO variants likely need cleanup/expansion.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hu_term</th>\n",
       "      <th>occurrences</th>\n",
       "      <th>hits</th>\n",
       "      <th>hit_rate_%</th>\n",
       "      <th>current_ro_forms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Európai</td>\n",
       "      <td>1033</td>\n",
       "      <td>584</td>\n",
       "      <td>56.534366</td>\n",
       "      <td>Comunităţilor Europene|Jurnalul Oficial|Tratat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hivatal</td>\n",
       "      <td>353</td>\n",
       "      <td>153</td>\n",
       "      <td>43.342776</td>\n",
       "      <td>Biroul|Oficiului|Biroului|Oficiul|Dacă</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gazdasági</td>\n",
       "      <td>192</td>\n",
       "      <td>66</td>\n",
       "      <td>34.375000</td>\n",
       "      <td>Comitetului Economic|Social|Comisia|Consiliulu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ezek</td>\n",
       "      <td>163</td>\n",
       "      <td>96</td>\n",
       "      <td>58.895706</td>\n",
       "      <td>Aceste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Azok</td>\n",
       "      <td>101</td>\n",
       "      <td>44</td>\n",
       "      <td>43.564356</td>\n",
       "      <td>Statele|Comisia|CEE|Regulamentul|Comisiei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alap</td>\n",
       "      <td>64</td>\n",
       "      <td>34</td>\n",
       "      <td>53.125000</td>\n",
       "      <td>Fondului|Fond|Comisia|Fondul|Comisiei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>FEJEZET</td>\n",
       "      <td>34</td>\n",
       "      <td>15</td>\n",
       "      <td>44.117647</td>\n",
       "      <td>CAPITOLUL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Az Európai Közösség</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>44.827586</td>\n",
       "      <td>Comunitatea|Comunitatea Europeană|Acordul|Comu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tagállamonként</td>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "      <td>58.620690</td>\n",
       "      <td>CEE|Comunităţii|Directivei Consiliului|Directi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>határozathoz</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>Textul|Textele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tanácsa</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>Consiliul|Securitate|Naţiunilor Unite|Consiliu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>irányelveket</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>56.250000</td>\n",
       "      <td>CEE|Directiva|Directivele|Consiliul|Directiva ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Adott</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>Dacă|Comisia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>BIZOTTSÁG</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>58.333333</td>\n",
       "      <td>REGULAMENTUL|CE|REGULAMENTUL COMISIEI|CEE|AL C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TANÁCS</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>36.363636</td>\n",
       "      <td>CE|REGULAMENTUL|CONSILIULUI|DIRECTIVA|REGULAME...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Olaj</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>Comitetului|Măsurile</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                hu_term  occurrences  hits  hit_rate_%  \\\n",
       "0               Európai         1033   584   56.534366   \n",
       "1               Hivatal          353   153   43.342776   \n",
       "6             Gazdasági          192    66   34.375000   \n",
       "3                  Ezek          163    96   58.895706   \n",
       "2                  Azok          101    44   43.564356   \n",
       "7                  Alap           64    34   53.125000   \n",
       "11              FEJEZET           34    15   44.117647   \n",
       "9   Az Európai Közösség           29    13   44.827586   \n",
       "13       tagállamonként           29    17   58.620690   \n",
       "8          határozathoz           26    13   50.000000   \n",
       "4               Tanácsa           20     9   45.000000   \n",
       "12         irányelveket           16     9   56.250000   \n",
       "10                Adott           12     5   41.666667   \n",
       "15            BIZOTTSÁG           12     7   58.333333   \n",
       "14               TANÁCS           11     4   36.363636   \n",
       "5                  Olaj           10     3   30.000000   \n",
       "\n",
       "                                     current_ro_forms  \n",
       "0   Comunităţilor Europene|Jurnalul Oficial|Tratat...  \n",
       "1              Biroul|Oficiului|Biroului|Oficiul|Dacă  \n",
       "6   Comitetului Economic|Social|Comisia|Consiliulu...  \n",
       "3                                              Aceste  \n",
       "2           Statele|Comisia|CEE|Regulamentul|Comisiei  \n",
       "7               Fondului|Fond|Comisia|Fondul|Comisiei  \n",
       "11                                          CAPITOLUL  \n",
       "9   Comunitatea|Comunitatea Europeană|Acordul|Comu...  \n",
       "13  CEE|Comunităţii|Directivei Consiliului|Directi...  \n",
       "8                                      Textul|Textele  \n",
       "4   Consiliul|Securitate|Naţiunilor Unite|Consiliu...  \n",
       "12  CEE|Directiva|Directivele|Consiliul|Directiva ...  \n",
       "10                                       Dacă|Comisia  \n",
       "15  REGULAMENTUL|CE|REGULAMENTUL COMISIEI|CEE|AL C...  \n",
       "14  CE|REGULAMENTUL|CONSILIULUI|DIRECTIVA|REGULAME...  \n",
       "5                                Comitetului|Măsurile  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Example misses (first 3 for each of top 5 missed terms) ===\n",
      "\n",
      "TERM: cikk\n",
      "HU: b) a 6. cikkben felsorolt csatlakozni kívánó országokban lévő projekt a következők valamelyikére vonatkozik:\n",
      "RO: (b) proiecte pe teritoriul ţărilor candidate la aderare unde este aplicabil art. 6, care prevede:\n",
      "Expected one of: ['articolul', 'articolul', 'în', 'alineatul', 'comisia']\n",
      "---\n",
      "HU: a) - nem állítják ki többé a 70/156/EGK irányelv 10. cikke (1) bekezdésének utolsó francia bekezdésében említett bizonyítvány másolatát valamely járműtípus tekintetében,\n",
      "RO: (a) - nu mai eliberează copii ale certificatului prevăzut la art. 10 alin. (1) ultima liniuţă din Directiva 70/156/CEE cu privire la un tip de vehicul,\n",
      "Expected one of: ['articolul', 'articolul', 'în', 'alineatul', 'comisia']\n",
      "---\n",
      "HU: A véleményt a Szerződés 148. cikkének (2) bekezdésében arra az esetre megállapított többséggel kell meghozni, amikor a határozatokat a tanácsnak a Bizottság javaslata alapján kell elfogadnia.\n",
      "RO: 148 alin. (2) din Tratat pentru deciziile pe care Consiliul trebuie să le adopte la propunerea Comisiei.\n",
      "Expected one of: ['articolul', 'articolul', 'în', 'alineatul', 'comisia']\n",
      "---\n",
      "\n",
      "TERM: bekezdés\n",
      "HU: a) - nem állítják ki többé a 70/156/EGK irányelv 10. cikke (1) bekezdésének utolsó francia bekezdésében említett bizonyítvány másolatát valamely járműtípus tekintetében,\n",
      "RO: (a) - nu mai eliberează copii ale certificatului prevăzut la art. 10 alin. (1) ultima liniuţă din Directiva 70/156/CEE cu privire la un tip de vehicul,\n",
      "Expected one of: ['în', 'comisia', 'alineatul', 'statele', 'fără']\n",
      "---\n",
      "HU: A véleményt a Szerződés 148. cikkének (2) bekezdésében arra az esetre megállapított többséggel kell meghozni, amikor a határozatokat a tanácsnak a Bizottság javaslata alapján kell elfogadnia.\n",
      "RO: 148 alin. (2) din Tratat pentru deciziile pe care Consiliul trebuie să le adopte la propunerea Comisiei.\n",
      "Expected one of: ['în', 'comisia', 'alineatul', 'statele', 'fără']\n",
      "---\n",
      "HU: (3) Azokra a képzésben részt vevőkre és diákokra vonatkozó dóziskorlátok, akik nem tartoznak az (1) és (2) bekezdés hatálya alá, megegyeznek a 13. cikkben a lakosság tagjaira meghatározott dóziskorlát\n",
      "RO: 3. Dozele-limită pentru ucenicii şi studenţii care nu fac obiectul dispoziţiilor alin. (1) şi (2) sunt identice cu dozele-limită pentru populaţie, stabilite la art. 13.\n",
      "Expected one of: ['în', 'comisia', 'alineatul', 'statele', 'fără']\n",
      "---\n",
      "\n",
      "TERM: tagállam\n",
      "HU: A felkeresett tagállamok képviselői részt vehetnek a vizsgálatokon.\n",
      "RO: La inspecţii pot lua parte reprezentanţi ai statelor membre vizitate.\n",
      "Expected one of: ['statul', 'în', 'comisia', 'dacă', 'comisiei']\n",
      "---\n",
      "HU: A tagállamok végrehajtják az előlegre való jogosultság indokoltságára vonatkozó szükséges ellenőrzéseket.\n",
      "RO: Statele membre efectuează controalele necesare verificării eligibilităţii la plata avansului.\n",
      "Expected one of: ['statul', 'în', 'comisia', 'dacă', 'comisiei']\n",
      "---\n",
      "HU: A szabályozó hatóságok, a tagállamok más érintett hatóságaival együtt, fontos szerepet játszanak a belső villamosenergia-piac megfelelő működésének elősegítésében.\n",
      "RO: Autorităţile de reglementare, împreună cu alte autorităţi relevante ale statelor membre, au un rol important, contribuind la funcţionarea corespunzătoare a pieţei interne a energiei electrice.\n",
      "Expected one of: ['statul', 'în', 'comisia', 'dacă', 'comisiei']\n",
      "---\n",
      "\n",
      "TERM: rendelet\n",
      "HU: Ez a rendelet az Eurόpai Közösségek Hivatalos Lapjában való kihirdetését követő harmadik napon lép hatályba.\n",
      "RO: Prezentul regulament intră în vigoare în a treia zi de la publicarea în Jurnalul Oficial al Comunităţilor Europene.\n",
      "Expected one of: ['regulamentul', 'cee', 'ce', 'regulamentului', 'anexa']\n",
      "---\n",
      "HU: \"(2) A legutóbb a 2902/89/EGK rendelettel\n",
      "RO: (1) şi se inserează următorul alineat:\n",
      "Expected one of: ['regulamentul', 'cee', 'ce', 'regulamentului', 'anexa']\n",
      "---\n",
      "HU: Ez a rendelet teljes egészében kötelező és közvetlenül alkalmazandó valamennyi tagállamban.\n",
      "RO: Prezentul regulament este obligatoriu (n toate elementele sale (i se aplică direct (n toate statele membre.\n",
      "Expected one of: ['regulamentul', 'cee', 'ce', 'regulamentului', 'anexa']\n",
      "---\n",
      "\n",
      "TERM: Európai\n",
      "HU: A felügyelőbizottság az Európai Unió minden tagállamának egy-egy képviselőjéből, valamint a Bizottságnak a közös vállalkozás közgyűlésében tagsággal rendelkező képviselőjéből áll.\n",
      "RO: Respectivul consiliu este alcătuit din câte un reprezentant al fiecărui stat membru al Uniunii Europene şi din reprezentantul Comisiei în consiliul de administraţie al întreprinderii comune.\n",
      "Expected one of: ['comunităților europene', 'jurnalul oficial', 'tratatul', 'comunității europene', 'comisia']\n",
      "---\n",
      "HU: Ez a rendelet szabályozza az Európai Közösség és a Bolgár Köztársaság, illetve az Európai Közösség és a Magyar Köztársaság között a közúti áruszállítás és a kombinált szállítás ösztönzése egyes feltét\n",
      "RO: Prezentul regulament stabileşte regulile ce trebuie aplicate pentru repartizarea între statele membre a autorizaţiilor puse la dispoziţia Comunităţii în temeiul art. 6 alin (2) din acordurile încheiat\n",
      "Expected one of: ['comunităților europene', 'jurnalul oficial', 'tratatul', 'comunității europene', 'comisia']\n",
      "---\n",
      "HU: Az említett cikkben meghatározott ütemezés betartásának és az irányítás ésszerűsítésének érdekében, a legutóbb az 1017/2001/EK rendelettel [3] módosított, a tagállamok által továbbítandó adatokról és \n",
      "RO: 4 din Regulamentul Comisiei (CE) nr. 296/96 din 16 februarie 1996 privind datele ce urmează a fi înaintate de către statele membre şi contabilizarea lunară a cheltuielilor finanţate din Secţiunea gara\n",
      "Expected one of: ['comunităților europene', 'jurnalul oficial', 'tratatul', 'comunității europene', 'comisia']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "GLOSS_PATH = Path(PROJECT_ROOT) / \"data\" / \"glossary.csv\"\n",
    "TEST_PATH  = Path(DATA_PROCESSED) / \"test.csv\"\n",
    "\n",
    "assert GLOSS_PATH.exists(), f\"Glossary not found at {GLOSS_PATH}. Run Option A first.\"\n",
    "assert TEST_PATH.exists(), f\"Test not found at {TEST_PATH}.\"\n",
    "\n",
    "gloss_df = pd.read_csv(GLOSS_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "assert \"hu\" in gloss_df.columns and \"ro\" in gloss_df.columns, \"glossary.csv must have columns: hu, ro\"\n",
    "assert \"hu\" in test_df.columns and \"ro\" in test_df.columns, \"test.csv must have columns: hu, ro\"\n",
    "\n",
    "src = test_df[\"hu\"].astype(str).tolist()\n",
    "ref = test_df[\"ro\"].astype(str).tolist()\n",
    "\n",
    "\n",
    "def norm_ro(s: str) -> str:\n",
    "    s = str(s).lower()\n",
    "    s = s.replace(\"ţ\", \"ț\").replace(\"ş\", \"ș\")\n",
    "    return unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "glossary = {}\n",
    "for _, row in gloss_df.iterrows():\n",
    "    hu_term = str(row[\"hu\"])\n",
    "    ro_forms = [norm_ro(x.strip()) for x in str(row[\"ro\"]).split(\"|\") if x.strip()]\n",
    "    if ro_forms:\n",
    "        glossary[hu_term] = ro_forms\n",
    "\n",
    "print(\"Loaded glossary entries:\", len(glossary))\n",
    "\n",
    "hits_per_term = Counter()\n",
    "misses_per_term = Counter()\n",
    "term_occurrences = Counter()\n",
    "\n",
    "for s_hu, s_ro in zip(src, ref):\n",
    "    s_ro_n = norm_ro(s_ro)\n",
    "    for hu_term, ro_forms in glossary.items():\n",
    "        if hu_term in s_hu:\n",
    "            term_occurrences[hu_term] += 1\n",
    "            ok = any(f in s_ro_n for f in ro_forms)\n",
    "            if ok:\n",
    "                hits_per_term[hu_term] += 1\n",
    "            else:\n",
    "                misses_per_term[hu_term] += 1\n",
    "\n",
    "total_mentions = sum(term_occurrences.values())\n",
    "total_hits = sum(hits_per_term.values())\n",
    "coverage_terms = sum(1 for t, c in term_occurrences.items() if c > 0)\n",
    "coverage_examples = sum(1 for s_hu in src if any(t in s_hu for t in glossary.keys()))\n",
    "\n",
    "print(\"\\n=== Glossary coverage on TEST ===\")\n",
    "print(\"Test sentences:\", len(src))\n",
    "print(\"Sentences containing ≥1 glossary term:\", coverage_examples)\n",
    "print(\"Unique glossary terms that appear in test:\", coverage_terms, f\" / {len(glossary)}\")\n",
    "print(\"Total term mentions (all occurrences):\", total_mentions)\n",
    "if total_mentions > 0:\n",
    "    print(\"Reference-side hit rate (upper bound realism):\", round(100 * total_hits / total_mentions, 2), \"%\")\n",
    "else:\n",
    "    print(\"No glossary terms found in test set (increase glossary size or adjust extraction thresholds).\")\n",
    "\n",
    "\n",
    "def show_top(counter: Counter, title: str, n=20):\n",
    "    print(\"\\n\" + title)\n",
    "    for term, c in counter.most_common(n):\n",
    "        occ = term_occurrences.get(term, 0)\n",
    "        hit = hits_per_term.get(term, 0)\n",
    "        miss = misses_per_term.get(term, 0)\n",
    "        print(f\"{term:40s}  occ={occ:4d}  hit={hit:4d}  miss={miss:4d}\")\n",
    "\n",
    "show_top(term_occurrences, \"Top glossary terms in TEST (by occurrences)\", n=25)\n",
    "show_top(misses_per_term,  \"Top MISSED glossary terms in TEST (ref doesn't contain expected RO forms)\", n=25)\n",
    "\n",
    "\n",
    "rows = []\n",
    "for term, occ in term_occurrences.items():\n",
    "    if occ < 10:\n",
    "        continue\n",
    "    hit = hits_per_term.get(term, 0)\n",
    "    rate = hit / occ if occ else 0.0\n",
    "    if rate < 0.6: \n",
    "        rows.append((term, occ, hit, 100*rate, gloss_df.loc[gloss_df[\"hu\"] == term, \"ro\"].iloc[0] if (gloss_df[\"hu\"] == term).any() else \"\"))\n",
    "\n",
    "if rows:\n",
    "    hard_df = pd.DataFrame(rows, columns=[\"hu_term\", \"occurrences\", \"hits\", \"hit_rate_%\", \"current_ro_forms\"])\n",
    "    hard_df = hard_df.sort_values([\"occurrences\", \"hit_rate_%\"], ascending=[False, True])\n",
    "    out_hard = Path(PROJECT_ROOT) / \"data\" / \"glossary_hard_terms.csv\"\n",
    "    hard_df.to_csv(out_hard, index=False)\n",
    "    print(\"\\n✅ Wrote:\", out_hard)\n",
    "    print(\"These are frequent terms where your RO variants likely need cleanup/expansion.\")\n",
    "    display(hard_df.head(30))\n",
    "else:\n",
    "    print(\"\\nNo frequent low-hit glossary terms found (good sign).\")\n",
    "\n",
    "\n",
    "top_missed = [t for t, _ in misses_per_term.most_common(5)]\n",
    "if top_missed:\n",
    "    print(\"\\n=== Example misses (first 3 for each of top 5 missed terms) ===\")\n",
    "    for term in top_missed:\n",
    "        print(\"\\nTERM:\", term)\n",
    "        shown = 0\n",
    "        ro_forms = glossary[term]\n",
    "        for s_hu, s_ro in zip(src, ref):\n",
    "            if term in s_hu and not any(f in norm_ro(s_ro) for f in ro_forms):\n",
    "                print(\"HU:\", s_hu[:200])\n",
    "                print(\"RO:\", s_ro[:200])\n",
    "                print(\"Expected one of:\", ro_forms)\n",
    "                print(\"---\")\n",
    "                shown += 1\n",
    "                if shown >= 3:\n",
    "                    break\n",
    "else:\n",
    "    print(\"\\nNo misses to sample.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb202f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: f59407a4-4ac4-4afe-98d8-767cbbaba5c5)')' thrown while requesting HEAD https://huggingface.co/Helsinki-NLP/opus-mt-hu-en/resolve/main/tokenizer_config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Torch: 2.5.1+cu121\n",
      "Loaded glossary entries: 500\n",
      "Loading Helsinki-NLP/opus-mt-hu-en + LoRA from: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\hu_en_legal_lora\\checkpoint-6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Helsinki-NLP/opus-mt-en-ro + LoRA from: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\en_ro_legal_lora\\checkpoint-6250\n",
      "Test size: 30366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HU→EN beam4 [normal]: 100%|██████████| 29820/29820 [1:20:21<00:00,  6.18sent/s]\n",
      "HU→EN beam4 [table]: 100%|██████████| 546/546 [00:12<00:00, 45.03sent/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building per-sentence force_words_ids from glossary (conservative)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "force_ids: 100%|██████████| 30366/30366 [00:05<00:00, 5524.10it/s]\n",
      "EN→RO beam4 + glossary [normal]:   0%|          | 0/30200 [00:00<?, ?sent/s]Constrained Beam Search was moved to a `custom_generate` repo: https://hf.co/transformers-community/constrained-beam-search. To prevent loss of backward compatibility, add `custom_generate='transformers-community/constrained-beam-search'` to your `generate` call before v4.62.0.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Constrained Beam Search requires `trust_remote_code=True` in your `generate` call, since it loads https://hf.co/transformers-community/constrained-beam-search.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 314\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding per-sentence force_words_ids from glossary (conservative)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    312\u001b[0m     force_ids \u001b[38;5;241m=\u001b[39m [build_force_words_ids_for_sentence(en_ro_tok, hu) \u001b[38;5;28;01mfor\u001b[39;00m hu \u001b[38;5;129;01min\u001b[39;00m tqdm(src_hu, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m--> 314\u001b[0m hyps \u001b[38;5;241m=\u001b[39m \u001b[43mbatched_generate_adaptive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43men_ro_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43men_ro_tok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43men_mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEN→RO beam\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mNUM_BEAMS\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m + glossary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_input_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_INPUT_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_BEAMS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_NEW_TOKENS_NORMAL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_BEAMS_TABLE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens_table\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_NEW_TOKENS_TABLE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43manti_repeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUSE_ANTI_REPEAT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_sentence_force_words_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_ids\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# 7) Metrics (BLEU + leaks + glossary accuracy)\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[0;32m    331\u001b[0m bleu \u001b[38;5;241m=\u001b[39m sacrebleu\u001b[38;5;241m.\u001b[39mcorpus_bleu(hyps, [ref_ro])\u001b[38;5;241m.\u001b[39mscore\n",
      "Cell \u001b[1;32mIn[6], line 270\u001b[0m, in \u001b[0;36mbatched_generate_adaptive\u001b[1;34m(model, tok, inputs, desc, bs, max_input_len, num_beams, max_new_tokens, num_beams_table, max_new_tokens_table, anti_repeat, per_sentence_force_words_ids)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m⚠ OOM -> reducing batch size to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcur_bs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    268\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m--> 270\u001b[0m \u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnormal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m _run(idx_table, num_beams_table, max_new_tokens_table, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(o \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m outs)\n",
      "Cell \u001b[1;32mIn[6], line 238\u001b[0m, in \u001b[0;36mbatched_generate_adaptive.<locals>._run\u001b[1;34m(indices, beams, max_new, tag)\u001b[0m\n\u001b[0;32m    236\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_words_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m fw\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m--> 238\u001b[0m         one_gen \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mone_inp, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    239\u001b[0m     decoded\u001b[38;5;241m.\u001b[39mappend(tok\u001b[38;5;241m.\u001b[39mbatch_decode(one_gen, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_idx, decoded):\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\peft\\peft_model.py:2374\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.generate\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   2372\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2373\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 2374\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\generation\\utils.py:2382\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2378\u001b[0m generation_config, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_generation_config(\n\u001b[0;32m   2379\u001b[0m     generation_config, use_model_defaults, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   2380\u001b[0m )\n\u001b[0;32m   2381\u001b[0m generation_mode \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mget_generation_mode(assistant_model)\n\u001b[1;32m-> 2382\u001b[0m deprecated_mode_repo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_deprecated_gen_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_generate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(custom_generate, Callable):\n\u001b[0;32m   2385\u001b[0m     decoding_method \u001b[38;5;241m=\u001b[39m custom_generate\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\generation\\utils.py:2195\u001b[0m, in \u001b[0;36mGenerationMixin._get_deprecated_gen_repo\u001b[1;34m(self, generation_mode, trust_remote_code, custom_generate)\u001b[0m\n\u001b[0;32m   2189\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m   2190\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_mode\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtitle()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was moved to a `custom_generate` repo: https://hf.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2191\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo prevent loss of backward compatibility, add `custom_generate=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2192\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto your `generate` call before v4.62.0.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2193\u001b[0m )\n\u001b[0;32m   2194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n\u001b[1;32m-> 2195\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2196\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgeneration_mode\u001b[38;5;241m.\u001b[39mname\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtitle()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires `trust_remote_code=True` in your `generate` call, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2197\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msince it loads https://hf.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2198\u001b[0m     )\n\u001b[0;32m   2199\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repo\n",
      "\u001b[1;31mValueError\u001b[0m: Constrained Beam Search requires `trust_remote_code=True` in your `generate` call, since it loads https://hf.co/transformers-community/constrained-beam-search."
     ]
    }
   ],
   "source": [
    "import os, gc, json, re, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "HU_EN_BASE = \"Helsinki-NLP/opus-mt-hu-en\"\n",
    "EN_RO_BASE = \"Helsinki-NLP/opus-mt-en-ro\"\n",
    "\n",
    "HU_EN_RUN = Path(PROJECT_ROOT) / \"checkpoints\" / \"hu_en_legal_lora\"\n",
    "EN_RO_RUN = Path(PROJECT_ROOT) / \"checkpoints\" / \"en_ro_legal_lora\"\n",
    "\n",
    "BASE_SAFE_ROOT = Path(PROJECT_ROOT) / \"safetensors_bases\"\n",
    "BASE_SAFE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_DIR = Path(OUTPUT_DIR)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Decode settings\n",
    "NUM_BEAMS = 4          \n",
    "MAX_INPUT_LEN = 256\n",
    "MAX_NEW_TOKENS_NORMAL = 128\n",
    "BS = 12 if DEVICE == \"cuda\" else 4\n",
    "\n",
    "# Table-like safe mode\n",
    "MAX_NEW_TOKENS_TABLE = 32\n",
    "NUM_BEAMS_TABLE = 1\n",
    "\n",
    "# Anti-repeat \n",
    "USE_ANTI_REPEAT = True\n",
    "NO_REPEAT_NGRAM = 3\n",
    "REPETITION_PENALTY = 1.10\n",
    "LENGTH_PENALTY = 1.0\n",
    "\n",
    "# Glossary forcing\n",
    "USE_GLOSSARY_CONSTRAINTS = True\n",
    "MAX_FORCED_TERMS_PER_SENT = 3\n",
    "\n",
    "\n",
    "# Load glossary.csv  (HU -> RO variants)\n",
    "\n",
    "GLOSS_PATH = Path(PROJECT_ROOT) / \"data\" / \"glossary.csv\"\n",
    "assert GLOSS_PATH.exists(), f\"Missing glossary at {GLOSS_PATH}. Create it with Option A first.\"\n",
    "\n",
    "def norm_ro(s: str) -> str:\n",
    "    s = str(s).lower()\n",
    "    s = s.replace(\"ţ\",\"ț\").replace(\"ş\",\"ș\")\n",
    "    return unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "gloss_df = pd.read_csv(GLOSS_PATH)\n",
    "assert \"hu\" in gloss_df.columns and \"ro\" in gloss_df.columns, \"glossary.csv must have columns: hu, ro\"\n",
    "\n",
    "glossary = {}\n",
    "for _, row in gloss_df.iterrows():\n",
    "    hu_term = str(row[\"hu\"])\n",
    "    ro_forms = [x.strip() for x in str(row[\"ro\"]).split(\"|\") if x.strip()]\n",
    "    if ro_forms:\n",
    "        glossary[hu_term] = ro_forms\n",
    "\n",
    "print(\"Loaded glossary entries:\", len(glossary))\n",
    "\n",
    "# Helpers: safetensors base + checkpoint loader\n",
    "def get_latest_checkpoint(run_dir: Path) -> Path:\n",
    "    ckpts = sorted(\n",
    "        [p for p in run_dir.glob(\"checkpoint-*\") if p.is_dir()],\n",
    "        key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else -1\n",
    "    )\n",
    "    return ckpts[-1] if ckpts else run_dir\n",
    "\n",
    "def ensure_base_safetensors(model_id: str, out_root: Path) -> Path:\n",
    "    out_dir = out_root / model_id.replace(\"/\", \"__\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if (out_dir / \"model.safetensors\").exists() and (out_dir / \"config.json\").exists():\n",
    "        return out_dir\n",
    "\n",
    "    print(f\"[Convert] Creating base safetensors for {model_id}\")\n",
    "    snap_dir = Path(snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        allow_patterns=[\n",
    "            \"config.json\",\n",
    "            \"tokenizer.json\",\n",
    "            \"tokenizer_config.json\",\n",
    "            \"source.spm\",\n",
    "            \"vocab.json\", \"merges.txt\",\n",
    "            \"pytorch_model.bin\",\n",
    "            \"pytorch_model.bin.index.json\",\n",
    "            \"pytorch_model-*.bin\",\n",
    "        ],\n",
    "    ))\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "    bin_path = snap_dir / \"pytorch_model.bin\"\n",
    "    if bin_path.exists():\n",
    "        state = torch.load(bin_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state, strict=False)\n",
    "        del state\n",
    "    else:\n",
    "        index_path = snap_dir / \"pytorch_model.bin.index.json\"\n",
    "        with open(index_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            index = json.load(f)\n",
    "        shard_files = sorted(set(index[\"weight_map\"].values()))\n",
    "        for sf in tqdm(shard_files, desc=f\"[Convert] shards {model_id.split('/')[-1]}\"):\n",
    "            shard_state = torch.load(snap_dir / sf, map_location=\"cpu\")\n",
    "            model.load_state_dict(shard_state, strict=False)\n",
    "            del shard_state\n",
    "\n",
    "    model.save_pretrained(out_dir, safe_serialization=True)\n",
    "    config.save_pretrained(out_dir)\n",
    "    return out_dir\n",
    "\n",
    "def load_lora_model(base_id: str, run_dir: Path):\n",
    "    base_safe = ensure_base_safetensors(base_id, BASE_SAFE_ROOT)\n",
    "    ckpt = get_latest_checkpoint(run_dir)\n",
    "    print(f\"Loading {base_id} + LoRA from:\", ckpt)\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(base_id, use_fast=True)\n",
    "    base = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        base_safe,\n",
    "        use_safetensors=True,\n",
    "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else None\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    model = PeftModel.from_pretrained(base, ckpt).to(DEVICE)\n",
    "    model.eval()\n",
    "    return model, tok, ckpt\n",
    "\n",
    "# Generation helpers (table detection + glossary forcing)\n",
    "_punct = set(list(\"0123456789.,;:-/()[]{}<>|_+*=—–%°\\\"' \\t\"))\n",
    "def is_table_like(s: str) -> bool:\n",
    "    if s is None:\n",
    "        return True\n",
    "    t = str(s).strip()\n",
    "    if len(t) == 0:\n",
    "        return True\n",
    "    if \"_BAR_\" in t or t.count(\"|\") >= 2:\n",
    "        return True\n",
    "    good = sum(ch in _punct for ch in t)\n",
    "    if good / max(1, len(t)) > 0.80:\n",
    "        return True\n",
    "    if re.search(r\"[-—–]{6,}\", t):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def build_force_words_ids_for_sentence(tok, hu_src: str):\n",
    "    \n",
    "    if not USE_GLOSSARY_CONSTRAINTS:\n",
    "        return None\n",
    "    hu_s = str(hu_src)\n",
    "    forced = []\n",
    "    for hu_term, ro_forms in glossary.items():\n",
    "        if hu_term in hu_s and ro_forms:\n",
    "            ro_form = ro_forms[0]\n",
    "            ids = tok(ro_form, add_special_tokens=False).input_ids\n",
    "            if ids:\n",
    "                forced.append(ids)\n",
    "        if len(forced) >= MAX_FORCED_TERMS_PER_SENT:\n",
    "            break\n",
    "    return forced if forced else None\n",
    "\n",
    "def batched_generate_adaptive(\n",
    "    model, tok,\n",
    "    inputs,                     \n",
    "    desc: str,\n",
    "    bs: int,\n",
    "    max_input_len: int,\n",
    "    num_beams: int,\n",
    "    max_new_tokens: int,\n",
    "    num_beams_table: int,\n",
    "    max_new_tokens_table: int,\n",
    "    anti_repeat: bool = False,\n",
    "    per_sentence_force_words_ids=None,   \n",
    "):\n",
    "    outs = [None] * len(inputs)\n",
    "    idx_table = [i for i, s in enumerate(inputs) if is_table_like(s)]\n",
    "    idx_norm  = [i for i in range(len(inputs)) if i not in set(idx_table)]\n",
    "\n",
    "    nr = NO_REPEAT_NGRAM if anti_repeat else 0\n",
    "    rp = REPETITION_PENALTY if anti_repeat else 1.0\n",
    "\n",
    "    def _run(indices, beams, max_new, tag):\n",
    "        if not indices:\n",
    "            return\n",
    "        i = 0\n",
    "        cur_bs = bs\n",
    "        pbar = tqdm(total=len(indices), desc=f\"{desc} [{tag}]\", unit=\"sent\")\n",
    "        while i < len(indices):\n",
    "            take = min(cur_bs, len(indices) - i)\n",
    "            batch_idx = indices[i:i+take]\n",
    "            batch_txt = [inputs[j] for j in batch_idx]\n",
    "            try:\n",
    "\n",
    "                if per_sentence_force_words_ids is not None and tag == \"normal\":\n",
    "                    any_forced = any(per_sentence_force_words_ids[j] for j in batch_idx)\n",
    "                    if any_forced:\n",
    "                        decoded = []\n",
    "                        for j, txt in zip(batch_idx, batch_txt):\n",
    "                            one_inp = tok(txt, return_tensors=\"pt\", truncation=True, max_length=max_input_len).to(DEVICE)\n",
    "                            kwargs = dict(\n",
    "                                num_beams=beams,\n",
    "                                max_new_tokens=max_new,\n",
    "                                do_sample=False,\n",
    "                                length_penalty=LENGTH_PENALTY,\n",
    "                                no_repeat_ngram_size=nr,\n",
    "                                repetition_penalty=rp,\n",
    "                            )\n",
    "                            fw = per_sentence_force_words_ids[j]\n",
    "                            if fw:\n",
    "                                kwargs[\"force_words_ids\"] = fw\n",
    "                            with torch.inference_mode():\n",
    "                                one_gen = model.generate(**one_inp, **kwargs)\n",
    "                            decoded.append(tok.batch_decode(one_gen, skip_special_tokens=True)[0])\n",
    "                        for j, d in zip(batch_idx, decoded):\n",
    "                            outs[j] = d\n",
    "                        i += take\n",
    "                        pbar.update(take)\n",
    "                        continue\n",
    "\n",
    "                inp = tok(batch_txt, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_input_len).to(DEVICE)\n",
    "                with torch.inference_mode():\n",
    "                    gen = model.generate(\n",
    "                        **inp,\n",
    "                        num_beams=beams,\n",
    "                        max_new_tokens=max_new,\n",
    "                        do_sample=False,\n",
    "                        length_penalty=LENGTH_PENALTY if tag == \"normal\" else 1.0,\n",
    "                        no_repeat_ngram_size=nr if tag == \"normal\" else 0,\n",
    "                        repetition_penalty=rp if tag == \"normal\" else 1.0,\n",
    "                    )\n",
    "                dec = tok.batch_decode(gen, skip_special_tokens=True)\n",
    "                for j, d in zip(batch_idx, dec):\n",
    "                    outs[j] = d\n",
    "                i += take\n",
    "                pbar.update(take)\n",
    "\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                torch.cuda.empty_cache()\n",
    "                cur_bs = max(1, cur_bs // 2)\n",
    "                print(f\"⚠ OOM -> reducing batch size to {cur_bs}\")\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "    _run(idx_norm,  num_beams,       max_new_tokens,       \"normal\")\n",
    "    _run(idx_table, num_beams_table, max_new_tokens_table, \"table\")\n",
    "\n",
    "    assert all(o is not None for o in outs)\n",
    "    return outs\n",
    "\n",
    "# Load both LoRA models\n",
    "\n",
    "hu_en_model, hu_en_tok, hu_en_ckpt = load_lora_model(HU_EN_BASE, HU_EN_RUN)\n",
    "en_ro_model, en_ro_tok, en_ro_ckpt = load_lora_model(EN_RO_BASE, EN_RO_RUN)\n",
    "\n",
    "\n",
    "# Load test data\n",
    "\n",
    "test_df = pd.read_csv(Path(DATA_PROCESSED) / \"test.csv\")\n",
    "src_hu = test_df[\"hu\"].astype(str).tolist()\n",
    "ref_ro = test_df[\"ro\"].astype(str).tolist()\n",
    "print(\"Test size:\", len(src_hu))\n",
    "\n",
    "\n",
    "# Run pivot prediction with glossary forcing in EN→RO\n",
    "\n",
    "# HU→EN stage: no glossary forcing (not needed)\n",
    "en_mid = batched_generate_adaptive(\n",
    "    hu_en_model, hu_en_tok,\n",
    "    inputs=src_hu,\n",
    "    desc=f\"HU→EN beam{NUM_BEAMS}\",\n",
    "    bs=BS,\n",
    "    max_input_len=MAX_INPUT_LEN,\n",
    "    num_beams=NUM_BEAMS,\n",
    "    max_new_tokens=MAX_NEW_TOKENS_NORMAL,\n",
    "    num_beams_table=NUM_BEAMS_TABLE,\n",
    "    max_new_tokens_table=MAX_NEW_TOKENS_TABLE,\n",
    "    anti_repeat=USE_ANTI_REPEAT,\n",
    "    per_sentence_force_words_ids=None\n",
    ")\n",
    "\n",
    "# Build constraints from HU source, but apply them during EN→RO\n",
    "force_ids = None\n",
    "if USE_GLOSSARY_CONSTRAINTS:\n",
    "    print(\"Building per-sentence force_words_ids from glossary (conservative)...\")\n",
    "    force_ids = [build_force_words_ids_for_sentence(en_ro_tok, hu) for hu in tqdm(src_hu, desc=\"force_ids\")]\n",
    "\n",
    "hyps = batched_generate_adaptive(\n",
    "    en_ro_model, en_ro_tok,\n",
    "    inputs=en_mid,\n",
    "    desc=f\"EN→RO beam{NUM_BEAMS} + glossary\",\n",
    "    bs=BS,\n",
    "    max_input_len=MAX_INPUT_LEN,\n",
    "    num_beams=NUM_BEAMS,\n",
    "    max_new_tokens=MAX_NEW_TOKENS_NORMAL,\n",
    "    num_beams_table=NUM_BEAMS_TABLE,\n",
    "    max_new_tokens_table=MAX_NEW_TOKENS_TABLE,\n",
    "    anti_repeat=USE_ANTI_REPEAT,\n",
    "    per_sentence_force_words_ids=force_ids\n",
    ")\n",
    "\n",
    "# Metrics (BLEU + leaks + glossary accuracy)\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(hyps, [ref_ro]).score\n",
    "print(\"\\nPivot BLEU:\", round(bleu, 2))\n",
    "\n",
    "hu_diacritics = set(\"áéíóöőúüűÁÉÍÓÖŐÚÜŰ\")\n",
    "def has_hu_diacritics(s: str) -> bool:\n",
    "    return any(c in hu_diacritics for c in str(s))\n",
    "\n",
    "def en_leak(s: str) -> bool:\n",
    "    s = \" \" + re.sub(r\"\\s+\", \" \", str(s).lower()) + \" \"\n",
    "    return any(w in s for w in [\" the \", \" and \", \" of \", \" to \", \" for \", \" with \", \" on \"])\n",
    "\n",
    "hu_leak = np.mean([has_hu_diacritics(h) for h in hyps]) * 100\n",
    "en_leak_rate = np.mean([en_leak(h) for h in hyps]) * 100\n",
    "print(\"HU diacritics leak %:\", round(hu_leak, 2))\n",
    "print(\"English leak proxy %:\", round(en_leak_rate, 2))\n",
    "\n",
    "# Glossary accuracy: among occurrences where HU term appears, did output contain any RO variant?\n",
    "gloss_norm = {k: [norm_ro(v) for v in vs] for k, vs in glossary.items()}\n",
    "\n",
    "def glossary_hit(src: str, hyp: str):\n",
    "    hyp_n = norm_ro(hyp)\n",
    "    checks = []\n",
    "    for hu_term, ro_forms in gloss_norm.items():\n",
    "        if hu_term in str(src):\n",
    "            checks.append(any(f in hyp_n for f in ro_forms))\n",
    "    return checks\n",
    "\n",
    "all_hits = [glossary_hit(s, h) for s, h in zip(src_hu, hyps)]\n",
    "flat = [x for row in all_hits for x in row]\n",
    "if flat:\n",
    "    print(\"Glossary accuracy %:\", round(np.mean(flat) * 100, 2))\n",
    "else:\n",
    "    print(\"Glossary accuracy: no glossary terms found in test.\")\n",
    "\n",
    "\n",
    "# Save outputs\n",
    "\n",
    "out_path = OUT_DIR / f\"pivot_glossary_beam{NUM_BEAMS}_{hu_en_ckpt.name}__{en_ro_ckpt.name}.csv\"\n",
    "pd.DataFrame({\n",
    "    \"source_hu\": src_hu,\n",
    "    \"pivot_en\": en_mid,\n",
    "    \"reference_ro\": ref_ro,\n",
    "    \"hypothesis_ro\": hyps\n",
    "}).to_csv(out_path, index=False)\n",
    "\n",
    "print(\"Saved:\", out_path)\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1978aef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EN→RO beam4 + glossary [normal]:   0%|          | 0/30200 [24:30<?, ?sent/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda\n",
      "Torch: 2.5.1+cu121\n",
      "Glossary entries: 500\n",
      "Loading Helsinki-NLP/opus-mt-hu-en + LoRA from: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\hu_en_legal_lora\\checkpoint-6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Helsinki-NLP/opus-mt-en-ro + LoRA from: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\checkpoints\\en_ro_legal_lora\\checkpoint-6250\n",
      "Test size: 30366\n",
      "Starting fresh EN→RO (no resume file).\n",
      "Pivot cache found: 168 rows. Missing pivot: 30198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HU→EN cache: 100%|██████████| 3775/3775 [3:38:34<00:00,  3.47s/batch]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pivot EN ready. Cache: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\\pivot_en_cache_checkpoint-6250_beam4.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EN→RO resumable:   0%|          | 0/3796 [00:00<?, ?batch/s]'(ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 70763103-6cdc-4b62-bb12-13ca1b1fcc60)')' thrown while requesting HEAD https://huggingface.co/transformers-community/constrained-beam-search/resolve/main/custom_generate/generate.py\n",
      "Retrying in 1s [Retry 1/5].\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RoG\\.cache\\huggingface\\hub\\models--transformers-community--constrained-beam-search. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "EN→RO resumable:   0%|          | 0/3796 [00:02<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "`transformers-community/constrained-beam-search` does not contain a `custom_generate` subdirectory with a `generate.py` file, can't load the custom generate function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\generation\\utils.py:419\u001b[0m, in \u001b[0;36mGenerationMixin.load_custom_generate\u001b[1;34m(self, pretrained_model_name_or_path, trust_remote_code, **kwargs)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     module \u001b[38;5;241m=\u001b[39m get_cached_module_file(\n\u001b[0;32m    420\u001b[0m         pretrained_model_name_or_path, module_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_generate/generate.py\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\dynamic_module_utils.py:467\u001b[0m, in \u001b[0;36mget_cached_module_file\u001b[1;34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ((submodule_path \u001b[38;5;241m/\u001b[39m module_file)\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_needed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m--> 467\u001b[0m     \u001b[43mget_cached_module_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mmodule_needed\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m     new_files\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_needed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\dynamic_module_utils.py:406\u001b[0m, in \u001b[0;36mget_cached_module_file\u001b[1;34m(pretrained_model_name_or_path, module_file, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, repo_type, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 406\u001b[0m     resolved_module_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodule_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local \u001b[38;5;129;01mand\u001b[39;00m cached_module \u001b[38;5;241m!=\u001b[39m resolved_module_file:\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\utils\\hub.py:322\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03mTries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 322\u001b[0m file \u001b[38;5;241m=\u001b[39m cached_files(path_or_repo_id\u001b[38;5;241m=\u001b[39mpath_or_repo_id, filenames\u001b[38;5;241m=\u001b[39m[filename], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    323\u001b[0m file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\utils\\hub.py:583\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    580\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_entries[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_entries) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;241m*\u001b[39mmissing_entries,)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m     )\n\u001b[1;32m--> 583\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    584\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not appear to have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Checkout \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/tree/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m for available files.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    586\u001b[0m     )\n\u001b[0;32m    588\u001b[0m \u001b[38;5;66;03m# Remove potential missing entries (we can silently remove them at this point based on the flags)\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: transformers-community/constrained-beam-search does not appear to have a file named custom_generate\\beam_constraints.py. Checkout 'https://huggingface.co/transformers-community/constrained-beam-search/tree/main' for available files.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 326\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;66;03m# If glossary forcing triggers constrained decoding, this call is now safe.\u001b[39;00m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 326\u001b[0m     ro_out \u001b[38;5;241m=\u001b[39m \u001b[43mgen_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43men_ro_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43men_ro_tok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43men_in\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43manti_repeat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manti\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_words_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mOutOfMemoryError:\n\u001b[0;32m    335\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[1;32mIn[8], line 242\u001b[0m, in \u001b[0;36mgen_batch\u001b[1;34m(model, tok, batch_txt, beams, max_new, anti_repeat, force_words_ids)\u001b[0m\n\u001b[0;32m    239\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_generate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformers-community/constrained-beam-search\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m--> 242\u001b[0m     out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minp, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tok\u001b[38;5;241m.\u001b[39mbatch_decode(out, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\peft\\peft_model.py:2374\u001b[0m, in \u001b[0;36mPeftModelForSeq2SeqLM.generate\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m   2372\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   2373\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 2374\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2376\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs:\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\generation\\utils.py:2364\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     generate_arguments \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m()\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m global_keys_to_exclude}\n\u001b[0;32m   2362\u001b[0m     generate_arguments\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m-> 2364\u001b[0m     custom_generate_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_custom_generate(\n\u001b[0;32m   2365\u001b[0m         custom_generate, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   2366\u001b[0m     )\n\u001b[0;32m   2367\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m custom_generate_function(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_arguments)\n\u001b[0;32m   2369\u001b[0m \u001b[38;5;66;03m# 1. Handle kwargs, `generation_config`, validate them and obtain generation mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\transformers\\generation\\utils.py:423\u001b[0m, in \u001b[0;36mGenerationMixin.load_custom_generate\u001b[1;34m(self, pretrained_model_name_or_path, trust_remote_code, **kwargs)\u001b[0m\n\u001b[0;32m    419\u001b[0m     module \u001b[38;5;241m=\u001b[39m get_cached_module_file(\n\u001b[0;32m    420\u001b[0m         pretrained_model_name_or_path, module_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_generate/generate.py\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    421\u001b[0m     )\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    424\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` does not contain a `custom_generate` subdirectory with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`generate.py` file, can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load the custom generate function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m     )\n\u001b[0;32m    428\u001b[0m \u001b[38;5;66;03m# Handle opt-in `trust_remote_code` and related exceptions\u001b[39;00m\n\u001b[0;32m    429\u001b[0m is_local_code \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(pretrained_model_name_or_path)\n",
      "\u001b[1;31mOSError\u001b[0m: `transformers-community/constrained-beam-search` does not contain a `custom_generate` subdirectory with a `generate.py` file, can't load the custom generate function."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, gc, json, re, unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import sacrebleu\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "\n",
    "HU_EN_BASE = \"Helsinki-NLP/opus-mt-hu-en\"\n",
    "EN_RO_BASE = \"Helsinki-NLP/opus-mt-en-ro\"\n",
    "\n",
    "HU_EN_RUN = Path(PROJECT_ROOT) / \"checkpoints\" / \"hu_en_legal_lora\"\n",
    "EN_RO_RUN = Path(PROJECT_ROOT) / \"checkpoints\" / \"en_ro_legal_lora\"\n",
    "\n",
    "\n",
    "BASE_SAFE_ROOT = Path(PROJECT_ROOT) / \"safetensors_bases\"\n",
    "BASE_SAFE_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_DIR = Path(OUTPUT_DIR)\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "NUM_BEAMS = 4\n",
    "MAX_INPUT_LEN = 256\n",
    "MAX_NEW = 128\n",
    "BS = 8 if DEVICE == \"cuda\" else 4\n",
    "\n",
    "# table-ish safe mode\n",
    "MAX_NEW_TABLE = 32\n",
    "BEAMS_TABLE = 1\n",
    "\n",
    "# anti-repeat\n",
    "USE_ANTI_REPEAT = True\n",
    "NO_REPEAT_NGRAM = 3\n",
    "REPETITION_PENALTY = 1.10\n",
    "LENGTH_PENALTY = 1.0\n",
    "\n",
    "# glossary forcing\n",
    "USE_GLOSSARY = True\n",
    "MAX_FORCED_TERMS_PER_SENT = 2  # conservative; constrained decoding can be slower\n",
    "\n",
    "# resumable files (beam-specific)\n",
    "RESUME_PATH = OUT_DIR / f\"pivot_glossary_RESUMABLE_beam{NUM_BEAMS}.csv\"\n",
    "PIVOT_CACHE_PATH = None  # will set after we know checkpoints\n",
    "\n",
    "\n",
    "GLOSS_PATH = Path(PROJECT_ROOT) / \"data\" / \"glossary.csv\"\n",
    "assert GLOSS_PATH.exists(), f\"Missing glossary at {GLOSS_PATH}. Run Option A first.\"\n",
    "gdf = pd.read_csv(GLOSS_PATH)\n",
    "assert {\"hu\",\"ro\"}.issubset(gdf.columns), \"glossary.csv must have columns hu, ro\"\n",
    "\n",
    "glossary = {}\n",
    "for _, row in gdf.iterrows():\n",
    "    hu_term = str(row[\"hu\"])\n",
    "    ro_forms = [x.strip() for x in str(row[\"ro\"]).split(\"|\") if x.strip()]\n",
    "    if ro_forms:\n",
    "        glossary[hu_term] = ro_forms\n",
    "\n",
    "print(\"Glossary entries:\", len(glossary))\n",
    "\n",
    "\n",
    "def load_tokenizer_local_first(model_id: str):\n",
    "    try:\n",
    "        return AutoTokenizer.from_pretrained(model_id, use_fast=True, local_files_only=True)\n",
    "    except Exception:\n",
    "        return AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
    "\n",
    "\n",
    "def get_latest_checkpoint(run_dir: Path) -> Path:\n",
    "    ckpts = sorted(\n",
    "        [p for p in run_dir.glob(\"checkpoint-*\") if p.is_dir()],\n",
    "        key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else -1\n",
    "    )\n",
    "    return ckpts[-1] if ckpts else run_dir\n",
    "\n",
    "def ensure_base_safetensors(model_id: str, out_root: Path) -> Path:\n",
    "    out_dir = out_root / model_id.replace(\"/\", \"__\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if (out_dir / \"model.safetensors\").exists() and (out_dir / \"config.json\").exists():\n",
    "        return out_dir\n",
    "\n",
    "    print(f\"[Convert] Base -> safetensors for {model_id}\")\n",
    "    snap_dir = Path(snapshot_download(\n",
    "        repo_id=model_id,\n",
    "        allow_patterns=[\n",
    "            \"config.json\",\"tokenizer.json\",\"tokenizer_config.json\",\"source.spm\",\n",
    "            \"vocab.json\",\"merges.txt\",\n",
    "            \"pytorch_model.bin\",\"pytorch_model.bin.index.json\",\"pytorch_model-*.bin\",\n",
    "        ],\n",
    "    ))\n",
    "\n",
    "    config = AutoConfig.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_config(config)\n",
    "\n",
    "    bin_path = snap_dir / \"pytorch_model.bin\"\n",
    "    if bin_path.exists():\n",
    "        state = torch.load(bin_path, map_location=\"cpu\")\n",
    "        model.load_state_dict(state, strict=False)\n",
    "        del state\n",
    "    else:\n",
    "        with open(snap_dir / \"pytorch_model.bin.index.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "            index = json.load(f)\n",
    "        shard_files = sorted(set(index[\"weight_map\"].values()))\n",
    "        for sf in tqdm(shard_files, desc=f\"[Convert] shards {model_id.split('/')[-1]}\"):\n",
    "            shard_state = torch.load(snap_dir / sf, map_location=\"cpu\")\n",
    "            model.load_state_dict(shard_state, strict=False)\n",
    "            del shard_state\n",
    "\n",
    "    model.save_pretrained(out_dir, safe_serialization=True)\n",
    "    config.save_pretrained(out_dir)\n",
    "    return out_dir\n",
    "\n",
    "def load_lora_model(base_id: str, run_dir: Path):\n",
    "    base_safe = ensure_base_safetensors(base_id, BASE_SAFE_ROOT)\n",
    "    ckpt = get_latest_checkpoint(run_dir)\n",
    "    print(f\"Loading {base_id} + LoRA from:\", ckpt)\n",
    "\n",
    "    tok = load_tokenizer_local_first(base_id)\n",
    "    base = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "        base_safe,\n",
    "        use_safetensors=True,\n",
    "        torch_dtype=torch.float16 if DEVICE==\"cuda\" else None,\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    model = PeftModel.from_pretrained(base, ckpt).to(DEVICE)\n",
    "    model.eval()\n",
    "    return model, tok, ckpt\n",
    "\n",
    "hu_en_model, hu_en_tok, hu_en_ckpt = load_lora_model(HU_EN_BASE, HU_EN_RUN)\n",
    "en_ro_model, en_ro_tok, en_ro_ckpt = load_lora_model(EN_RO_BASE, EN_RO_RUN)\n",
    "\n",
    "\n",
    "PIVOT_CACHE_PATH = OUT_DIR / f\"pivot_en_cache_{hu_en_ckpt.name}_beam{NUM_BEAMS}.csv\"\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(Path(DATA_PROCESSED) / \"test.csv\")\n",
    "src_hu = test_df[\"hu\"].astype(str).tolist()\n",
    "ref_ro = test_df[\"ro\"].astype(str).tolist()\n",
    "N = len(src_hu)\n",
    "print(\"Test size:\", N)\n",
    "\n",
    "\n",
    "_punct = set(list(\"0123456789.,;:-/()[]{}<>|_+*=—–%°\\\"' \\t\"))\n",
    "def is_table_like(s: str) -> bool:\n",
    "    t = str(s).strip()\n",
    "    if not t:\n",
    "        return True\n",
    "    if \"_BAR_\" in t or t.count(\"|\") >= 2:\n",
    "        return True\n",
    "    good = sum(ch in _punct for ch in t)\n",
    "    if good / max(1, len(t)) > 0.80:\n",
    "        return True\n",
    "    if re.search(r\"[-—–]{6,}\", t):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def build_force_words_ids(tok, hu_src: str):\n",
    "    if not USE_GLOSSARY:\n",
    "        return None\n",
    "    hu_s = str(hu_src)\n",
    "    forced = []\n",
    "    for hu_term, ro_forms in glossary.items():\n",
    "        if hu_term in hu_s and ro_forms:\n",
    "            ids = tok(ro_forms[0], add_special_tokens=False).input_ids\n",
    "            if ids:\n",
    "                forced.append(ids)\n",
    "        if len(forced) >= MAX_FORCED_TERMS_PER_SENT:\n",
    "            break\n",
    "    return forced if forced else None\n",
    "\n",
    "\n",
    "def gen_batch(model, tok, batch_txt, beams, max_new, anti_repeat=False, force_words_ids=None):\n",
    "    nr = NO_REPEAT_NGRAM if anti_repeat else 0\n",
    "    rp = REPETITION_PENALTY if anti_repeat else 1.0\n",
    "\n",
    "    inp = tok(\n",
    "        batch_txt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_INPUT_LEN\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    kwargs = dict(\n",
    "        num_beams=beams,\n",
    "        max_new_tokens=max_new,\n",
    "        do_sample=False,\n",
    "        length_penalty=LENGTH_PENALTY,\n",
    "        no_repeat_ngram_size=nr,\n",
    "        repetition_penalty=rp,\n",
    "    )\n",
    "\n",
    "    \n",
    "    if force_words_ids is not None:\n",
    "        kwargs[\"force_words_ids\"] = force_words_ids\n",
    "        kwargs[\"trust_remote_code\"] = True\n",
    "        kwargs[\"custom_generate\"] = \"transformers-community/constrained-beam-search\"\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(**inp, **kwargs)\n",
    "    return tok.batch_decode(out, skip_special_tokens=True)\n",
    "\n",
    "def append_rows(path: Path, rows: list):\n",
    "    df = pd.DataFrame(rows)\n",
    "    header = not path.exists()\n",
    "    df.to_csv(path, mode=\"a\", index=False, header=header)\n",
    "\n",
    "\n",
    "if RESUME_PATH.exists():\n",
    "    done_df = pd.read_csv(RESUME_PATH)\n",
    "    done = set(done_df[\"idx\"].astype(int).tolist())\n",
    "    print(f\"Resuming EN→RO: found {len(done)} completed rows in {RESUME_PATH}\")\n",
    "else:\n",
    "    done = set()\n",
    "    print(\"Starting fresh EN→RO (no resume file).\")\n",
    "\n",
    "pivot_en = [None] * N\n",
    "\n",
    "if PIVOT_CACHE_PATH.exists():\n",
    "    piv = pd.read_csv(PIVOT_CACHE_PATH)\n",
    "    for _, r in piv.iterrows():\n",
    "        pivot_en[int(r[\"idx\"])] = r[\"pivot_en\"]\n",
    "    missing = [i for i, v in enumerate(pivot_en) if v is None]\n",
    "    print(f\"Pivot cache found: {len(piv)} rows. Missing pivot:\", len(missing))\n",
    "else:\n",
    "    missing = list(range(N))\n",
    "    print(\"No pivot cache; will compute all pivot EN.\")\n",
    "\n",
    "if missing:\n",
    "    for start in tqdm(range(0, len(missing), BS), desc=\"HU→EN cache\", unit=\"batch\"):\n",
    "        batch_idx = missing[start:start+BS]\n",
    "        batch_hu = [src_hu[i] for i in batch_idx]\n",
    "\n",
    "        outs = []\n",
    "        for x in batch_hu:\n",
    "            if is_table_like(x):\n",
    "                outs.append(gen_batch(hu_en_model, hu_en_tok, [x], BEAMS_TABLE, MAX_NEW_TABLE, anti_repeat=False)[0])\n",
    "            else:\n",
    "                outs.append(gen_batch(hu_en_model, hu_en_tok, [x], NUM_BEAMS, MAX_NEW, anti_repeat=USE_ANTI_REPEAT)[0])\n",
    "\n",
    "        for i, out in zip(batch_idx, outs):\n",
    "            pivot_en[i] = out\n",
    "\n",
    "        append_rows(PIVOT_CACHE_PATH, [{\"idx\": i, \"pivot_en\": pivot_en[i]} for i in batch_idx])\n",
    "\n",
    "print(\"Pivot EN ready. Cache:\", PIVOT_CACHE_PATH)\n",
    "\n",
    "\n",
    "for start in tqdm(range(0, N, BS), desc=\"EN→RO resumable\", unit=\"batch\"):\n",
    "    batch_idx = list(range(start, min(N, start+BS)))\n",
    "\n",
    "    if all(i in done for i in batch_idx):\n",
    "        continue\n",
    "\n",
    "    rows_to_write = []\n",
    "    for i in batch_idx:\n",
    "        if i in done:\n",
    "            continue\n",
    "\n",
    "        hu_in = src_hu[i]\n",
    "        en_in = pivot_en[i]\n",
    "\n",
    "        \n",
    "        if is_table_like(hu_in) or is_table_like(en_in):\n",
    "            beams = BEAMS_TABLE\n",
    "            max_new = MAX_NEW_TABLE\n",
    "            anti = False\n",
    "            fw = None\n",
    "        else:\n",
    "            beams = NUM_BEAMS\n",
    "            max_new = MAX_NEW\n",
    "            anti = USE_ANTI_REPEAT\n",
    "            fw = build_force_words_ids(en_ro_tok, hu_in)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            ro_out = gen_batch(\n",
    "                en_ro_model, en_ro_tok,\n",
    "                [en_in],\n",
    "                beams=beams,\n",
    "                max_new=max_new,\n",
    "                anti_repeat=anti,\n",
    "                force_words_ids=fw\n",
    "            )[0]\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            torch.cuda.empty_cache()\n",
    "            ro_out = gen_batch(en_ro_model, en_ro_tok, [en_in], beams=1, max_new=MAX_NEW_TABLE, anti_repeat=False, force_words_ids=None)[0]\n",
    "\n",
    "        rows_to_write.append({\n",
    "            \"idx\": i,\n",
    "            \"source_hu\": hu_in,\n",
    "            \"pivot_en\": en_in,\n",
    "            \"reference_ro\": ref_ro[i],\n",
    "            \"hypothesis_ro\": ro_out\n",
    "        })\n",
    "\n",
    "    if rows_to_write:\n",
    "        append_rows(RESUME_PATH, rows_to_write)\n",
    "        for r in rows_to_write:\n",
    "            done.add(int(r[\"idx\"]))\n",
    "\n",
    "print(\"Finished EN→RO. Final file:\", RESUME_PATH)\n",
    "\n",
    "\n",
    "final_df = pd.read_csv(RESUME_PATH).sort_values(\"idx\")\n",
    "hyps = final_df[\"hypothesis_ro\"].astype(str).tolist()\n",
    "refs = final_df[\"reference_ro\"].astype(str).tolist()\n",
    "srcs = final_df[\"source_hu\"].astype(str).tolist()\n",
    "\n",
    "bleu = sacrebleu.corpus_bleu(hyps, [refs]).score\n",
    "print(\"\\nFinal BLEU:\", round(bleu, 2))\n",
    "\n",
    "hu_diac = set(\"áéíóöőúüűÁÉÍÓÖŐÚÜŰ\")\n",
    "def has_hu_diacritics(s: str) -> bool:\n",
    "    return any(c in hu_diac for c in str(s))\n",
    "\n",
    "def en_leak(s: str) -> bool:\n",
    "    s = \" \" + re.sub(r\"\\s+\", \" \", str(s).lower()) + \" \"\n",
    "    return any(w in s for w in [\" the \", \" and \", \" of \", \" to \", \" for \", \" with \", \" on \"])\n",
    "\n",
    "hu_leak = np.mean([has_hu_diacritics(h) for h in hyps]) * 100\n",
    "en_leak_rate = np.mean([en_leak(h) for h in hyps]) * 100\n",
    "print(\"HU diacritics leak %:\", round(hu_leak, 2))\n",
    "print(\"English leak proxy %:\", round(en_leak_rate, 2))\n",
    "\n",
    "def norm_ro(s: str) -> str:\n",
    "    s = str(s).lower().replace(\"ţ\",\"ț\").replace(\"ş\",\"ș\")\n",
    "    return unicodedata.normalize(\"NFKC\", s)\n",
    "\n",
    "gloss_norm = {k: [norm_ro(v) for v in vs] for k, vs in glossary.items()}\n",
    "checks = []\n",
    "for s_hu, hyp in zip(srcs, hyps):\n",
    "    hyp_n = norm_ro(hyp)\n",
    "    for hu_term, ro_forms in gloss_norm.items():\n",
    "        if hu_term in s_hu:\n",
    "            checks.append(any(f in hyp_n for f in ro_forms))\n",
    "\n",
    "if checks:\n",
    "    print(\"Glossary accuracy %:\", round(np.mean(checks) * 100, 2))\n",
    "else:\n",
    "    print(\"Glossary accuracy: no glossary terms found in test.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "423585e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unbabel-comet\n",
      "  Downloading unbabel_comet-2.2.7-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting entmax<2.0,>=1.1 (from unbabel-comet)\n",
      "  Downloading entmax-1.3-py3-none-any.whl.metadata (348 bytes)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from unbabel-comet) (0.36.0)\n",
      "Collecting jsonargparse==3.13.1 (from unbabel-comet)\n",
      "  Downloading jsonargparse-3.13.1-py3-none-any.whl.metadata (55 kB)\n",
      "     ---------------------------------------- 0.0/55.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 55.5/55.5 kB 1.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.20.0 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from unbabel-comet) (1.26.1)\n",
      "Requirement already satisfied: pandas>=1.4.1 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from unbabel-comet) (2.1.2)\n",
      "Collecting protobuf<5.0.0,>=4.24.4 (from unbabel-comet)\n",
      "  Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Collecting pytorch-lightning<3.0.0,>=2.0.0 (from unbabel-comet)\n",
      "  Downloading pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: sacrebleu<3.0.0,>=2.0.0 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from unbabel-comet) (2.4.2)\n",
      "Collecting scipy<2.0.0,>=1.5.4 (from unbabel-comet)\n",
      "  Downloading scipy-1.15.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sentencepiece<0.3.0,>=0.2.0 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from unbabel-comet) (0.2.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from unbabel-comet) (2.5.1+cu121)\n",
      "Collecting torchmetrics<0.11.0,>=0.10.2 (from unbabel-comet)\n",
      "  Downloading torchmetrics-0.10.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: transformers<5.0,>=4.17 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from unbabel-comet) (4.57.5)\n",
      "Requirement already satisfied: PyYAML>=3.13 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from jsonargparse==3.13.1->unbabel-comet) (6.0.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2023.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (23.2)\n",
      "Requirement already satisfied: requests in c:\\users\\rog\\appdata\\roaming\\python\\python310\\site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->unbabel-comet) (4.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from pandas>=1.4.1->unbabel-comet) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from pandas>=1.4.1->unbabel-comet) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from pandas>=1.4.1->unbabel-comet) (2023.3)\n",
      "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet)\n",
      "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: portalocker in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (2.8.2)\n",
      "Requirement already satisfied: regex in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (2023.10.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (5.2.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from torch>=1.6.0->unbabel-comet) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from torch>=1.6.0->unbabel-comet) (3.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from torch>=1.6.0->unbabel-comet) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from sympy==1.13.1->torch>=1.6.0->unbabel-comet) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from transformers<5.0,>=4.17->unbabel-comet) (0.7.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (3.9.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from lightning-utilities>=0.10.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (68.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.4.1->unbabel-comet) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from jinja2->torch>=1.6.0->unbabel-comet) (2.1.3)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from portalocker->sacrebleu<3.0.0,>=2.0.0->unbabel-comet) (305.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rog\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rog\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rog\\appdata\\roaming\\python\\python310\\site-packages (from requests->huggingface-hub<1.0,>=0.19.3->unbabel-comet) (2023.11.17)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (1.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\rog\\anaconda3\\envs\\pythonrl\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning<3.0.0,>=2.0.0->unbabel-comet) (4.0.3)\n",
      "Downloading unbabel_comet-2.2.7-py3-none-any.whl (90 kB)\n",
      "   ---------------------------------------- 0.0/91.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 91.0/91.0 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading jsonargparse-3.13.1-py3-none-any.whl (101 kB)\n",
      "   ---------------------------------------- 0.0/101.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 101.4/101.4 kB 6.1 MB/s eta 0:00:00\n",
      "Downloading entmax-1.3-py3-none-any.whl (13 kB)\n",
      "Downloading protobuf-4.25.8-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 413.7/413.7 kB 12.6 MB/s eta 0:00:00\n",
      "Downloading pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n",
      "   ---------------------------------------- 0.0/849.5 kB ? eta -:--:--\n",
      "   --------------------------------------  839.7/849.5 kB 17.6 MB/s eta 0:00:01\n",
      "   --------------------------------------- 849.5/849.5 kB 13.5 MB/s eta 0:00:00\n",
      "Downloading scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "   ---------------------------------------- 0.0/41.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.9/41.3 MB 28.7 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 1.9/41.3 MB 24.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 3.0/41.3 MB 23.6 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 3.9/41.3 MB 22.4 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 4.6/41.3 MB 21.0 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 5.8/41.3 MB 21.8 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 6.9/41.3 MB 22.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 8.3/41.3 MB 23.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 9.5/41.3 MB 23.5 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 10.2/41.3 MB 23.3 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 11.5/41.3 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 13.0/41.3 MB 24.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 14.5/41.3 MB 27.3 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 15.9/41.3 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 17.5/41.3 MB 29.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 18.7/41.3 MB 28.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 20.0/41.3 MB 28.5 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 21.5/41.3 MB 31.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 22.3/41.3 MB 31.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 23.7/41.3 MB 29.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 24.7/41.3 MB 28.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 25.7/41.3 MB 26.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 26.4/41.3 MB 27.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 27.4/41.3 MB 24.3 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 28.3/41.3 MB 23.4 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 29.2/41.3 MB 22.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 30.4/41.3 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 31.2/41.3 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 32.4/41.3 MB 21.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 33.4/41.3 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 34.4/41.3 MB 21.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 35.6/41.3 MB 21.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 36.7/41.3 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 37.8/41.3 MB 23.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 39.0/41.3 MB 23.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 40.2/41.3 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.3/41.3 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  41.3/41.3 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.3/41.3 MB 21.8 MB/s eta 0:00:00\n",
      "Downloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n",
      "   ---------------------------------------- 0.0/529.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 529.7/529.7 kB 32.5 MB/s eta 0:00:00\n",
      "Downloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
      "Installing collected packages: scipy, protobuf, lightning-utilities, jsonargparse, torchmetrics, entmax, pytorch-lightning, unbabel-comet\n",
      "Successfully installed entmax-1.3 jsonargparse-3.13.1 lightning-utilities-0.15.2 protobuf-4.25.8 pytorch-lightning-2.6.0 scipy-1.15.3 torchmetrics-0.10.3 unbabel-comet-2.2.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "onnxruntime 1.16.2 requires flatbuffers, which is not installed.\n",
      "rembg 2.0.52 requires scikit-image, which is not installed.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U unbabel-comet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8c3302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: D:\\UniNou\\Master\\Anul 2\\Machine Translation\\Proiect 2\\MT-project-ro-hu-legal\\data\\outputs\\pivot_predictions_checkpoint-6250__checkpoint-6250_beam1.csv\n",
      "Rows: 30366\n",
      "Using columns → src: source_hu, ref: reference_ro, hyp: hypothesis_ro\n",
      "DEVICE: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RoG\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 5 files: 100%|██████████| 5/5 [00:00<00:00, 15.71it/s]\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\lightning_fabric\\utilities\\cloud_io.py:73: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\RoG\\.cache\\huggingface\\hub\\models--Unbabel--wmt22-comet-da\\snapshots\\2760a223ac957f30acfb18c8aa649b01cf1d75f2\\checkpoints\\model.ckpt`\n",
      "Encoder model frozen.\n",
      "c:\\Users\\RoG\\anaconda3\\envs\\pythonRL\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:197: Found keys that are not in the model state dict but in the checkpoint: ['encoder.model.embeddings.position_ids']\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Predicting DataLoader 0: 100%|██████████| 3796/3796 [09:13<00:00,  6.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COMET score: 0.8797\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Compute COMET score for a saved prediction CSV\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from comet import download_model, load_from_checkpoint\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# 1) Path to result CSV\n",
    "# -------------------------\n",
    "CSV_PATH = OUTPUT_DIR / \"pivot_predictions_checkpoint-6250__checkpoint-6250_beam1.csv\"\n",
    "# change filename if needed\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"Loaded:\", CSV_PATH)\n",
    "print(\"Rows:\", len(df))\n",
    "\n",
    "# -------------------------\n",
    "# 2) Detect columns robustly\n",
    "# -------------------------\n",
    "# Expected columns (any of these naming variants)\n",
    "SRC_COLS = [\"source\", \"source_hu\", \"hu\"]\n",
    "REF_COLS = [\"reference\", \"reference_ro\", \"ro\"]\n",
    "HYP_COLS = [\"hypothesis\", \"hypothesis_ro\"]\n",
    "\n",
    "def find_col(candidates):\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise ValueError(f\"None of {candidates} found in CSV columns: {df.columns.tolist()}\")\n",
    "\n",
    "src_col = find_col(SRC_COLS)\n",
    "ref_col = find_col(REF_COLS)\n",
    "hyp_col = find_col(HYP_COLS)\n",
    "\n",
    "print(f\"Using columns → src: {src_col}, ref: {ref_col}, hyp: {hyp_col}\")\n",
    "\n",
    "srcs = df[src_col].astype(str).tolist()\n",
    "refs = df[ref_col].astype(str).tolist()\n",
    "hyps = df[hyp_col].astype(str).tolist()\n",
    "\n",
    "# -------------------------\n",
    "# 3) Load COMET model\n",
    "# -------------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "comet_model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "comet_model = load_from_checkpoint(comet_model_path)\n",
    "comet_model.to(DEVICE)\n",
    "comet_model.eval()\n",
    "\n",
    "# -------------------------\n",
    "# 4) Prepare data\n",
    "# -------------------------\n",
    "data = [\n",
    "    {\"src\": s, \"mt\": h, \"ref\": r}\n",
    "    for s, h, r in zip(srcs, hyps, refs)\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# 5) Compute COMET\n",
    "# -------------------------\n",
    "with torch.no_grad():\n",
    "    scores = comet_model.predict(\n",
    "        data,\n",
    "        batch_size=8 if DEVICE == \"cuda\" else 2,\n",
    "        gpus=1 if DEVICE == \"cuda\" else 0,\n",
    "        progress_bar=True\n",
    "    )\n",
    "\n",
    "comet_score = scores[\"system_score\"]\n",
    "print(\"\\nCOMET score:\", round(comet_score, 4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
